{
  "fileSummary": {
    "generationHeader": "This file is a merged representation of the entire codebase, combined into a single document by Repomix.",
    "purpose": "This file contains a packed representation of the entire repository's contents.\nIt is designed to be easily consumable by AI systems for analysis, code review,\nor other automated processes.",
    "fileFormat": "The content is organized as follows:\n1. This summary section\n2. Repository information\n3. Directory structure\n4. Repository files, each consisting of:\n   - File path as a key\n   - Full contents of the file as the value",
    "usageGuidelines": "- This file should be treated as read-only. Any changes should be made to the\n  original repository files, not this packed version.\n- When processing this file, use the file path to distinguish\n  between different files in the repository.\n- Be aware that this file may contain sensitive information. Handle it with\n  the same level of security as you would the original repository.",
    "notes": "- Some files may have been excluded based on .gitignore rules and Repomix's configuration\n- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files\n- Files matching patterns in .gitignore are excluded\n- Files matching default ignore patterns are excluded\n- Files are sorted by Git change count (files with more changes are at the bottom)"
  },
  "directoryStructure": "applications/\n  consumer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  producer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  applications-flow.md\n  docker-compose.yml\ninfrastructure/\n  postgres/\n    init/\n      01-init.sql\n  docker-compose-application.yml\n  infrastructure.md\nmonitoring/\n  alertmanager/\n    alertmanager.yml\n  grafana/\n    dashboards/\n      applications/\n        data-pipeline.json\n        red-metrics.json\n        slo-dashboard.json\n      infrastructure/\n        container-metrics.json\n        kafka.json\n        postgresql.json\n      logs/\n        logs-explorer.json\n        test.json\n      overview/\n        correlation-dashboard.json\n        system-overview.json\n      tracing/\n        tracing-overview.json\n    provisioning/\n      dashboards/\n        dashboards.yml\n      datasources/\n        datasources.yml\n  loki/\n    rules/\n      fake/\n        alerts.yml\n      alerts.yml\n    loki-config.yml\n  prometheus/\n    rules/\n      alert_rules.yml\n      recording_rules.yml\n      slo_alerts.yml\n      slo_rules.yml\n    prometheus.yml\n  promtail/\n    promtail-config.yml\n  stress-testing/\n    load-test/\n      Dockerfile\n      load_test.py\n      requirements.txt\n    scripts/\n      chaos_test.sh\n      ramp_up_test.sh\n      run_load_test.sh\n      spike_test.sh\n    docker-compose-stress-test.yml\n  docker-compose-monitoring.yml\n  loki.md\n  SLO.md\n  stack.md\nnetworks/\n  docker-compose-network.yml\nmonitoring-overview.md\nrepomix-output.md\nrepomix-output.xml\nstructure.md",
  "files": {
    "applications/consumer/config.py": "from pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    \"\"\"Consumer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    kafka_consumer_group: str = \"order-processor\"\n    \n    # PostgreSQL settings\n    postgres_host: str = \"postgres\"\n    postgres_port: int = 5432\n    postgres_db: str = \"datawarehouse\"\n    postgres_user: str = \"postgres\"\n    postgres_password: str = \"postgres123\"\n    \n    # Consumer settings\n    batch_size: int = 10\n    poll_timeout_ms: int = 1000\n    \n    # Metrics server\n    metrics_port: int = 8001\n    \n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-consumer\"\n    otel_enabled: bool = True\n    \n    # Application\n    app_name: str = \"data-consumer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n    \n    @property\n    def postgres_dsn(self) -> str:\n        return (\n            f\"host={self.postgres_host} \"\n            f\"port={self.postgres_port} \"\n            f\"dbname={self.postgres_db} \"\n            f\"user={self.postgres_user} \"\n            f\"password={self.postgres_password}\"\n        )\n\n\nsettings = Settings()",
    "applications/consumer/Dockerfile": "# applications/consumer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8001\n\n# Run the consumer\nCMD [\"python\", \"-u\", \"main.py\"]",
    "applications/consumer/logger.py": "# applications/consumer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()",
    "applications/consumer/main.py": "# applications/consumer/main.py\n\nimport json\nimport time\nfrom datetime import datetime, timezone\n\nimport psycopg2\nfrom psycopg2.extras import execute_batch\nfrom kafka import KafkaConsumer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode, SpanKind\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_consumer_info',\n    'Consumer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'consumer_group': settings.kafka_consumer_group,\n    'pattern': 'RED'\n})\n\nMESSAGES_CONSUMED = Counter(\n    'pipeline_consumer_messages_total',\n    'Total number of messages consumed',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PROCESSED = Counter(\n    'pipeline_consumer_batches_total',\n    'Total number of batches processed',\n    ['status']\n)\n\nDB_OPERATIONS = Counter(\n    'pipeline_consumer_db_operations_total',\n    'Total database operations',\n    ['operation', 'status']\n)\n\nERRORS = Counter(\n    'pipeline_consumer_errors_total',\n    'Total number of errors by type and stage',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_message_duration_seconds',\n    'Time to process a single message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_batch_duration_seconds',\n    'Time to process a batch of messages',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nDB_QUERY_DURATION = Histogram(\n    'pipeline_consumer_db_query_duration_seconds',\n    'Database query duration',\n    ['operation'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5)\n)\n\nKAFKA_POLL_DURATION = Histogram(\n    'pipeline_consumer_kafka_poll_duration_seconds',\n    'Kafka poll duration',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0)\n)\n\nEND_TO_END_LATENCY = Histogram(\n    'pipeline_consumer_end_to_end_latency_seconds',\n    'End-to-end latency from order creation to database insert',\n    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0)\n)\n\nORDERS_PROCESSED = Counter(\n    'pipeline_business_orders_processed_total',\n    'Total orders processed by category',\n    ['category']\n)\n\nREVENUE_PROCESSED = Counter(\n    'pipeline_business_revenue_processed_total',\n    'Total revenue processed',\n    ['category']\n)\n\nCONSUMER_UP = Gauge(\n    'pipeline_consumer_up',\n    'Consumer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_kafka_connected',\n    'Kafka connection status'\n)\n\nDB_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_db_connected',\n    'Database connection status'\n)\n\nCONSUMER_LAG = Gauge(\n    'pipeline_consumer_lag_messages',\n    'Consumer lag per partition',\n    ['topic', 'partition']\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_consumer_current_batch_size',\n    'Number of messages in current batch'\n)\n\nLAST_PROCESS_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_process_timestamp',\n    'Timestamp of last successful process'\n)\n\nLAST_COMMIT_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_commit_timestamp',\n    'Timestamp of last offset commit'\n)\n\n\n# ===========================================\n# Database Handler\n# ===========================================\n\nclass DatabaseHandler:\n    def __init__(self):\n        self.conn = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to PostgreSQL with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.conn = psycopg2.connect(settings.postgres_dsn)\n                self.conn.autocommit = False\n                logger.info(\n                    \"Connected to PostgreSQL\",\n                    extra={\n                        \"event\": \"db_connected\",\n                        \"host\": settings.postgres_host,\n                        \"database\": settings.postgres_db,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                DB_CONNECTION_STATUS.set(1)\n                return\n            except psycopg2.Error as e:\n                DB_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"db_init\").inc()\n                logger.warning(\n                    \"PostgreSQL connection failed, retrying...\",\n                    extra={\n                        \"event\": \"db_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to PostgreSQL after maximum retries\")\n    \n    def insert_orders(self, orders: list[dict], parent_span=None) -> int:\n        \"\"\"Insert batch of orders into database with tracing\"\"\"\n        if not orders:\n            return 0\n        \n        with tracer.start_as_current_span(\n            \"db_insert_orders\",\n            kind=SpanKind.CLIENT\n        ) as span:\n            span.set_attribute(\"db.system\", \"postgresql\")\n            span.set_attribute(\"db.name\", settings.postgres_db)\n            span.set_attribute(\"db.operation\", \"INSERT\")\n            span.set_attribute(\"db.batch_size\", len(orders))\n            \n            insert_sql = \"\"\"\n                INSERT INTO ecommerce.orders (\n                    order_id, customer_id, product_id, product_name, \n                    category, quantity, unit_price, total_amount,\n                    order_status, created_at, processed_at\n                ) VALUES (\n                    %(order_id)s, %(customer_id)s, %(product_id)s, %(product_name)s,\n                    %(category)s, %(quantity)s, %(unit_price)s, %(total_amount)s,\n                    %(order_status)s, %(created_at)s, %(processed_at)s\n                )\n                ON CONFLICT (order_id) DO NOTHING\n            \"\"\"\n            \n            try:\n                start_time = time.time()\n                \n                with self.conn.cursor() as cur:\n                    process_time = datetime.now(timezone.utc)\n                    \n                    for order in orders:\n                        order['processed_at'] = process_time.isoformat()\n                        if isinstance(order.get('created_at'), str):\n                            order['created_at'] = datetime.fromisoformat(\n                                order['created_at'].replace('Z', '+00:00')\n                            )\n                    \n                    execute_batch(cur, insert_sql, orders, page_size=100)\n                    inserted = cur.rowcount\n                    self.conn.commit()\n                \n                duration = time.time() - start_time\n                \n                span.set_attribute(\"db.rows_affected\", inserted)\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                span.set_status(Status(StatusCode.OK))\n                \n                DB_QUERY_DURATION.labels(operation=\"insert_batch\").observe(duration)\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"success\").inc(len(orders))\n                \n                logger.debug(\n                    \"Orders inserted into database\",\n                    extra={\n                        \"event\": \"db_insert_success\",\n                        \"count\": inserted,\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return inserted\n                \n            except psycopg2.Error as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                ERRORS.labels(error_type=type(e).__name__, stage=\"db_insert\").inc()\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"failed\").inc(len(orders))\n                \n                logger.error(\n                    \"Database insert failed\",\n                    extra={\n                        \"event\": \"db_insert_error\",\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__,\n                        \"batch_size\": len(orders)\n                    }\n                )\n                self.conn.rollback()\n                \n                try:\n                    self._connect()\n                except Exception:\n                    DB_CONNECTION_STATUS.set(0)\n                \n                return 0\n    \n    def close(self):\n        \"\"\"Close database connection\"\"\"\n        if self.conn:\n            self.conn.close()\n            DB_CONNECTION_STATUS.set(0)\n            logger.info(\"Database connection closed\", extra={\"event\": \"db_closed\"})\n\n\n# ===========================================\n# Kafka Consumer\n# ===========================================\n\nclass OrderConsumer:\n    def __init__(self, db_handler: DatabaseHandler):\n        self.consumer = None\n        self.db = db_handler\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.consumer = KafkaConsumer(\n                    settings.kafka_topic,\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    group_id=settings.kafka_consumer_group,\n                    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n                    auto_offset_reset='earliest',\n                    enable_auto_commit=False,\n                    max_poll_records=settings.batch_size,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"topic\": settings.kafka_topic,\n                        \"consumer_group\": settings.kafka_consumer_group,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                CONSUMER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"kafka_init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def process_messages(self):\n        \"\"\"Main processing loop with tracing\"\"\"\n        batch: list[dict] = []\n        \n        logger.info(\"Starting message processing loop\", extra={\"event\": \"processing_started\"})\n        \n        try:\n            while True:\n                # Poll for messages\n                with tracer.start_as_current_span(\n                    \"kafka_poll\",\n                    kind=SpanKind.CONSUMER\n                ) as poll_span:\n                    poll_start = time.time()\n                    records = self.consumer.poll(\n                        timeout_ms=settings.poll_timeout_ms,\n                        max_records=settings.batch_size\n                    )\n                    poll_duration = time.time() - poll_start\n                    \n                    poll_span.set_attribute(\"messaging.system\", \"kafka\")\n                    poll_span.set_attribute(\"messaging.operation\", \"poll\")\n                    poll_span.set_attribute(\"duration_ms\", round(poll_duration * 1000, 2))\n                    \n                    KAFKA_POLL_DURATION.observe(poll_duration)\n                \n                if not records:\n                    if batch:\n                        self._process_batch(batch)\n                        batch = []\n                    continue\n                \n                # Process received messages\n                for topic_partition, messages in records.items():\n                    for message in messages:\n                        try:\n                            msg_start = time.time()\n                            order = message.value\n                            category = order.get(\"category\", \"unknown\")\n                            \n                            # Create span for consuming\n                            with tracer.start_as_current_span(\n                                \"consume_message\",\n                                kind=SpanKind.CONSUMER\n                            ) as msg_span:\n                                # Add span attributes\n                                msg_span.set_attribute(\"messaging.system\", \"kafka\")\n                                msg_span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n                                msg_span.set_attribute(\"messaging.kafka.partition\", topic_partition.partition)\n                                msg_span.set_attribute(\"messaging.kafka.offset\", message.offset)\n                                msg_span.set_attribute(\"order.id\", order.get(\"order_id\", \"\"))\n                                msg_span.set_attribute(\"order.category\", category)\n                                \n                                # Link to producer trace if available\n                                if \"trace_id\" in order:\n                                    msg_span.set_attribute(\"producer.trace_id\", order[\"trace_id\"])\n                                \n                                batch.append(order)\n                                \n                                MESSAGES_CONSUMED.labels(\n                                    topic=settings.kafka_topic,\n                                    status=\"success\",\n                                    category=category\n                                ).inc()\n                                \n                                msg_duration = time.time() - msg_start\n                                MESSAGE_PROCESS_DURATION.observe(msg_duration)\n                                \n                                # Calculate end-to-end latency\n                                if 'created_at' in order:\n                                    try:\n                                        created_at = datetime.fromisoformat(\n                                            order['created_at'].replace('Z', '+00:00')\n                                        )\n                                        e2e_latency = (datetime.now(timezone.utc) - created_at).total_seconds()\n                                        END_TO_END_LATENCY.observe(e2e_latency)\n                                        msg_span.set_attribute(\"e2e_latency_seconds\", e2e_latency)\n                                    except Exception:\n                                        pass\n                                \n                                CONSUMER_LAG.labels(\n                                    topic=topic_partition.topic,\n                                    partition=str(topic_partition.partition)\n                                ).set(message.offset)\n                                \n                                msg_span.set_status(Status(StatusCode.OK))\n                                \n                                logger.info(\n                                    \"Message consumed\",\n                                    extra={\n                                        \"event\": \"message_consumed\",\n                                        \"order_id\": order.get(\"order_id\"),\n                                        \"trace_id\": order.get(\"trace_id\", \"\"),\n                                        \"category\": category,\n                                        \"partition\": topic_partition.partition,\n                                        \"offset\": message.offset\n                                    }\n                                )\n                                \n                        except Exception as e:\n                            MESSAGES_CONSUMED.labels(\n                                topic=settings.kafka_topic,\n                                status=\"failed\",\n                                category=\"unknown\"\n                            ).inc()\n                            \n                            ERRORS.labels(\n                                error_type=type(e).__name__,\n                                stage=\"message_parse\"\n                            ).inc()\n                            \n                            logger.error(\n                                \"Failed to process message\",\n                                extra={\n                                    \"event\": \"message_parse_error\",\n                                    \"error\": str(e),\n                                    \"error_type\": type(e).__name__,\n                                    \"partition\": topic_partition.partition,\n                                    \"offset\": message.offset\n                                }\n                            )\n\n                \n                CURRENT_BATCH_SIZE.set(len(batch))\n                \n                if len(batch) >= settings.batch_size:\n                    self._process_batch(batch)\n                    batch = []\n                    \n        except KeyboardInterrupt:\n            logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n            if batch:\n                self._process_batch(batch)\n    \n    def _process_batch(self, batch: list[dict]):\n        \"\"\"Process and commit a batch of messages with tracing\"\"\"\n        if not batch:\n            return\n        \n        with tracer.start_as_current_span(\n            \"process_batch\",\n            kind=SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", len(batch))\n            \n            start_time = time.time()\n            \n            # Insert to database\n            inserted = self.db.insert_orders(batch)\n            \n            # Commit Kafka offsets\n            self.consumer.commit()\n            LAST_COMMIT_TIMESTAMP.set(time.time())\n            \n            duration = time.time() - start_time\n            \n            batch_span.set_attribute(\"batch.inserted\", inserted)\n            batch_span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n            \n            BATCH_PROCESS_DURATION.observe(duration)\n            \n            if inserted == len(batch):\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"success\").inc()\n            elif inserted == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"No records inserted\"))\n                BATCHES_PROCESSED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"partial\").inc()\n            \n            # Business metrics\n            for order in batch:\n                category = order.get(\"category\", \"unknown\")\n                ORDERS_PROCESSED.labels(category=category).inc()\n                REVENUE_PROCESSED.labels(category=category).inc(order.get(\"total_amount\", 0))\n            \n            LAST_PROCESS_TIMESTAMP.set(time.time())\n            CURRENT_BATCH_SIZE.set(0)\n            \n            logger.info(\n                \"Batch processed\",\n                extra={\n                    \"event\": \"batch_processed\",\n                    \"batch_size\": len(batch),\n                    \"inserted\": inserted,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(len(batch) / duration, 2) if duration > 0 else 0\n                }\n            )\n    \n    def close(self):\n        \"\"\"Close the consumer\"\"\"\n        if self.consumer:\n            self.consumer.close()\n            CONSUMER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Consumer closed\", extra={\"event\": \"consumer_closed\"})\n\n\n# ===========================================\n# Main\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Consumer\",\n        extra={\n            \"event\": \"consumer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"consumer_group\": settings.kafka_consumer_group,\n                \"postgres_host\": settings.postgres_host,\n                \"postgres_db\": settings.postgres_db,\n                \"batch_size\": settings.batch_size,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create handlers\n    db_handler = DatabaseHandler()\n    consumer = OrderConsumer(db_handler)\n    \n    try:\n        consumer.process_messages()\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in consumer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        consumer.close()\n        db_handler.close()\n        logger.info(\"Consumer stopped\", extra={\"event\": \"consumer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()",
    "applications/consumer/requirements.txt": "# Kafka\nkafka-python-ng==2.2.2\n\n# PostgreSQL\npsycopg2-binary==2.9.9\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0",
    "applications/consumer/tracing.py": "# applications/consumer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()",
    "applications/producer/config.py": "# applications/producer/config.py\n\nfrom pydantic_settings import BaseSettings\nfrom typing import Optional\n\n\nclass Settings(BaseSettings):\n    \"\"\"Producer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    \n    # Producer settings\n    produce_interval_seconds: float = 1.0  # Produce every N seconds\n    batch_size: int = 5  # Messages per batch\n    \n    # Metrics server\n    metrics_port: int = 8000\n\n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-producer\"\n    otel_enabled: bool = True\n\n    # Application\n    app_name: str = \"data-producer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n\n\nsettings = Settings()",
    "applications/producer/Dockerfile": "# applications/producer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8000\n\n# Run the producer\nCMD [\"python\", \"-u\", \"main.py\"]",
    "applications/producer/logger.py": "# applications/producer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()",
    "applications/producer/main.py": "# applications/producer/main.py\n\nimport json\nimport random\nimport time\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_producer_info',\n    'Producer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'pattern': 'RED'\n})\n\nMESSAGES_PRODUCED = Counter(\n    'pipeline_producer_messages_total',\n    'Total number of messages produced',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PRODUCED = Counter(\n    'pipeline_producer_batches_total',\n    'Total number of batches produced',\n    ['status']\n)\n\nERRORS = Counter(\n    'pipeline_producer_errors_total',\n    'Total number of errors by type',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_message_duration_seconds',\n    'Time to produce a single message to Kafka',\n    ['topic'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_batch_duration_seconds',\n    'Time to produce a batch of messages',\n    ['topic'],\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nMESSAGE_SIZE = Histogram(\n    'pipeline_producer_message_size_bytes',\n    'Size of produced messages in bytes',\n    ['topic'],\n    buckets=(100, 500, 1000, 2500, 5000, 10000, 25000)\n)\n\nORDERS_BY_CATEGORY = Counter(\n    'pipeline_business_orders_total',\n    'Total orders by category',\n    ['category']\n)\n\nREVENUE = Counter(\n    'pipeline_business_revenue_total',\n    'Total revenue generated',\n    ['category']\n)\n\nORDER_VALUE = Histogram(\n    'pipeline_business_order_value',\n    'Distribution of order values',\n    ['category'],\n    buckets=(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\n)\n\nITEMS_PER_ORDER = Histogram(\n    'pipeline_business_items_per_order',\n    'Distribution of items per order',\n    buckets=(1, 2, 3, 4, 5, 10)\n)\n\nPRODUCER_UP = Gauge(\n    'pipeline_producer_up',\n    'Producer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_producer_kafka_connected',\n    'Kafka connection status (1=connected, 0=disconnected)'\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_producer_current_batch_size',\n    'Current configured batch size'\n)\n\nLAST_PRODUCE_TIMESTAMP = Gauge(\n    'pipeline_producer_last_produce_timestamp',\n    'Timestamp of last successful produce'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\n\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order(trace_id: str = None) -> dict[str, Any]:\n    \"\"\"Generate a fake e-commerce order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    order = {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n    }\n    \n    # Add trace_id for correlation\n    if trace_id:\n        order[\"trace_id\"] = trace_id\n    \n    return order\n\n\n# ===========================================\n# Kafka Producer\n# ===========================================\n\nclass OrderProducer:\n    def __init__(self):\n        self.producer = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks='all',\n                    retries=3,\n                    max_in_flight_requests_per_connection=1,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                PRODUCER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def produce(self, order: dict) -> bool:\n        \"\"\"Produce a single order to Kafka with tracing\"\"\"\n        category = order.get(\"category\", \"unknown\")\n        \n        # Create span for this operation\n        with tracer.start_as_current_span(\n            \"produce_order\",\n            kind=trace.SpanKind.PRODUCER\n        ) as span:\n            # Add span attributes\n            span.set_attribute(\"messaging.system\", \"kafka\")\n            span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n            span.set_attribute(\"order.id\", order[\"order_id\"])\n            span.set_attribute(\"order.category\", category)\n            span.set_attribute(\"order.amount\", order[\"total_amount\"])\n            \n            # Get trace_id and add to order for correlation\n            trace_id = format(span.get_span_context().trace_id, '032x')\n            order[\"trace_id\"] = trace_id\n            \n            message_bytes = json.dumps(order).encode('utf-8')\n            span.set_attribute(\"messaging.message.payload_size_bytes\", len(message_bytes))\n            \n            try:\n                start_time = time.time()\n                \n                future = self.producer.send(\n                    settings.kafka_topic,\n                    key=order[\"order_id\"],\n                    value=order\n                )\n                future.get(timeout=10)\n                \n                duration = time.time() - start_time\n                \n                # Record success in span\n                span.set_status(Status(StatusCode.OK))\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                \n                # Metrics\n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"success\",\n                    category=category\n                ).inc()\n                \n                MESSAGE_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n                MESSAGE_SIZE.labels(topic=settings.kafka_topic).observe(len(message_bytes))\n                \n                ORDERS_BY_CATEGORY.labels(category=category).inc()\n                REVENUE.labels(category=category).inc(order[\"total_amount\"])\n                ORDER_VALUE.labels(category=category).observe(order[\"total_amount\"])\n                ITEMS_PER_ORDER.observe(order[\"quantity\"])\n                \n                LAST_PRODUCE_TIMESTAMP.set(time.time())\n                \n                logger.debug(\n                    \"Order produced successfully\",\n                    extra={\n                        \"event\": \"order_produced\",\n                        \"order_id\": order[\"order_id\"],\n                        \"trace_id\": trace_id,\n                        \"category\": category,\n                        \"amount\": order[\"total_amount\"],\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return True\n                \n            except Exception as e:\n                # Record error in span\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"failed\",\n                    category=category\n                ).inc()\n                \n                ERRORS.labels(\n                    error_type=type(e).__name__,\n                    stage=\"produce\"\n                ).inc()\n                \n                logger.error(\n                    \"Failed to produce order\",\n                    extra={\n                        \"event\": \"produce_error\",\n                        \"order_id\": order.get(\"order_id\"),\n                        \"trace_id\": trace_id,\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__\n                    }\n                )\n                return False\n    \n    def produce_batch(self, batch_size: int) -> tuple[int, int]:\n        \"\"\"Produce a batch of orders with tracing\"\"\"\n        \n        # Create parent span for batch\n        with tracer.start_as_current_span(\n            \"produce_batch\",\n            kind=trace.SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", batch_size)\n            \n            success_count = 0\n            failed_count = 0\n            batch_orders = []\n            \n            CURRENT_BATCH_SIZE.set(batch_size)\n            \n            start_time = time.time()\n            \n            for _ in range(batch_size):\n                order = generate_order()\n                if self.produce(order):\n                    success_count += 1\n                    batch_orders.append(order[\"order_id\"])\n                else:\n                    failed_count += 1\n            \n            self.producer.flush()\n            \n            duration = time.time() - start_time\n            \n            # Record batch results in span\n            batch_span.set_attribute(\"batch.success_count\", success_count)\n            batch_span.set_attribute(\"batch.failed_count\", failed_count)\n            batch_span.set_attribute(\"batch.duration_ms\", round(duration * 1000, 2))\n            \n            if failed_count == 0:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"success\").inc()\n            elif success_count == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"All messages failed\"))\n                BATCHES_PRODUCED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"partial\").inc()\n            \n            BATCH_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n            \n            logger.info(\n                \"Batch produced\",\n                extra={\n                    \"event\": \"batch_produced\",\n                    \"batch_size\": batch_size,\n                    \"success_count\": success_count,\n                    \"failed_count\": failed_count,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(batch_size / duration, 2) if duration > 0 else 0\n                }\n            )\n            \n            return success_count, failed_count\n    \n    def close(self):\n        \"\"\"Close the producer\"\"\"\n        if self.producer:\n            self.producer.close()\n            PRODUCER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Producer closed\", extra={\"event\": \"producer_closed\"})\n\n\n# ===========================================\n# Main Loop\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Producer\",\n        extra={\n            \"event\": \"producer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"batch_size\": settings.batch_size,\n                \"interval_seconds\": settings.produce_interval_seconds,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create producer\n    producer = OrderProducer()\n    \n    try:\n        batch_number = 0\n        while True:\n            batch_number += 1\n            \n            # Create span for each iteration\n            with tracer.start_as_current_span(f\"batch_iteration_{batch_number}\"):\n                success, failed = producer.produce_batch(settings.batch_size)\n            \n            time.sleep(settings.produce_interval_seconds)\n            \n    except KeyboardInterrupt:\n        logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in producer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        producer.close()\n        logger.info(\"Producer stopped\", extra={\"event\": \"producer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()",
    "applications/producer/requirements.txt": "# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data generation\nfaker==24.4.0\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0",
    "applications/producer/tracing.py": "# applications/producer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()",
    "applications/applications-flow.md": "```mermaid\ngraph LR\n    subgraph \"applications/docker-compose.yml\"\n        subgraph \"Producer\"\n            P[Data Producer<br/>:8000/metrics]\n            F[Faker Library]\n        end\n        \n        subgraph \"Consumer\"\n            C[Data Consumer<br/>:8001/metrics]\n        end\n    end\n    \n    subgraph \"infrastructure/\"\n        K[Kafka]\n        PG[(PostgreSQL)]\n    end\n    \n    subgraph \"monitoring/\"\n        PR[Prometheus]\n    end\n    \n    F --> P\n    P -->|produce orders| K\n    K -->|consume| C\n    C -->|insert| PG\n    \n    P -.->|metrics| PR\n    C -.->|metrics| PR\n```",
    "applications/docker-compose.yml": "# applications/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Data Producer - Generate fake orders\n  # ===========================================\n  data-producer:\n    build:\n      context: ./producer\n      dockerfile: Dockerfile\n    container_name: data-producer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      \n      # Producer settings\n      PRODUCE_INTERVAL_SECONDS: \"1.0\"\n      BATCH_SIZE: \"5\"\n      \n      # Metrics\n      METRICS_PORT: \"8000\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8000:8000\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8000/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Data Consumer - Process orders to PostgreSQL\n  # ===========================================\n  data-consumer:\n    build:\n      context: ./consumer\n      dockerfile: Dockerfile\n    container_name: data-consumer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      KAFKA_CONSUMER_GROUP: order-processor\n      \n      # PostgreSQL settings\n      POSTGRES_HOST: postgres\n      POSTGRES_PORT: \"5432\"\n      POSTGRES_DB: datawarehouse\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres123\n      \n      # Consumer settings\n      BATCH_SIZE: \"10\"\n      POLL_TIMEOUT_MS: \"1000\"\n      \n      # Metrics\n      METRICS_PORT: \"8001\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8001:8001\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    #   postgres:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8001/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Networks\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true",
    "infrastructure/postgres/init/01-init.sql": "-- infrastructure/postgres/init/01-init.sql\n\n-- ===========================================\n-- Database cho Data Pipeline\n-- ===========================================\n\n-- Tạo schema cho e-commerce data\nCREATE SCHEMA IF NOT EXISTS ecommerce;\n\n-- Bảng orders - nơi lưu data từ Kafka consumer\nCREATE TABLE ecommerce.orders (\n    id SERIAL PRIMARY KEY,\n    order_id VARCHAR(50) UNIQUE NOT NULL,\n    customer_id VARCHAR(50) NOT NULL,\n    product_id VARCHAR(50) NOT NULL,\n    product_name VARCHAR(255) NOT NULL,\n    category VARCHAR(100),\n    quantity INTEGER NOT NULL,\n    unit_price DECIMAL(10, 2) NOT NULL,\n    total_amount DECIMAL(10, 2) NOT NULL,\n    order_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    processed_at TIMESTAMP,\n    \n    -- Indexes cho queries thường dùng\n    CONSTRAINT chk_quantity CHECK (quantity > 0),\n    CONSTRAINT chk_price CHECK (unit_price > 0)\n);\n\nCREATE INDEX idx_orders_customer ON ecommerce.orders(customer_id);\nCREATE INDEX idx_orders_created_at ON ecommerce.orders(created_at);\nCREATE INDEX idx_orders_status ON ecommerce.orders(order_status);\nCREATE INDEX idx_orders_category ON ecommerce.orders(category);\n\n-- Bảng để track processing metrics\nCREATE TABLE ecommerce.processing_logs (\n    id SERIAL PRIMARY KEY,\n    batch_id VARCHAR(50) NOT NULL,\n    records_processed INTEGER NOT NULL,\n    records_failed INTEGER DEFAULT 0,\n    processing_time_ms INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- View cho monitoring\nCREATE VIEW ecommerce.orders_summary AS\nSELECT \n    DATE(created_at) as order_date,\n    COUNT(*) as total_orders,\n    SUM(total_amount) as total_revenue,\n    AVG(total_amount) as avg_order_value,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM ecommerce.orders\nGROUP BY DATE(created_at)\nORDER BY order_date DESC;\n\n-- Grant permissions\nGRANT ALL PRIVILEGES ON SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA ecommerce TO postgres;\n\n-- Log initialization\nDO $$\nBEGIN\n    RAISE NOTICE 'Database initialization completed successfully!';\nEND $$;",
    "infrastructure/infrastructure.md": "```mermaid\ngraph TB\n    subgraph \"infrastructure/docker-compose.yml\"\n        subgraph \"Message Queue\"\n            ZK[Zookeeper:2181]\n            K[Kafka:9092]\n            KE[Kafka Exporter:9308]\n        end\n        \n        subgraph \"Database\"\n            PG[(PostgreSQL:5432)]\n            PE[Postgres Exporter:9187]\n        end\n        \n        ZK --> K\n        K -.->|metrics| KE\n        PG -.->|metrics| PE\n    end\n    \n    subgraph \"monitoring/\"\n        P[Prometheus]\n    end\n    \n    KE -->|scrape :9308| P\n    PE -->|scrape :9187| P\n```",
    "monitoring/alertmanager/alertmanager.yml": "# monitoring/alertmanager/alertmanager.yml\n\nglobal:\n  # Thời gian chờ trước khi gửi lại alert nếu vẫn còn firing\n  resolve_timeout: 5m\n\n# Route tree - định tuyến alerts\nroute:\n  # Default receiver\n  receiver: 'default-receiver'\n  \n  # Group alerts by these labels\n  group_by: ['alertname', 'severity', 'service']\n  \n  # Thời gian chờ trước khi gửi group đầu tiên\n  group_wait: 30s\n  \n  # Thời gian chờ trước khi gửi alerts mới trong cùng group\n  group_interval: 5m\n  \n  # Thời gian chờ trước khi gửi lại alert đã gửi\n  repeat_interval: 4h\n\n  # Child routes - routing dựa trên labels\n  routes:\n    # Critical alerts - gửi ngay\n    - match:\n        severity: critical\n      receiver: 'critical-receiver'\n      group_wait: 10s\n      repeat_interval: 1h\n\n    # Warning alerts\n    - match:\n        severity: warning\n      receiver: 'warning-receiver'\n      repeat_interval: 4h\n\n    # Info alerts - ít urgent hơn\n    - match:\n        severity: info\n      receiver: 'default-receiver'\n      repeat_interval: 12h\n\n# Inhibition rules - suppress alerts\ninhibit_rules:\n  # Nếu critical đang fire, suppress warning cùng service\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'service']\n\n# Receivers - nơi nhận alerts\nreceivers:\n  - name: 'default-receiver'\n    # Webhook để test (có thể xem trong Alertmanager UI)\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n\n  - name: 'critical-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n    # Có thể thêm Slack, Email, PagerDuty ở đây\n    # slack_configs:\n    #   - api_url: 'https://hooks.slack.com/services/xxx/yyy/zzz'\n    #     channel: '#alerts-critical'\n    #     send_resolved: true\n\n  - name: 'warning-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true",
    "monitoring/grafana/dashboards/applications/data-pipeline.json": "{\n  \"uid\": \"data-pipeline-dashboard\",\n  \"title\": \"Data Pipeline Overview\",\n  \"description\": \"Monitoring data pipeline: Producer → Kafka → Consumer → PostgreSQL\",\n  \"tags\": [\"applications\", \"data-pipeline\", \"producer\", \"consumer\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🚀 Data Pipeline Flow\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## Pipeline: **Producer** → **Kafka** → **Consumer** → **PostgreSQL**\\n\\n*Metrics sẽ hiển thị khi Producer và Consumer được khởi động.*\"\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-producer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 6, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-consumer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Messages Produced (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_produced_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Messages Consumed (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_consumed_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Production Rate (msg/s)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_messages_produced_total[1m])\",\n          \"legendFormat\": \"Produced\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_messages_consumed_total[1m])\",\n          \"legendFormat\": \"Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Produced\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Processing Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Errors\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-producer\\\"}[1m])\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-consumer\\\"}[1m])\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"errors/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 0.1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Database Inserts (from Consumer)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_db_inserts_total[1m])\",\n          \"legendFormat\": \"Inserts/s\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"single\", \"sort\": \"none\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"gradientMode\": \"hue\", \"lineWidth\": 1 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Orders by Category (Real-time)\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_orders_by_category)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Revenue Generated\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_total_revenue\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"noValue\": \"$0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Pipeline Health Score\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(1 - (rate(pipeline_errors_total[5m]) / (rate(pipeline_messages_produced_total[5m]) + 0.001))) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"noValue\": \"N/A\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 90 }, { \"color\": \"green\", \"value\": 99 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/applications/red-metrics.json": "{\n  \"uid\": \"red-metrics-dashboard\",\n  \"title\": \"RED Metrics - Data Pipeline\",\n  \"description\": \"Rate, Errors, Duration metrics for the data pipeline\",\n  \"tags\": [\"red\", \"applications\", \"sre\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 2, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 📊 RED Metrics Overview\\n**R**ate (throughput) | **E**rrors (failures) | **D**uration (latency)   The golden signals for service health\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"🚀 PRODUCER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 2 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Producer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Success\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Failed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Success\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Failed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Producer Latency Percentiles\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 8 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Producer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 10 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_producer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📥 CONSUMER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 17 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency (P95)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 30 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"DB Inserts/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Inserted to DB\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Inserted to DB\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"purple\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 17,\n      \"title\": \"End-to-End Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 18,\n      \"title\": \"Consumer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 25 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_consumer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"💰 BUSINESS METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 35 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Orders/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_orders_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Revenue/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 23,\n      \"title\": \"Avg Order Value\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[5m])) / (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 24,\n      \"title\": \"Orders by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_orders_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"short\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 25,\n      \"title\": \"Revenue by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 18, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_revenue_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"currencyUSD\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 26,\n      \"title\": \"Revenue Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 41 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"legendFormat\": \"{{category}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"lineWidth\": 2, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 27,\n      \"title\": \"Order Value Distribution\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 44 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"Median\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.90, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p90\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p99 (High Value)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/applications/slo-dashboard.json": "{\n  \"uid\": \"slo-dashboard\",\n  \"title\": \"SLI/SLO Dashboard\",\n  \"description\": \"Service Level Indicators and Objectives monitoring\",\n  \"tags\": [\"slo\", \"sre\", \"reliability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-24h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 🎯 SLI/SLO Dashboard\\n\\n| Service | SLO Target | Error Budget (30d) |\\n|---------|------------|--------------------|\\n| Producer Success Rate | 99.9% | 43.2 min |\\n| Consumer Success Rate | 99.9% | 43.2 min |\\n| Latency P95 | < 100ms (Producer), < 5s (E2E) | - |\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"📊 ERROR BUDGET STATUS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 3 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 6, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Producer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Consumer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 15, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Budget Burn Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 18, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:consumed_percent\",\n          \"legendFormat\": \"Producer Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"error_budget:consumer:consumed_percent\",\n          \"legendFormat\": \"Consumer Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📈 SLI METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 10 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Consumer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"Producer Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"Database Availability\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.95 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Lag\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 500 },\n              { \"color\": \"red\", \"value\": 1000 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"SLI Trends Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"legendFormat\": \"Producer Success Rate\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"legendFormat\": \"Consumer Success Rate\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"99.9\",\n          \"legendFormat\": \"SLO Target (99.9%)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"min\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 99,\n          \"max\": 100\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"SLO Target (99.9%)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Burn Rate Trend\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"legendFormat\": \"Producer (1h)\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"legendFormat\": \"Consumer (1h)\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"1\",\n          \"legendFormat\": \"Normal Rate (1x)\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"14.4\",\n          \"legendFormat\": \"Critical (14.4x)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"x\",\n          \"min\": 0\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Normal Rate (1x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Critical (14.4x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📋 SLO SUMMARY TABLE\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 23 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"SLO Status\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 24 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"E\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true },\n            \"renameByName\": { \n              \"Value #A\": \"Producer Success %\",\n              \"Value #B\": \"Consumer Success %\",\n              \"Value #C\": \"Database Availability %\",\n              \"Value #D\": \"Producer Budget %\",\n              \"Value #E\": \"Consumer Budget %\"\n            }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"md\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"center\", \"displayMode\": \"color-background-solid\" },\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/infrastructure/container-metrics.json": "{\n  \"uid\": \"container-metrics\",\n  \"title\": \"Container Metrics\",\n  \"description\": \"Docker container resource usage monitoring\",\n  \"tags\": [\"infrastructure\", \"containers\", \"cadvisor\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"container\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(container_cpu_usage_seconds_total{name=~\\\".+\\\"}, name)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Running Containers\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(container_last_seen{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total CPU Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m])) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Memory Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(container_memory_usage_bytes{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 4294967296 }, { \"color\": \"red\", \"value\": 8589934592 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Network In\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_receive_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Network Out\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_transmit_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"CPU Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\"$container\\\"}[5m]) * 100\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Memory Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\"$container\\\"}\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Network I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_network_receive_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Receive\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_network_transmit_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Transmit\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Disk I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_fs_reads_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Read\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_fs_writes_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Write\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Container Resource Table\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m]) * 100\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"container_spec_memory_limit_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true, \"__name__\": true, \"id\": true, \"image\": true, \"instance\": true, \"job\": true },\n            \"renameByName\": { \"name\": \"Container\", \"Value #A\": \"CPU %\", \"Value #B\": \"Memory Used\", \"Value #C\": \"Memory Limit\" }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\",\n        \"footer\": { \"show\": true, \"reducer\": [\"sum\"], \"countRows\": false }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"CPU %\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"percent\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"max\", \"value\": 100 },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 80 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Used\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"bytes\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Limit\" }, \n            \"properties\": [{ \"id\": \"unit\", \"value\": \"bytes\" }] \n          }\n        ]\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/infrastructure/kafka.json": "{\n  \"uid\": \"kafka-dashboard\",\n  \"title\": \"Kafka Overview\",\n  \"description\": \"Monitoring Apache Kafka cluster health and performance\",\n  \"tags\": [\"infrastructure\", \"kafka\", \"messaging\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"topic\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(kafka_topic_partitions, topic)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Kafka Brokers Up\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_brokers)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total Topics\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Under Replicated Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_under_replicated_partition)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Offline Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_leader{leader=\\\"-1\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Consumer Groups\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(count by (consumergroup) (kafka_consumergroup_current_offset))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Messages In per Second (by Topic)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (topic) (rate(kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}[1m]))\",\n          \"legendFormat\": \"{{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Consumer Group Lag\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (consumergroup, topic) (kafka_consumergroup_lag{topic=~\\\"$topic\\\"})\",\n          \"legendFormat\": \"{{consumergroup}} - {{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1000 }, { \"color\": \"red\", \"value\": 10000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Topic Partitions\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partitions{topic=~\\\"$topic\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"organize\", \"options\": { \"excludeByName\": { \"Time\": true, \"__name__\": true, \"instance\": true, \"job\": true }, \"renameByName\": { \"topic\": \"Topic\", \"Value\": \"Partitions\" } } }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Partitions\" }, \"properties\": [{ \"id\": \"custom.displayMode\", \"value\": \"color-background\" }, { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Current Offset by Partition\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-{{partition}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"stepAfter\", \"fillOpacity\": 0, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Consumer Group Lag Sum\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 8, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"min\": 0,\n          \"max\": 100000,\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5000 }, { \"color\": \"red\", \"value\": 50000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Messages per Partition\",\n      \"type\": \"bargauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 16, \"x\": 8, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-p{{partition}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"orientation\": \"horizontal\",\n        \"displayMode\": \"gradient\",\n        \"showUnfilled\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/infrastructure/postgresql.json": "{\n  \"uid\": \"postgresql-dashboard\",\n  \"title\": \"PostgreSQL Overview\",\n  \"description\": \"Monitoring PostgreSQL database performance and health\",\n  \"tags\": [\"infrastructure\", \"postgresql\", \"database\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"PostgreSQL Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_up\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Database Size\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_database_size_bytes{datname=\\\"datawarehouse\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1073741824 }, { \"color\": \"red\", \"value\": 5368709120 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Active Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(pg_stat_activity_count{datname=\\\"datawarehouse\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 100 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Max Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_settings_max_connections\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - pg_postmaster_start_time_seconds\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Connections by State\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_activity_count{datname=\\\"datawarehouse\\\"}\",\n          \"legendFormat\": \"{{state}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\", \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Transactions per Second\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_xact_commit{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Commits\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_xact_rollback{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Rollbacks\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Rollbacks\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Rows Operations\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_tup_inserted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Inserted\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_tup_updated{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Updated\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"rate(pg_stat_database_tup_deleted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Deleted\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"rate(pg_stat_database_tup_fetched{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Fetched\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"rowsps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Cache Hit Ratio\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} / (pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} + pg_stat_database_blks_read{datname=\\\"datawarehouse\\\"}) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 80 }, { \"color\": \"green\", \"value\": 95 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Deadlocks\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_deadlocks{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Temp Files Created\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 16 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_temp_files{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 10 }, { \"color\": \"red\", \"value\": 50 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/logs/logs-explorer.json": "{\n  \"uid\": \"logs-dashboard\",\n  \"title\": \"Logs Explorer\",\n  \"description\": \"Centralized logs from all services\",\n  \"tags\": [\"logs\", \"loki\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"level\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"search\",\n        \"type\": \"textbox\",\n        \"current\": { \"value\": \"\" },\n        \"label\": \"Search\"\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 5, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"fixed\", \"fixedColor\": \"red\" }\n        },\n        \"overrides\": []\n      }\n    },\n     {\n      \"id\": 3,\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | json | level=~\\\".+\\\" | __error__=\\\"\\\" [$__range]))\",\n          \"legendFormat\": \"{{level}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"palette-classic\" },\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"INFO\": { \"color\": \"green\", \"index\": 0 },\n                \"info\": { \"color\": \"green\", \"index\": 1 },\n                \"ERROR\": { \"color\": \"red\", \"index\": 2 },\n                \"error\": { \"color\": \"red\", \"index\": 3 },\n                \"WARNING\": { \"color\": \"yellow\", \"index\": 4 },\n                \"warning\": { \"color\": \"yellow\", \"index\": 5 },\n                \"WARN\": { \"color\": \"yellow\", \"index\": 6 },\n                \"warn\": { \"color\": \"yellow\", \"index\": 7 },\n                \"DEBUG\": { \"color\": \"blue\", \"index\": 8 },\n                \"debug\": { \"color\": \"blue\", \"index\": 9 }\n              }\n            }\n          ]\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 18, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"red\", \"value\": 10 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 11 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"All Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 15, \"w\": 24, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"dedupStrategy\": \"none\",\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 12, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 38 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Order Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 32,\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/logs/test.json": "{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"description\": \"Centralized logs from all services\",\n  \"editable\": true,\n  \"fiscalYearStartMonth\": 0,\n  \"graphTooltip\": 0,\n  \"id\": 12,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"normal\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 1,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [\n            \"sum\"\n          ],\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"fixedColor\": \"red\",\n            \"mode\": \"fixed\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 70,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"none\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 6\n      },\n      \"id\": 2,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [],\n          \"displayMode\": \"list\",\n          \"placement\": \"bottom\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            }\n          },\n          \"mappings\": [],\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"ERROR\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"red\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"WARNING\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"yellow\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"INFO\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"green\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"DEBUG\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"blue\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          }\n        ]\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 6\n      },\n      \"id\": 3,\n      \"options\": {\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"values\": [\n            \"value\",\n            \"percent\"\n          ]\n        },\n        \"pieType\": \"donut\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"sort\": \"desc\",\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"single\",\n          \"sort\": \"none\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"direction\": \"backward\",\n          \"editorMode\": \"code\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | __error__=\\\"\\\" [$__range]))\",\n          \"instant\": true,\n          \"legendFormat\": \"{{level}}\",\n          \"queryType\": \"range\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [],\n          \"noValue\": \"0\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"yellow\",\n                \"value\": 1\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 10\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 6\n      },\n      \"id\": 4,\n      \"options\": {\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"orientation\": \"auto\",\n        \"percentChangeColorMode\": \"standard\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"showPercentChange\": false,\n        \"textMode\": \"auto\",\n        \"wideLayout\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 11\n      },\n      \"id\": 10,\n      \"panels\": [],\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 15,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"id\": 11,\n      \"options\": {\n        \"dedupStrategy\": \"none\",\n        \"enableInfiniteScrolling\": false,\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showCommonLabels\": false,\n        \"showControls\": false,\n        \"showLabels\": true,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"All Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 27\n      },\n      \"id\": 20,\n      \"panels\": [],\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 28\n      },\n      \"id\": 21,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 28\n      },\n      \"id\": 22,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 38\n      },\n      \"id\": 30,\n      \"panels\": [],\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 39\n      },\n      \"id\": 31,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Order Events\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 39\n      },\n      \"id\": 32,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\"\n    }\n  ],\n  \"preload\": false,\n  \"refresh\": \"10s\",\n  \"schemaVersion\": 42,\n  \"tags\": [\n    \"logs\",\n    \"loki\",\n    \"observability\"\n  ],\n  \"templating\": {\n    \"list\": [\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"service\",\n        \"options\": [],\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"level\",\n        \"options\": [],\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"value\": \"\"\n        },\n        \"label\": \"Search\",\n        \"name\": \"search\",\n        \"type\": \"textbox\"\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {},\n  \"timezone\": \"browser\",\n  \"title\": \"Logs Explorer\",\n  \"uid\": \"logs-dashboard\",\n  \"version\": 3\n}",
    "monitoring/grafana/dashboards/overview/correlation-dashboard.json": "{\n  \"uid\": \"correlation-dashboard\",\n  \"title\": \"Metrics & Logs Correlation\",\n  \"description\": \"View metrics and related logs side by side\",\n  \"tags\": [\"correlation\", \"metrics\", \"logs\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"name\": \"Errors\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"enable\": true,\n        \"iconColor\": \"red\",\n        \"expr\": \"{service=\\\"$service\\\"} |= \\\"ERROR\\\"\",\n        \"titleFormat\": \"Error\",\n        \"textFormat\": \"{{ __line__ }}\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"📊 Metrics Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Message Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"msg/min\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byRegexp\", \"options\": \"/Error/\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Latency P95\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Producer P95\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Consumer E2E P95\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 8 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Log Volume (matches time range above)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 4, \"w\": 24, \"x\": 0, \"y\": 9 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=\\\"$service\\\"} | json | __error__=\\\"\\\" [$__interval]))\",\n          \"legendFormat\": \"{{level}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"right\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"ERROR\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"INFO\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"WARNING\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Service Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 13 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"}\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔴 Error Logs Only\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 25 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Error Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 26 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"} |~ \\\"(?i)error|ERROR\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/overview/system-overview.json": "{\n  \"uid\": \"system-overview\",\n  \"title\": \"System Overview\",\n  \"description\": \"Overview of all monitored services\",\n  \"tags\": [\"overview\", \"prometheus\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Services Health\",\n      \"description\": \"Status of all monitored services (1 = UP, 0 = DOWN)\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 4,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up\",\n          \"legendFormat\": \"{{job}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"orientation\": \"horizontal\",\n        \"textMode\": \"auto\",\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"0\": {\n                  \"text\": \"DOWN\",\n                  \"color\": \"red\",\n                  \"index\": 0\n                },\n                \"1\": {\n                  \"text\": \"UP\",\n                  \"color\": \"green\",\n                  \"index\": 1\n                }\n              }\n            }\n          ],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"green\", \"value\": 1 }\n            ]\n          },\n          \"unit\": \"none\"\n        },\n        \"overrides\": []\n      }\n    },\n    \n    {\n      \"id\": 2,\n      \"title\": \"Prometheus Scrape Duration\",\n      \"description\": \"Time taken to scrape each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_duration_seconds\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"line\",\n            \"lineInterpolation\": \"smooth\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"scheme\",\n            \"spanNulls\": false,\n            \"lineWidth\": 2,\n            \"pointSize\": 5,\n            \"showPoints\": \"auto\"\n          },\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 0.5 },\n              { \"color\": \"red\", \"value\": 1 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Samples Scraped\",\n      \"description\": \"Number of metrics scraped from each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_samples_scraped\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"hue\",\n            \"lineWidth\": 1\n          },\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Prometheus Memory Usage\",\n      \"type\": \"gauge\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"process_resident_memory_bytes{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"min\": 0,\n          \"max\": 1073741824,\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 536870912 },\n              { \"color\": \"red\", \"value\": 858993459 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Prometheus Storage Size\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 6,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_storage_blocks_bytes\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 5368709120 },\n              { \"color\": \"red\", \"value\": 10737418240 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Total Time Series\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_head_series\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - process_start_time_seconds{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}",
    "monitoring/grafana/dashboards/tracing/tracing-overview.json": "{\n  \"uid\": \"tracing-overview\",\n  \"title\": \"Distributed Tracing Overview\",\n  \"description\": \"Jaeger traces visualization and analysis\",\n  \"tags\": [\"tracing\", \"jaeger\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      },\n      {\n        \"name\": \"operation\",\n        \"type\": \"custom\",\n        \"query\": \"all,produce_order,produce_batch,consume_message,process_batch,db_insert_orders\",\n        \"includeAll\": true,\n        \"current\": { \"text\": \"All\", \"value\": \"$__all\" },\n        \"options\": [\n          { \"text\": \"All\", \"value\": \"$__all\", \"selected\": true },\n          { \"text\": \"produce_order\", \"value\": \"produce_order\", \"selected\": false },\n          { \"text\": \"produce_batch\", \"value\": \"produce_batch\", \"selected\": false },\n          { \"text\": \"consume_message\", \"value\": \"consume_message\", \"selected\": false },\n          { \"text\": \"process_batch\", \"value\": \"process_batch\", \"selected\": false },\n          { \"text\": \"db_insert_orders\", \"value\": \"db_insert_orders\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🔍 Tracing Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Trace Count\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[5m])) * 300\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Avg Duration (Producer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Avg Duration (Consumer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"E2E Latency P95\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 10 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Services\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(up{job=~\\\"data-producer|data-consumer\\\"} == 1)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📊 Latency Distribution\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 5 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"End-to-End Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔎 Trace Search\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 14 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Recent Traces\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"jaeger\", \"uid\": \"jaeger\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"queryType\": \"search\",\n          \"service\": \"$service\",\n          \"limit\": 20\n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\" }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Application Logs (with trace_id)\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 24, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} | json | __error__=\\\"\\\" | trace_id =~ \\\".+\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}",
    "monitoring/grafana/provisioning/dashboards/dashboards.yml": "# monitoring/grafana/provisioning/dashboards/dashboards.yml\n\napiVersion: 1\n\nproviders:\n  # ===========================================\n  # Overview Dashboards\n  # ===========================================\n  - name: 'overview'\n    orgId: 1\n    folder: 'Overview'\n    folderUid: 'overview'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: true              # ✅  cho edit trên UI\n    options:\n      path: /var/lib/grafana/dashboards/overview\n\n  # ===========================================\n  # Infrastructure Dashboards\n  # ===========================================\n  - name: 'infrastructure'\n    orgId: 1\n    folder: 'Infrastructure'\n    folderUid: 'infrastructure'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/infrastructure\n\n  # ===========================================\n  # Application Dashboards\n  # ===========================================\n  - name: 'applications'\n    orgId: 1\n    folder: 'Applications'\n    folderUid: 'applications'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/applications\n\n  # ===========================================\n  # Logs Dashboards\n  # ===========================================\n  - name: 'logs'\n    orgId: 1\n    folder: 'Logs'\n    folderUid: 'logs'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/logs\n\n  - name: 'tracing'\n    orgId: 1\n    folder: 'Tracing'\n    folderUid: 'tracing'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/tracing",
    "monitoring/grafana/provisioning/datasources/datasources.yml": "# monitoring/grafana/provisioning/datasources/datasources.yml\n\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    uid: prometheus                    # ✅ UID cố định - RẤT QUAN TRỌNG!\n    access: proxy\n    url: http://prometheus:9090\n    isDefault: true\n    editable: true                    # Không cho edit trên UI để tránh drift\n    jsonData:\n      timeInterval: \"15s\"\n      httpMethod: POST                 # POST tốt hơn cho large queries\n\n  - name: Alertmanager\n    type: alertmanager\n    uid: alertmanager\n    access: proxy\n    url: http://alertmanager:9093\n    editable: false\n    jsonData:\n      implementation: prometheus\n\n  - name: Loki\n    type: loki\n    uid: loki\n    access: proxy\n    url: http://loki:3100\n    editable: false\n    jsonData:\n      timeout: 60\n      maxLines: 1000\n      derivedFields:\n        # Link từ logs sang related logs by order_id\n        - name: order_id\n          matcherRegex: '\"order_id\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"loki\",\"queries\":[{\"expr\":\"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"$${__value.raw}\\\"\"}]}'\n          datasourceUid: loki\n          urlDisplayLabel: \"View related logs\"\n        \n        # Link từ logs sang metrics by service\n        - name: service_metrics\n          matcherRegex: '\"service\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"prometheus\",\"queries\":[{\"expr\":\"up{job=\\\"$${__value.raw}\\\"}\"}]}'\n          datasourceUid: prometheus\n          urlDisplayLabel: \"View metrics\"\n\n        - name: TraceID\n          matcherRegex: '\"trace_id\":\\s*\"([a-f0-9]+)\"'\n          url: '$${__value.raw}'\n          datasourceUid: jaeger\n          urlDisplayLabel: \"View Trace\"\n\n  - name: Jaeger\n    type: jaeger\n    uid: jaeger\n    access: proxy\n    url: http://jaeger:16686\n    editable: false\n    jsonData:\n      tracesToLogs:\n        datasourceUid: loki\n        tags: ['service']\n        mappedTags: [{ key: 'service.name', value: 'service' }]\n        mapTagNamesEnabled: true\n        spanStartTimeShift: '-1h'\n        spanEndTimeShift: '1h'\n        filterByTraceID: true\n        filterBySpanID: false\n        lokiSearch: true",
    "monitoring/loki/rules/fake/alerts.yml": "# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"",
    "monitoring/loki/rules/alerts.yml": "# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"",
    "monitoring/loki/loki-config.yml": "# monitoring/loki/loki-config.yml\n\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n  grpc_listen_port: 9096\n  log_level: info\n\ncommon:\n  instance_addr: 127.0.0.1\n  path_prefix: /loki\n  storage:\n    filesystem:\n      chunks_directory: /loki/chunks\n      rules_directory: /loki/rules\n  replication_factor: 1\n  ring:\n    kvstore:\n      store: inmemory\n\nquery_range:\n  results_cache:\n    cache:\n      embedded_cache:\n        enabled: true\n        max_size_mb: 100\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: tsdb\n      object_store: filesystem\n      schema: v13\n      index:\n        prefix: index_\n        period: 24h\n\nstorage_config:\n  filesystem:\n    directory: /loki/storage\n\nlimits_config:\n  retention_period: 168h  # 7 days\n  ingestion_rate_mb: 10\n  ingestion_burst_size_mb: 20\n  max_streams_per_user: 10000\n  max_line_size: 256kb\n\ncompactor:\n  working_directory: /loki/compactor\n  compaction_interval: 10m\n  retention_enabled: true\n  retention_delete_delay: 2h\n  delete_request_store: filesystem\n\nruler:\n  alertmanager_url: http://alertmanager:9093\n  storage:\n    type: local\n    local:\n      directory: /loki/rules\n  rule_path: /loki/rules-temp\n  enable_api: true\n  enable_alertmanager_v2: true\n  ring:\n    kvstore:\n      store: inmemory\n\nanalytics:\n  reporting_enabled: false",
    "monitoring/prometheus/rules/alert_rules.yml": "# monitoring/prometheus/rules/alert_rules.yml\n\ngroups:\n  # ===========================================\n  # Service Health Alerts\n  # ===========================================\n  - name: service_health_alerts\n    rules:\n      # Service Down - Critical\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"{{ $labels.job }} has been down for more than 1 minute.\"\n          runbook_url: \"https://wiki.example.com/runbook/service-down\"\n\n      # Service Flapping - Warning\n      - alert: ServiceFlapping\n        expr: changes(up[10m]) > 3\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Service {{ $labels.job }} is flapping\"\n          description: \"{{ $labels.job }} has changed state {{ $value }} times in the last 10 minutes.\"\n\n  # ===========================================\n  # Data Pipeline Alerts\n  # ===========================================\n  - name: pipeline_alerts\n    rules:\n      # Producer Down\n      - alert: ProducerDown\n        expr: up{job=\"data-producer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n        annotations:\n          summary: \"Data Producer is down\"\n          description: \"Data Producer has been down for more than 1 minute. No data is being produced to Kafka.\"\n\n      # Consumer Down\n      - alert: ConsumerDown\n        expr: up{job=\"data-consumer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: consumer\n        annotations:\n          summary: \"Data Consumer is down\"\n          description: \"Data Consumer has been down for more than 1 minute. No data is being processed.\"\n\n      # Producer Not Producing\n      - alert: ProducerNotProducing\n        expr: rate(pipeline_messages_produced_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer is not producing messages\"\n          description: \"No messages have been produced in the last 5 minutes.\"\n\n      # Consumer Not Consuming\n      - alert: ConsumerNotConsuming\n        expr: rate(pipeline_messages_consumed_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer is not consuming messages\"\n          description: \"No messages have been consumed in the last 5 minutes.\"\n\n      # High Error Rate - Producer\n      - alert: ProducerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_produced_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_produced_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer error rate is high\"\n          description: \"Producer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Error Rate - Consumer\n      - alert: ConsumerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_consumed_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_consumed_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer error rate is high\"\n          description: \"Consumer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Processing Latency\n      - alert: HighProcessingLatency\n        expr: pipeline:processing_latency_p95:seconds > 1\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Processing latency is high\"\n          description: \"P95 processing latency is {{ $value | printf \\\"%.2f\\\" }}s (threshold: 1s).\"\n\n  # ===========================================\n  # Kafka Alerts\n  # ===========================================\n  - name: kafka_alerts\n    rules:\n      # Kafka Down\n      - alert: KafkaDown\n        expr: up{job=\"kafka\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka exporter is down\"\n          description: \"Cannot scrape Kafka metrics. Kafka might be down.\"\n\n      # High Consumer Lag\n      - alert: KafkaHighConsumerLag\n        expr: kafka:consumer_lag:sum > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is high\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 1000).\"\n\n      # Critical Consumer Lag\n      - alert: KafkaCriticalConsumerLag\n        expr: kafka:consumer_lag:sum > 10000\n        for: 5m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is critical\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 10000). Consumer might be stuck.\"\n\n      # No Messages Flowing\n      - alert: KafkaNoMessagesFlowing\n        expr: kafka:messages_in:rate1m:total == 0\n        for: 10m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"No messages flowing through Kafka\"\n          description: \"No messages have been produced to Kafka in the last 10 minutes.\"\n\n  # ===========================================\n  # PostgreSQL Alerts\n  # ===========================================\n  - name: postgresql_alerts\n    rules:\n      # PostgreSQL Down\n      - alert: PostgreSQLDown\n        expr: pg_up == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL is down\"\n          description: \"PostgreSQL database is not responding.\"\n\n      # High Connection Usage\n      - alert: PostgreSQLHighConnections\n        expr: postgresql:connections:usage_percent > 80\n        for: 5m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is high\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical Connection Usage\n      - alert: PostgreSQLCriticalConnections\n        expr: postgresql:connections:usage_percent > 95\n        for: 2m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is critical\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%). New connections may fail.\"\n\n      # Low Cache Hit Ratio\n      - alert: PostgreSQLLowCacheHitRatio\n        expr: postgresql:cache_hit_ratio:percent < 90\n        for: 10m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL cache hit ratio is low\"\n          description: \"Cache hit ratio is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 90%). Consider increasing shared_buffers.\"\n\n      # Deadlocks Detected\n      - alert: PostgreSQLDeadlocks\n        expr: increase(pg_stat_database_deadlocks{datname=\"datawarehouse\"}[5m]) > 0\n        for: 0m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL deadlocks detected\"\n          description: \"{{ $value }} deadlocks detected in the last 5 minutes.\"\n\n  # ===========================================\n  # Container Resource Alerts (Fixed)\n  # ===========================================\n  - name: container_alerts\n    rules:\n      # High CPU Usage\n      - alert: ContainerHighCPU\n        expr: container:cpu_usage_percent:rate5m > 80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical CPU Usage\n      - alert: ContainerCriticalCPU\n        expr: container:cpu_usage_percent:rate5m > 95\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%).\"\n\n      # High Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerHighMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 80\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 80%).\"\n\n      # Critical Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerCriticalMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 95\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 95%). OOM kill imminent.\"\n\n      # High Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerHighMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 1073741824\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} using high memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 1GB).\"\n\n      # Critical Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerCriticalMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 2147483648\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} using critical memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 2GB).\"\n\n      # Container Restarting\n      - alert: ContainerRestarting\n        expr: increase(container_restart_count{name=~\".+\"}[1h]) > 3\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} is restarting frequently\"\n          description: \"Container has restarted {{ $value | printf \\\"%.0f\\\" }} times in the last hour.\"",
    "monitoring/prometheus/rules/recording_rules.yml": "# monitoring/prometheus/rules/recording_rules.yml\n\ngroups:\n  # ===========================================\n  # Container Metrics - Pre-computed\n  # ===========================================\n  - name: container_metrics\n    interval: 15s\n    rules:\n      - record: container:cpu_usage_percent:rate5m\n        expr: rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]) * 100\n\n      - record: container:memory_usage_bytes:current\n        expr: container_memory_usage_bytes{name=~\".+\"}\n\n      - record: container:memory_usage_percent:current\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} / \n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n\n      - record: container:network_receive_bytes:rate5m\n        expr: rate(container_network_receive_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:network_transmit_bytes:rate5m\n        expr: rate(container_network_transmit_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:cpu_usage_percent:total\n        expr: sum(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m])) * 100\n\n      - record: container:memory_usage_bytes:total\n        expr: sum(container_memory_usage_bytes{name=~\".+\"})\n\n      - record: container:count:total\n        expr: count(container_last_seen{name=~\".+\"})\n\n  # ===========================================\n  # RED Metrics - Producer\n  # ===========================================\n  - name: red_producer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:producer:rate_per_second\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:producer:rate_per_minute\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:producer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_producer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:producer:errors_per_minute\n        expr: sum(rate(pipeline_producer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:producer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n  # ===========================================\n  # RED Metrics - Consumer\n  # ===========================================\n  - name: red_consumer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:consumer:rate_per_second\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:consumer:rate_per_minute\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m])) * 60\n\n      - record: red:consumer:db_inserts_per_minute\n        expr: sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:consumer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_consumer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:consumer:errors_per_minute\n        expr: sum(rate(pipeline_consumer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:consumer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      # End-to-End Latency\n      - record: red:e2e:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n  # ===========================================\n  # Business Metrics\n  # ===========================================\n  - name: business_metrics\n    interval: 15s\n    rules:\n      - record: business:orders:rate_per_minute\n        expr: sum(rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue:rate_per_minute\n        expr: sum(rate(pipeline_business_revenue_total[1m])) * 60\n\n      - record: business:avg_order_value\n        expr: |\n          sum(rate(pipeline_business_revenue_total[5m])) /\n          (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\n\n      - record: business:orders_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\n\n  # ===========================================\n  # Kafka Metrics\n  # ===========================================\n  - name: kafka_metrics\n    interval: 15s\n    rules:\n      - record: kafka:messages_in:rate1m\n        expr: sum by (topic) (rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:messages_in:rate1m:total\n        expr: sum(rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:consumer_lag:total\n        expr: sum by (consumergroup, topic) (kafka_consumergroup_lag)\n\n      - record: kafka:consumer_lag:sum\n        expr: sum(kafka_consumergroup_lag)\n\n      - record: kafka:partitions:count\n        expr: sum(kafka_topic_partitions)\n\n  # ===========================================\n  # PostgreSQL Metrics\n  # ===========================================\n  - name: postgresql_metrics\n    interval: 15s\n    rules:\n      - record: postgresql:transactions:rate1m\n        expr: rate(pg_stat_database_xact_commit{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_inserted:rate1m\n        expr: rate(pg_stat_database_tup_inserted{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_fetched:rate1m\n        expr: rate(pg_stat_database_tup_fetched{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:connections:active\n        expr: sum(pg_stat_activity_count{datname=\"datawarehouse\"})\n\n      - record: postgresql:connections:usage_percent\n        expr: |\n          (sum(pg_stat_activity_count{datname=\"datawarehouse\"}) / \n           pg_settings_max_connections) * 100\n\n      - record: postgresql:cache_hit_ratio:percent\n        expr: |\n          (\n            pg_stat_database_blks_hit{datname=\"datawarehouse\"} /\n            (pg_stat_database_blks_hit{datname=\"datawarehouse\"} + \n             pg_stat_database_blks_read{datname=\"datawarehouse\"} + 0.001)\n          ) * 100\n\n      - record: postgresql:database_size:bytes\n        expr: pg_database_size_bytes{datname=\"datawarehouse\"}\n\n  # ===========================================\n  # Service Health\n  # ===========================================\n  - name: service_health\n    interval: 15s\n    rules:\n      - record: service:up:status\n        expr: up\n\n      - record: service:healthy:count\n        expr: count(up == 1)\n\n      - record: service:unhealthy:count\n        expr: count(up == 0)\n\n      - record: service:health:percent\n        expr: (count(up == 1) / count(up)) * 100",
    "monitoring/prometheus/rules/slo_alerts.yml": "# monitoring/prometheus/rules/slo_alerts.yml\n\ngroups:\n  # ===========================================\n  # Error Budget Alerts\n  # ===========================================\n  - name: error_budget_alerts\n    rules:\n      # Producer error budget low\n      - alert: ProducerErrorBudgetLow\n        expr: error_budget:producer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is running low\"\n          description: \"Producer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      # Producer error budget critical\n      - alert: ProducerErrorBudgetCritical\n        expr: error_budget:producer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is critically low\"\n          description: \"Producer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining. Consider freezing deployments.\"\n\n      # Producer error budget exhausted\n      - alert: ProducerErrorBudgetExhausted\n        expr: error_budget:producer:remaining_percent <= 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget exhausted!\"\n          description: \"Producer has exhausted its error budget. SLO is being violated.\"\n\n      # Consumer error budget alerts\n      - alert: ConsumerErrorBudgetLow\n        expr: error_budget:consumer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is running low\"\n          description: \"Consumer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      - alert: ConsumerErrorBudgetCritical\n        expr: error_budget:consumer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is critically low\"\n          description: \"Consumer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining.\"\n\n  # ===========================================\n  # Burn Rate Alerts (Multi-window)\n  # ===========================================\n  - name: burn_rate_alerts\n    rules:\n      # Fast burn - will exhaust budget in ~2 hours\n      # 1h window with 14.4x burn rate\n      - alert: ProducerHighBurnRate\n        expr: |\n          burn_rate:producer:1h > 14.4\n          and\n          burn_rate:producer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer is burning error budget too fast\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. At this rate, error budget will be exhausted in ~2 hours.\"\n\n      # Slow burn - will exhaust budget in ~1 day\n      - alert: ProducerModerateBurnRate\n        expr: |\n          burn_rate:producer:6h > 6\n          and\n          burn_rate:producer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer burn rate is elevated\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. Error budget may be exhausted within a day.\"\n\n      # Consumer burn rate alerts\n      - alert: ConsumerHighBurnRate\n        expr: |\n          burn_rate:consumer:1h > 14.4\n          and\n          burn_rate:consumer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer is burning error budget too fast\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n      - alert: ConsumerModerateBurnRate\n        expr: |\n          burn_rate:consumer:6h > 6\n          and\n          burn_rate:consumer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer burn rate is elevated\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n  # ===========================================\n  # SLO Violation Alerts\n  # ===========================================\n  - name: slo_violation_alerts\n    rules:\n      # Producer SLO violations\n      - alert: ProducerSLOViolation\n        expr: sli:producer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer SLO is being violated\"\n          description: \"Producer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ProducerLatencySLOViolation\n        expr: sli:producer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of requests are under 100ms (target: 99%)\"\n\n      # Consumer SLO violations\n      - alert: ConsumerSLOViolation\n        expr: sli:consumer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer SLO is being violated\"\n          description: \"Consumer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ConsumerLatencySLOViolation\n        expr: sli:consumer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer E2E latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of messages are processed under 5s (target: 99%)\"\n\n      # Data freshness SLO\n      - alert: DataFreshnessSLOViolation\n        expr: sum(kafka_consumergroup_lag) > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: pipeline\n          category: slo\n        annotations:\n          summary: \"Data freshness SLO is being violated\"\n          description: \"Consumer lag is {{ $value }} messages (threshold: 1000)\"",
    "monitoring/prometheus/rules/slo_rules.yml": "# monitoring/prometheus/rules/slo_rules.yml\n\ngroups:\n  # ===========================================\n  # SLI Definitions\n  # ===========================================\n  - name: sli_metrics\n    interval: 30s\n    rules:\n      # --- Producer SLIs ---\n      \n      # Availability SLI: % of time producer is up\n      - record: sli:producer:availability\n        expr: avg_over_time(up{job=\"data-producer\"}[5m])\n\n      # Success Rate SLI: % of successful messages\n      - record: sli:producer:success_rate\n        expr: |\n          sum(rate(pipeline_producer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of requests under threshold (100ms)\n      - record: sli:producer:latency_good\n        expr: |\n          sum(rate(pipeline_producer_message_duration_seconds_bucket{le=\"0.1\"}[5m])) /\n          (sum(rate(pipeline_producer_message_duration_seconds_count[5m])) + 0.001)\n\n      # --- Consumer SLIs ---\n      \n      # Availability SLI\n      - record: sli:consumer:availability\n        expr: avg_over_time(up{job=\"data-consumer\"}[5m])\n\n      # Success Rate SLI\n      - record: sli:consumer:success_rate\n        expr: |\n          sum(rate(pipeline_consumer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of E2E latency under threshold (5s)\n      - record: sli:consumer:latency_good\n        expr: |\n          sum(rate(pipeline_consumer_end_to_end_latency_seconds_bucket{le=\"5.0\"}[5m])) /\n          (sum(rate(pipeline_consumer_end_to_end_latency_seconds_count[5m])) + 0.001)\n\n      # --- Pipeline SLIs ---\n      \n      # Data Freshness SLI: Consumer lag < 1000 messages\n      - record: sli:pipeline:data_freshness\n        expr: |\n          (sum(kafka_consumergroup_lag) < 1000) or vector(0)\n\n      # --- Database SLIs ---\n      \n      # Availability SLI\n      - record: sli:database:availability\n        expr: avg_over_time(pg_up[5m])\n\n      # Query Success Rate\n      - record: sli:database:query_success_rate\n        expr: |\n          sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_db_operations_total[5m])) + 0.001)\n\n  # ===========================================\n  # SLO Calculations (Rolling Windows)\n  # ===========================================\n  - name: slo_calculations\n    interval: 1m\n    rules:\n      # --- Producer SLOs ---\n      \n      # 30-day rolling availability (target: 99.9%)\n      - record: slo:producer:availability_30d\n        expr: avg_over_time(sli:producer:availability[30d])\n\n      # 30-day rolling success rate (target: 99.9%)\n      - record: slo:producer:success_rate_30d\n        expr: avg_over_time(sli:producer:success_rate[30d])\n\n      # 30-day rolling latency compliance (target: 99%)\n      - record: slo:producer:latency_compliance_30d\n        expr: avg_over_time(sli:producer:latency_good[30d])\n\n      # --- Consumer SLOs ---\n      \n      - record: slo:consumer:availability_30d\n        expr: avg_over_time(sli:consumer:availability[30d])\n\n      - record: slo:consumer:success_rate_30d\n        expr: avg_over_time(sli:consumer:success_rate[30d])\n\n      - record: slo:consumer:latency_compliance_30d\n        expr: avg_over_time(sli:consumer:latency_good[30d])\n\n      # --- Database SLOs ---\n      \n      - record: slo:database:availability_30d\n        expr: avg_over_time(sli:database:availability[30d])\n\n      - record: slo:database:query_success_rate_30d\n        expr: avg_over_time(sli:database:query_success_rate[30d])\n\n  # ===========================================\n  # Error Budget Calculations\n  # ===========================================\n  - name: error_budget\n    interval: 1m\n    rules:\n      # --- Producer Error Budget ---\n      \n      # Error budget remaining (target 99.9% = 0.1% budget)\n      # Formula: (SLO - (1 - current_success_rate)) / (1 - SLO)\n      - record: error_budget:producer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:producer:success_rate)) / 0.001\n          ) * 100\n\n      # Error budget consumed\n      - record: error_budget:producer:consumed_percent\n        expr: 100 - error_budget:producer:remaining_percent\n\n      # --- Consumer Error Budget ---\n      \n      - record: error_budget:consumer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:consumer:success_rate)) / 0.001\n          ) * 100\n\n      - record: error_budget:consumer:consumed_percent\n        expr: 100 - error_budget:consumer:remaining_percent\n\n      # --- Combined Pipeline Error Budget ---\n      \n      - record: error_budget:pipeline:remaining_percent\n        expr: |\n          (\n            error_budget:producer:remaining_percent + \n            error_budget:consumer:remaining_percent\n          ) / 2\n\n  # ===========================================\n  # Burn Rate Calculations\n  # ===========================================\n  - name: burn_rate\n    interval: 1m\n    rules:\n      # Burn rate = actual error rate / allowed error rate\n      # If burn rate > 1, we're consuming budget faster than allowed\n      \n      # Producer burn rate (1h window)\n      - record: burn_rate:producer:1h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[1h])) / 0.001\n\n      # Producer burn rate (6h window)\n      - record: burn_rate:producer:6h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[6h])) / 0.001\n\n      # Consumer burn rate (1h window)\n      - record: burn_rate:consumer:1h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[1h])) / 0.001\n\n      # Consumer burn rate (6h window)\n      - record: burn_rate:consumer:6h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[6h])) / 0.001",
    "monitoring/prometheus/prometheus.yml": "# monitoring/prometheus/prometheus.yml\n\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    monitor: 'data-pipeline-monitor'\n\notlp:\n  promote_resource_attributes:\n    - service.name\n    - service.namespace\n    - service.instance.id\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n            - alertmanager:9093\n\nrule_files:\n  - /etc/prometheus/rules/*.yml\n\nscrape_configs:\n  # ===========================================\n  # Monitoring Stack\n  # ===========================================\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n        labels:\n          service: 'prometheus'\n          layer: 'monitoring'\n\n  - job_name: 'grafana'\n    static_configs:\n      - targets: ['grafana:3000']\n        labels:\n          service: 'grafana'\n          layer: 'monitoring'\n\n  - job_name: 'alertmanager'\n    static_configs:\n      - targets: ['alertmanager:9093']\n        labels:\n          service: 'alertmanager'\n          layer: 'monitoring'\n\n  - job_name: 'cadvisor'\n    static_configs:\n      - targets: ['cadvisor:8080']\n        labels:\n          service: 'cadvisor'\n          layer: 'monitoring'\n    metric_relabel_configs:\n      # Giữ lại metrics của containers trong docker-compose project\n      - source_labels: [container_label_com_docker_compose_project]\n        regex: '.+'\n        action: keep\n      # Chỉ drop các metrics không cần thiết (bỏ container_last_seen khỏi list)\n      - source_labels: [__name__]\n        regex: 'container_(tasks_state|memory_failures_total)'\n        action: drop\n\n  # ===========================================\n  # Infrastructure\n  # ===========================================\n  - job_name: 'postgresql'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n        labels:\n          service: 'postgresql'\n          layer: 'infrastructure'\n          database: 'datawarehouse'\n\n  - job_name: 'kafka'\n    static_configs:\n      - targets: ['kafka-exporter:9308']\n        labels:\n          service: 'kafka'\n          layer: 'infrastructure'\n\n  # ===========================================\n  # Applications\n  # ===========================================\n  - job_name: 'data-producer'\n    static_configs:\n      - targets: ['data-producer:8000']\n        labels:\n          service: 'producer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  - job_name: 'data-consumer'\n    static_configs:\n      - targets: ['data-consumer:8001']\n        labels:\n          service: 'consumer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  # ===========================================\n  # stress test\n  # ===========================================\n  - job_name: 'load-test'\n    static_configs:\n      - targets: ['load-test:8002']\n        labels:\n          service: 'load-test'\n          layer: 'testing'\n    scrape_interval: 5s",
    "monitoring/promtail/promtail-config.yml": "# monitoring/promtail/promtail-config.yml\n\nserver:\n  http_listen_port: 9080\n  grpc_listen_port: 0\n  log_level: info\n\npositions:\n  filename: /tmp/positions.yaml\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n    tenant_id: fake\n\nscrape_configs:\n  # ===========================================\n  # Docker Container Logs\n  # ===========================================\n  - job_name: docker\n    docker_sd_configs:\n      - host: unix:///var/run/docker.sock\n        refresh_interval: 5s\n    relabel_configs:\n      # Lấy container name làm label\n      - source_labels: ['__meta_docker_container_name']\n        regex: '/(.*)'\n        target_label: 'container'\n      \n      # Lấy compose service name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'service'\n      \n      # Lấy compose project name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        target_label: 'project'\n      \n      # Lấy container ID\n      - source_labels: ['__meta_docker_container_id']\n        target_label: 'container_id'\n      \n      # Thêm job label\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'job'\n      \n      # Filter: chỉ lấy logs từ containers có compose project\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        regex: '.+'\n        action: keep\n\n    pipeline_stages:\n      # ===========================================\n      # Parse JSON logs (từ Producer/Consumer)\n      # ===========================================\n      - match:\n          selector: '{service=~\"data-producer|data-consumer\"}'\n          stages:\n            - json:\n                expressions:\n                  level: level\n                  message: message\n                  event: event\n                  service: service\n                  timestamp: timestamp\n                  order_id: order_id\n                  category: category\n                  error: error\n                  error_type: error_type\n                  batch_size: batch_size\n                  duration_ms: duration_ms\n                  throughput_per_sec: throughput_per_sec\n            \n            # Set log level as label\n            - labels:\n                level:\n                event:\n            \n            # Set timestamp from log\n            - timestamp:\n                source: timestamp\n                format: RFC3339Nano\n                fallback_formats:\n                  - RFC3339\n            \n            # Extract metrics from logs (optional)\n            - metrics:\n                log_lines_total:\n                  type: Counter\n                  description: \"Total log lines\"\n                  source: message\n                  config:\n                    action: inc\n                    match_all: true\n\n      # ===========================================\n      # Parse Kafka logs\n      # ===========================================\n      - match:\n          selector: '{service=\"kafka\"}'\n          stages:\n            - regex:\n                expression: '^\\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})\\] (?P<level>\\w+) (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Parse PostgreSQL logs\n      # ===========================================\n      - match:\n          selector: '{service=\"postgres\"}'\n          stages:\n            - regex:\n                expression: '^(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3} \\w+) \\[(?P<pid>\\d+)\\] (?P<level>\\w+):  (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Default: keep raw log\n      # ===========================================\n      - match:\n          selector: '{level=\"\"}'\n          stages:\n            - static_labels:\n                level: info",
    "monitoring/stress-testing/load-test/Dockerfile": "# stress-testing/load-test/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY load_test.py .\n\nENTRYPOINT [\"python\", \"load_test.py\"]\nCMD [\"--help\"]",
    "monitoring/stress-testing/load-test/load_test.py": "# stress-testing/load-test/load_test.py\n\nimport json\nimport random\nimport time\nimport uuid\nimport threading\nimport signal\nimport sys\nfrom datetime import datetime, timezone\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any\n\nimport click\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\n\n# ===========================================\n# Metrics\n# ===========================================\nMESSAGES_SENT = Counter(\n    'loadtest_messages_sent_total',\n    'Total messages sent',\n    ['status']\n)\n\nSEND_DURATION = Histogram(\n    'loadtest_send_duration_seconds',\n    'Time to send message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)\n)\n\nCURRENT_RPS = Gauge(\n    'loadtest_current_rps',\n    'Current requests per second'\n)\n\nACTIVE_THREADS = Gauge(\n    'loadtest_active_threads',\n    'Number of active threads'\n)\n\nTARGET_RPS = Gauge(\n    'loadtest_target_rps',\n    'Target requests per second'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order() -> dict[str, Any]:\n    \"\"\"Generate a fake order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    return {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n        \"load_test\": True,\n    }\n\n\n# ===========================================\n# Load Tester\n# ===========================================\nclass LoadTester:\n    def __init__(self, bootstrap_servers: str, topic: str):\n        self.bootstrap_servers = bootstrap_servers\n        self.topic = topic\n        self.producer = None\n        self.running = False\n        self.stats = {\n            \"sent\": 0,\n            \"failed\": 0,\n            \"start_time\": None,\n        }\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka\"\"\"\n        max_retries = 10\n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=self.bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks=1,  # Faster for load testing\n                    linger_ms=5,\n                    batch_size=16384,\n                    buffer_memory=33554432,\n                )\n                click.echo(f\"✅ Connected to Kafka: {self.bootstrap_servers}\")\n                return\n            except KafkaError as e:\n                click.echo(f\"⏳ Kafka connection attempt {attempt + 1}/{max_retries}: {e}\")\n                time.sleep(2)\n        \n        raise Exception(\"Failed to connect to Kafka\")\n    \n    def send_message(self) -> bool:\n        \"\"\"Send a single message\"\"\"\n        order = generate_order()\n        \n        try:\n            start = time.time()\n            future = self.producer.send(\n                self.topic,\n                key=order[\"order_id\"],\n                value=order\n            )\n            future.get(timeout=10)\n            duration = time.time() - start\n            \n            MESSAGES_SENT.labels(status=\"success\").inc()\n            SEND_DURATION.observe(duration)\n            self.stats[\"sent\"] += 1\n            return True\n            \n        except Exception as e:\n            MESSAGES_SENT.labels(status=\"failed\").inc()\n            self.stats[\"failed\"] += 1\n            return False\n    \n    def run_constant_load(self, rps: int, duration_seconds: int, threads: int = 10):\n        \"\"\"Run constant load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        TARGET_RPS.set(rps)\n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n🚀 Starting Constant Load Test\")\n        click.echo(f\"   Target RPS: {rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        interval = 1.0 / rps if rps > 0 else 1\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                batch_start = time.time()\n                futures = []\n                \n                # Submit batch of requests\n                for _ in range(min(rps, 100)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                # Wait for completion\n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                # Calculate actual RPS\n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                # Print progress\n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Sent: {self.stats['sent']:,} | \"\n                    f\"Failed: {self.stats['failed']:,} | \"\n                    f\"RPS: {actual_rps:.1f}\",\n                    nl=False\n                )\n                \n                # Rate limiting\n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_ramp_up(self, start_rps: int, end_rps: int, duration_seconds: int, threads: int = 20):\n        \"\"\"Run ramp-up load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n📈 Starting Ramp-Up Load Test\")\n        click.echo(f\"   Start RPS: {start_rps}\")\n        click.echo(f\"   End RPS: {end_rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        rps_increment = (end_rps - start_rps) / duration_seconds\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                elapsed = time.time() - self.stats[\"start_time\"]\n                current_target_rps = int(start_rps + (rps_increment * elapsed))\n                TARGET_RPS.set(current_target_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_target_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Target RPS: {current_target_rps:3d} | \"\n                    f\"Actual RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_spike(self, base_rps: int, spike_rps: int, spike_duration: int, total_duration: int, threads: int = 20):\n        \"\"\"Run spike test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n⚡ Starting Spike Test\")\n        click.echo(f\"   Base RPS: {base_rps}\")\n        click.echo(f\"   Spike RPS: {spike_rps}\")\n        click.echo(f\"   Spike Duration: {spike_duration}s\")\n        click.echo(f\"   Total Duration: {total_duration}s\")\n        click.echo(\"-\" * 50)\n        \n        end_time = time.time() + total_duration\n        spike_start = time.time() + (total_duration - spike_duration) / 2\n        spike_end = spike_start + spike_duration\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                current_time = time.time()\n                \n                # Determine current RPS\n                if spike_start <= current_time <= spike_end:\n                    current_rps = spike_rps\n                    phase = \"🔥 SPIKE\"\n                else:\n                    current_rps = base_rps\n                    phase = \"📊 BASE \"\n                \n                TARGET_RPS.set(current_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r{phase} | Remaining: {remaining:3d}s | \"\n                    f\"RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def _print_summary(self):\n        \"\"\"Print test summary\"\"\"\n        elapsed = time.time() - self.stats[\"start_time\"]\n        total = self.stats[\"sent\"] + self.stats[\"failed\"]\n        success_rate = (self.stats[\"sent\"] / total * 100) if total > 0 else 0\n        avg_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n        \n        click.echo(\"\\n\")\n        click.echo(\"=\" * 50)\n        click.echo(\"📊 LOAD TEST SUMMARY\")\n        click.echo(\"=\" * 50)\n        click.echo(f\"   Duration:     {elapsed:.1f}s\")\n        click.echo(f\"   Total Sent:   {self.stats['sent']:,}\")\n        click.echo(f\"   Failed:       {self.stats['failed']:,}\")\n        click.echo(f\"   Success Rate: {success_rate:.2f}%\")\n        click.echo(f\"   Avg RPS:      {avg_rps:.1f}\")\n        click.echo(\"=\" * 50)\n    \n    def stop(self):\n        \"\"\"Stop the test\"\"\"\n        self.running = False\n        if self.producer:\n            self.producer.close()\n\n\n# ===========================================\n# CLI\n# ===========================================\n@click.group()\ndef cli():\n    \"\"\"Load Testing Tool for Data Pipeline\"\"\"\n    pass\n\n\n@cli.command()\n@click.option('--rps', default=50, help='Requests per second')\n@click.option('--duration', default=60, help='Duration in seconds')\n@click.option('--threads', default=10, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef constant(rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run constant load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_constant_load(rps, duration, threads)\n\n\n@cli.command()\n@click.option('--start-rps', default=10, help='Starting RPS')\n@click.option('--end-rps', default=100, help='Ending RPS')\n@click.option('--duration', default=120, help='Duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef rampup(start_rps, end_rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run ramp-up load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_ramp_up(start_rps, end_rps, duration, threads)\n\n\n@cli.command()\n@click.option('--base-rps', default=20, help='Base RPS')\n@click.option('--spike-rps', default=200, help='Spike RPS')\n@click.option('--spike-duration', default=30, help='Spike duration in seconds')\n@click.option('--total-duration', default=120, help='Total duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef spike(base_rps, spike_rps, spike_duration, total_duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run spike test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_spike(base_rps, spike_rps, spike_duration, total_duration, threads)\n\n\nif __name__ == \"__main__\":\n    cli()",
    "monitoring/stress-testing/load-test/requirements.txt": "# stress-testing/load-test/requirements.txt\n\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data\nfaker==24.4.0\n\n# CLI\nclick==8.1.7\n\n# Metrics\nprometheus-client==0.20.0\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1",
    "monitoring/stress-testing/scripts/chaos_test.sh": "#!/bin/bash\n# stress-testing/scripts/chaos_test.sh\n\necho \"💥 Starting Chaos Test\"\necho \"======================\"\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\ncd \"$(dirname \"$0\")/../../\"\n\n# Function to check service health\ncheck_health() {\n    local service=$1\n    local url=$2\n    if curl -s \"$url\" > /dev/null 2>&1; then\n        echo -e \"${GREEN}✅ $service is UP${NC}\"\n        return 0\n    else\n        echo -e \"${RED}❌ $service is DOWN${NC}\"\n        return 1\n    fi\n}\n\n# Function to wait for recovery\nwait_for_recovery() {\n    local service=$1\n    local url=$2\n    local max_wait=60\n    local waited=0\n    \n    echo -e \"${YELLOW}⏳ Waiting for $service to recover...${NC}\"\n    \n    while [ $waited -lt $max_wait ]; do\n        if curl -s \"$url\" > /dev/null 2>&1; then\n            echo -e \"${GREEN}✅ $service recovered after ${waited}s${NC}\"\n            return 0\n        fi\n        sleep 2\n        waited=$((waited + 2))\n    done\n    \n    echo -e \"${RED}❌ $service did not recover within ${max_wait}s${NC}\"\n    return 1\n}\n\necho \"\"\necho \"📊 Initial Health Check\"\necho \"-----------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\ncheck_health \"Prometheus\" \"http://localhost:9090/-/healthy\"\ncheck_health \"Grafana\" \"http://localhost:3000/api/health\"\n\necho \"\"\necho \"💥 Test 1: Kill Consumer\"\necho \"------------------------\"\ndocker stop data-consumer\necho -e \"${YELLOW}Consumer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Consumer...\"\ndocker start data-consumer\nwait_for_recovery \"Consumer\" \"http://localhost:8001/metrics\"\n\necho \"\"\necho \"💥 Test 2: Kill Producer\"\necho \"------------------------\"\ndocker stop data-producer\necho -e \"${YELLOW}Producer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Producer...\"\ndocker start data-producer\nwait_for_recovery \"Producer\" \"http://localhost:8000/metrics\"\n\necho \"\"\necho \"💥 Test 3: Kill Kafka (WARNING: This will affect data flow)\"\necho \"------------------------------------------------------------\"\nread -p \"Do you want to proceed? (y/N) \" -n 1 -r\necho\nif [[ $REPLY =~ ^[Yy]$ ]]; then\n    docker stop kafka\n    echo -e \"${YELLOW}Kafka stopped. Check Grafana for cascading failures...${NC}\"\n    echo \"Waiting 60 seconds...\"\n    sleep 60\n    \n    echo \"\"\n    echo \"🔄 Restarting Kafka...\"\n    docker start kafka\n    sleep 10\n    wait_for_recovery \"Kafka\" \"http://localhost:9308/metrics\"\nfi\n\necho \"\"\necho \"📊 Final Health Check\"\necho \"---------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\n\necho \"\"\necho \"✅ Chaos Test Complete!\"\necho \"Check Grafana dashboards and Alertmanager for results.\"",
    "monitoring/stress-testing/scripts/ramp_up_test.sh": "#!/bin/bash\n# stress-testing/scripts/ramp_up_test.sh\n\necho \"📈 Starting Ramp-Up Load Test\"\necho \"==============================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test rampup \\\n    --start-rps ${START_RPS:-10} \\\n    --end-rps ${END_RPS:-100} \\\n    --duration ${DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002",
    "monitoring/stress-testing/scripts/run_load_test.sh": "#!/bin/bash\n# stress-testing/scripts/run_load_test.sh\n\necho \"🚀 Starting Constant Load Test\"\necho \"================================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test constant \\\n    --rps ${RPS:-50} \\\n    --duration ${DURATION:-60} \\\n    --threads ${THREADS:-10} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002",
    "monitoring/stress-testing/scripts/spike_test.sh": "#!/bin/bash\n# stress-testing/scripts/spike_test.sh\n\necho \"⚡ Starting Spike Test\"\necho \"======================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test spike \\\n    --base-rps ${BASE_RPS:-20} \\\n    --spike-rps ${SPIKE_RPS:-200} \\\n    --spike-duration ${SPIKE_DURATION:-30} \\\n    --total-duration ${TOTAL_DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002",
    "monitoring/stress-testing/docker-compose-stress-test.yml": "# stress-testing/docker-compose.yml\n\nservices:\n  load-test:\n    build:\n      context: ./load-test\n      dockerfile: Dockerfile\n    container_name: load-test\n    networks:\n      - monitoring-net\n    ports:\n      - \"8002:8002\"\n    environment:\n      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092\n      - KAFKA_TOPIC=ecommerce.orders\n    # Command sẽ được override khi chạy\n    command: [\"--help\"]\n\nnetworks:\n  monitoring-net:\n    external: true",
    "monitoring/docker-compose-monitoring.yml": "# monitoring/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Prometheus - Metrics Collection & Storage\n  # ===========================================\n  prometheus:\n    image: prom/prometheus:v3.5.0\n    container_name: prometheus\n    restart: unless-stopped\n    \n    # Command line arguments\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention.time=15d'        # Giữ data 15 ngày\n      - '--web.enable-lifecycle'                   # Cho phép reload config via API\n      - '--web.enable-admin-api'                   # Enable admin API\n      - '--web.enable-remote-write-receiver'\n    volumes:\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - ./prometheus/rules:/etc/prometheus/rules:ro\n      - prometheus_data:/prometheus\n    \n    ports:\n      - \"9090:9090\"\n    \n    networks:\n      - monitoring-net\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9090/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Alertmanager\n  # ===========================================\n  alertmanager:\n    image: prom/alertmanager:v0.27.0\n    container_name: alertmanager\n    restart: unless-stopped\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n      - '--web.external-url=http://localhost:9093'\n      - '--cluster.listen-address='\n    volumes:\n      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro\n      - alertmanager_data:/alertmanager\n    ports:\n      - \"9093:9093\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9093/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n\n  # ===========================================\n  # Loki - Log Aggregation\n  # ===========================================\n  loki:\n    image: grafana/loki:3.3.2\n    container_name: loki\n    restart: unless-stopped\n    command: -config.file=/etc/loki/loki-config.yml\n    volumes:\n      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro\n      - ./loki/rules:/loki/rules:ro\n      - loki_data:/loki\n    ports:\n      - \"3100:3100\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3100/ready\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Promtail - Log Collector\n  # ===========================================\n  promtail:\n    image: grafana/promtail:3.3.2\n    container_name: promtail\n    restart: unless-stopped\n    command: -config.file=/etc/promtail/promtail-config.yml\n    volumes:\n      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n    ports:\n      - \"9080:9080\"\n    networks:\n      - monitoring-net\n    depends_on:\n      loki:\n        condition: service_healthy\n\n\n  # ===========================================\n  # Grafana - Visualization & Dashboards\n  # ===========================================\n  grafana:\n    image: grafana/grafana:12.3.1\n    container_name: grafana\n    restart: unless-stopped\n    \n    environment:\n      # Admin credentials\n      - GF_SECURITY_ADMIN_USER=admin\n      - GF_SECURITY_ADMIN_PASSWORD=admin123\n      \n      # Server settings\n      - GF_SERVER_ROOT_URL=http://localhost:3000\n      \n      # Disable analytics\n      - GF_ANALYTICS_REPORTING_ENABLED=false\n      - GF_ANALYTICS_CHECK_FOR_UPDATES=false\n      \n      # Feature toggles (Grafana 12 features)\n      - GF_FEATURE_TOGGLES_ENABLE=nestedFolders\n    volumes:\n      # Provisioning - auto setup datasources & dashboards\n      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro\n      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro\n      \n      # Dashboard JSON files - mount trực tiếp từ local 👇\n      - ./grafana/dashboards/overview:/var/lib/grafana/dashboards/overview:ro\n      - ./grafana/dashboards/infrastructure:/var/lib/grafana/dashboards/infrastructure:ro\n      - ./grafana/dashboards/applications:/var/lib/grafana/dashboards/applications:ro\n      - ./grafana/dashboards/logs:/var/lib/grafana/dashboards/logs:ro\n      - ./grafana/dashboards/tracing:/var/lib/grafana/dashboards/tracing:ro\n      # Persistent storage\n      - grafana_data:/var/lib/grafana\n    \n    ports:\n      - \"3000:3000\"\n    \n    networks:\n      - monitoring-net\n    \n    depends_on:\n      prometheus:\n        condition: service_healthy\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3000/api/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  # ===========================================\n  # cAdvisor - Container Metrics\n  # ===========================================\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor:v0.51.0\n    container_name: cadvisor\n    restart: unless-stopped\n    privileged: true\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    ports:\n      - \"8080:8080\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8080/healthz\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n  # Jaeger - Distributed Tracing\n  # ===========================================\n  jaeger:\n    image: jaegertracing/all-in-one:1.54\n    container_name: jaeger\n    restart: unless-stopped\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n      - LOG_LEVEL=info\n    ports:\n      - \"16686:16686\"   # Jaeger UI\n      - \"4317:4317\"     # OTLP gRPC\n      - \"4318:4318\"     # OTLP HTTP\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:16686/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Volumes - Persistent Storage\n# ===========================================\nvolumes:\n  prometheus_data:\n    name: prometheus_data\n  grafana_data:\n    name: grafana_data\n  grafana_dashboards:\n    name: grafana_dashboards\n  alertmanager_data:\n    name: alertmanager_data\n  loki_data:\n    name: loki_data\n# ===========================================\n# Networks - External Network\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true",
    "monitoring/loki.md": "```mermaid\ngraph LR\n    subgraph \"Applications\"\n        P[Producer]\n        C[Consumer]\n    end\n    \n    subgraph \"Log Collection\"\n        PR[Promtail<br/>Log Collector]\n    end\n    \n    subgraph \"Storage & Query\"\n        L[Loki<br/>Log Aggregation]\n        G[Grafana<br/>Visualization]\n    end\n    \n    P -->|stdout/stderr| PR\n    C -->|stdout/stderr| PR\n    PR -->|push logs| L\n    L -->|query| G\n```",
    "monitoring/SLO.md": "```mermaid\ngraph TB\n    subgraph \"Definitions\"\n        SLI[SLI - Service Level Indicator<br/>📊 Metric đo lường]\n        SLO[SLO - Service Level Objective<br/>🎯 Mục tiêu cần đạt]\n        SLA[SLA - Service Level Agreement<br/>📝 Cam kết với khách hàng]\n    end\n    \n    SLI --> SLO --> SLA\n    \n    subgraph \"Example\"\n        E1[SLI: 99.5% requests < 500ms]\n        E2[SLO: 99.9% availability]\n        E3[SLA: Hoàn tiền nếu < 99.5%]\n    end\n```",
    "monitoring/stack.md": "```mermaid\ngraph TB\n    subgraph \"Observability Stack\"\n        M[Metrics<br/>Prometheus]\n        L[Logs<br/>Loki]\n        A[Alerts<br/>Alertmanager]\n        V[Visualization<br/>Grafana]\n    end\n    \n    subgraph \"Data Pipeline\"\n        P[Producer]\n        K[Kafka]\n        C[Consumer]\n        DB[(PostgreSQL)]\n    end\n    \n    subgraph \"Infrastructure\"\n        CA[cAdvisor]\n        KE[Kafka Exporter]\n        PE[Postgres Exporter]\n    end\n    \n    P --> K --> C --> DB\n    P & C --> M\n    P & C --> L\n    CA & KE & PE --> M\n    M & L --> A\n    M & L & A --> V\n```",
    "networks/docker-compose-network.yml": "networks:\n  monitoring-net:\n    name: monitoring-net\n    driver: bridge",
    "monitoring-overview.md": "```mermaid\ngraph LR\n    subgraph \"Data Source\"\n        A[🐍 Python Producer<br/>Fake e-commerce data]\n    end\n    \n    subgraph \"Message Queue\"\n        B[📨 Apache Kafka<br/>+ Zookeeper]\n    end\n    \n    subgraph \"Processing & Storage\"\n        C[🐍 Python Consumer<br/>Transform data]\n        D[(🐘 PostgreSQL<br/>Data Warehouse)]\n    end\n    \n    subgraph \"Monitoring Stack\"\n        E[📊 Prometheus]\n        F[📈 Grafana]\n    end\n    \n    A -->|produce orders| B\n    B -->|consume| C\n    C -->|insert| D\n    \n    A -.->|metrics| E\n    B -.->|metrics| E\n    C -.->|metrics| E\n    D -.->|metrics| E\n    E -->|visualize| F\n```",
    "repomix-output.md": "This file is a merged representation of the entire codebase, combined into a single document by Repomix.\n\n# File Summary\n\n## Purpose\nThis file contains a packed representation of the entire repository's contents.\nIt is designed to be easily consumable by AI systems for analysis, code review,\nor other automated processes.\n\n## File Format\nThe content is organized as follows:\n1. This summary section\n2. Repository information\n3. Directory structure\n4. Repository files (if enabled)\n5. Multiple file entries, each consisting of:\n  a. A header with the file path (## File: path/to/file)\n  b. The full contents of the file in a code block\n\n## Usage Guidelines\n- This file should be treated as read-only. Any changes should be made to the\n  original repository files, not this packed version.\n- When processing this file, use the file path to distinguish\n  between different files in the repository.\n- Be aware that this file may contain sensitive information. Handle it with\n  the same level of security as you would the original repository.\n\n## Notes\n- Some files may have been excluded based on .gitignore rules and Repomix's configuration\n- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files\n- Files matching patterns in .gitignore are excluded\n- Files matching default ignore patterns are excluded\n- Files are sorted by Git change count (files with more changes are at the bottom)\n\n# Directory Structure\n```\napplications/\n  consumer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  producer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  applications-flow.md\n  docker-compose.yml\ninfrastructure/\n  postgres/\n    init/\n      01-init.sql\n  docker-compose-application.yml\n  infrastructure.md\nmonitoring/\n  alertmanager/\n    alertmanager.yml\n  grafana/\n    dashboards/\n      applications/\n        data-pipeline.json\n        red-metrics.json\n        slo-dashboard.json\n      infrastructure/\n        container-metrics.json\n        kafka.json\n        postgresql.json\n      logs/\n        logs-explorer.json\n        test.json\n      overview/\n        correlation-dashboard.json\n        system-overview.json\n      tracing/\n        tracing-overview.json\n    provisioning/\n      dashboards/\n        dashboards.yml\n      datasources/\n        datasources.yml\n  loki/\n    rules/\n      fake/\n        alerts.yml\n      alerts.yml\n    loki-config.yml\n  prometheus/\n    rules/\n      alert_rules.yml\n      recording_rules.yml\n      slo_alerts.yml\n      slo_rules.yml\n    prometheus.yml\n  promtail/\n    promtail-config.yml\n  stress-testing/\n    load-test/\n      Dockerfile\n      load_test.py\n      requirements.txt\n    scripts/\n      chaos_test.sh\n      ramp_up_test.sh\n      run_load_test.sh\n      spike_test.sh\n    docker-compose-stress-test.yml\n  docker-compose-monitoring.yml\n  loki.md\n  SLO.md\n  stack.md\nnetworks/\n  docker-compose-network.yml\nmonitoring-overview.md\nstructure.md\n```\n\n# Files\n\n## File: applications/consumer/config.py\n````python\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    \"\"\"Consumer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    kafka_consumer_group: str = \"order-processor\"\n    \n    # PostgreSQL settings\n    postgres_host: str = \"postgres\"\n    postgres_port: int = 5432\n    postgres_db: str = \"datawarehouse\"\n    postgres_user: str = \"postgres\"\n    postgres_password: str = \"postgres123\"\n    \n    # Consumer settings\n    batch_size: int = 10\n    poll_timeout_ms: int = 1000\n    \n    # Metrics server\n    metrics_port: int = 8001\n    \n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-consumer\"\n    otel_enabled: bool = True\n    \n    # Application\n    app_name: str = \"data-consumer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n    \n    @property\n    def postgres_dsn(self) -> str:\n        return (\n            f\"host={self.postgres_host} \"\n            f\"port={self.postgres_port} \"\n            f\"dbname={self.postgres_db} \"\n            f\"user={self.postgres_user} \"\n            f\"password={self.postgres_password}\"\n        )\n\n\nsettings = Settings()\n````\n\n## File: applications/consumer/Dockerfile\n````\n# applications/consumer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8001\n\n# Run the consumer\nCMD [\"python\", \"-u\", \"main.py\"]\n````\n\n## File: applications/consumer/logger.py\n````python\n# applications/consumer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()\n````\n\n## File: applications/consumer/main.py\n````python\n# applications/consumer/main.py\n\nimport json\nimport time\nfrom datetime import datetime, timezone\n\nimport psycopg2\nfrom psycopg2.extras import execute_batch\nfrom kafka import KafkaConsumer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode, SpanKind\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_consumer_info',\n    'Consumer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'consumer_group': settings.kafka_consumer_group,\n    'pattern': 'RED'\n})\n\nMESSAGES_CONSUMED = Counter(\n    'pipeline_consumer_messages_total',\n    'Total number of messages consumed',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PROCESSED = Counter(\n    'pipeline_consumer_batches_total',\n    'Total number of batches processed',\n    ['status']\n)\n\nDB_OPERATIONS = Counter(\n    'pipeline_consumer_db_operations_total',\n    'Total database operations',\n    ['operation', 'status']\n)\n\nERRORS = Counter(\n    'pipeline_consumer_errors_total',\n    'Total number of errors by type and stage',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_message_duration_seconds',\n    'Time to process a single message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_batch_duration_seconds',\n    'Time to process a batch of messages',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nDB_QUERY_DURATION = Histogram(\n    'pipeline_consumer_db_query_duration_seconds',\n    'Database query duration',\n    ['operation'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5)\n)\n\nKAFKA_POLL_DURATION = Histogram(\n    'pipeline_consumer_kafka_poll_duration_seconds',\n    'Kafka poll duration',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0)\n)\n\nEND_TO_END_LATENCY = Histogram(\n    'pipeline_consumer_end_to_end_latency_seconds',\n    'End-to-end latency from order creation to database insert',\n    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0)\n)\n\nORDERS_PROCESSED = Counter(\n    'pipeline_business_orders_processed_total',\n    'Total orders processed by category',\n    ['category']\n)\n\nREVENUE_PROCESSED = Counter(\n    'pipeline_business_revenue_processed_total',\n    'Total revenue processed',\n    ['category']\n)\n\nCONSUMER_UP = Gauge(\n    'pipeline_consumer_up',\n    'Consumer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_kafka_connected',\n    'Kafka connection status'\n)\n\nDB_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_db_connected',\n    'Database connection status'\n)\n\nCONSUMER_LAG = Gauge(\n    'pipeline_consumer_lag_messages',\n    'Consumer lag per partition',\n    ['topic', 'partition']\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_consumer_current_batch_size',\n    'Number of messages in current batch'\n)\n\nLAST_PROCESS_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_process_timestamp',\n    'Timestamp of last successful process'\n)\n\nLAST_COMMIT_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_commit_timestamp',\n    'Timestamp of last offset commit'\n)\n\n\n# ===========================================\n# Database Handler\n# ===========================================\n\nclass DatabaseHandler:\n    def __init__(self):\n        self.conn = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to PostgreSQL with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.conn = psycopg2.connect(settings.postgres_dsn)\n                self.conn.autocommit = False\n                logger.info(\n                    \"Connected to PostgreSQL\",\n                    extra={\n                        \"event\": \"db_connected\",\n                        \"host\": settings.postgres_host,\n                        \"database\": settings.postgres_db,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                DB_CONNECTION_STATUS.set(1)\n                return\n            except psycopg2.Error as e:\n                DB_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"db_init\").inc()\n                logger.warning(\n                    \"PostgreSQL connection failed, retrying...\",\n                    extra={\n                        \"event\": \"db_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to PostgreSQL after maximum retries\")\n    \n    def insert_orders(self, orders: list[dict], parent_span=None) -> int:\n        \"\"\"Insert batch of orders into database with tracing\"\"\"\n        if not orders:\n            return 0\n        \n        with tracer.start_as_current_span(\n            \"db_insert_orders\",\n            kind=SpanKind.CLIENT\n        ) as span:\n            span.set_attribute(\"db.system\", \"postgresql\")\n            span.set_attribute(\"db.name\", settings.postgres_db)\n            span.set_attribute(\"db.operation\", \"INSERT\")\n            span.set_attribute(\"db.batch_size\", len(orders))\n            \n            insert_sql = \"\"\"\n                INSERT INTO ecommerce.orders (\n                    order_id, customer_id, product_id, product_name, \n                    category, quantity, unit_price, total_amount,\n                    order_status, created_at, processed_at\n                ) VALUES (\n                    %(order_id)s, %(customer_id)s, %(product_id)s, %(product_name)s,\n                    %(category)s, %(quantity)s, %(unit_price)s, %(total_amount)s,\n                    %(order_status)s, %(created_at)s, %(processed_at)s\n                )\n                ON CONFLICT (order_id) DO NOTHING\n            \"\"\"\n            \n            try:\n                start_time = time.time()\n                \n                with self.conn.cursor() as cur:\n                    process_time = datetime.now(timezone.utc)\n                    \n                    for order in orders:\n                        order['processed_at'] = process_time.isoformat()\n                        if isinstance(order.get('created_at'), str):\n                            order['created_at'] = datetime.fromisoformat(\n                                order['created_at'].replace('Z', '+00:00')\n                            )\n                    \n                    execute_batch(cur, insert_sql, orders, page_size=100)\n                    inserted = cur.rowcount\n                    self.conn.commit()\n                \n                duration = time.time() - start_time\n                \n                span.set_attribute(\"db.rows_affected\", inserted)\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                span.set_status(Status(StatusCode.OK))\n                \n                DB_QUERY_DURATION.labels(operation=\"insert_batch\").observe(duration)\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"success\").inc(len(orders))\n                \n                logger.debug(\n                    \"Orders inserted into database\",\n                    extra={\n                        \"event\": \"db_insert_success\",\n                        \"count\": inserted,\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return inserted\n                \n            except psycopg2.Error as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                ERRORS.labels(error_type=type(e).__name__, stage=\"db_insert\").inc()\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"failed\").inc(len(orders))\n                \n                logger.error(\n                    \"Database insert failed\",\n                    extra={\n                        \"event\": \"db_insert_error\",\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__,\n                        \"batch_size\": len(orders)\n                    }\n                )\n                self.conn.rollback()\n                \n                try:\n                    self._connect()\n                except Exception:\n                    DB_CONNECTION_STATUS.set(0)\n                \n                return 0\n    \n    def close(self):\n        \"\"\"Close database connection\"\"\"\n        if self.conn:\n            self.conn.close()\n            DB_CONNECTION_STATUS.set(0)\n            logger.info(\"Database connection closed\", extra={\"event\": \"db_closed\"})\n\n\n# ===========================================\n# Kafka Consumer\n# ===========================================\n\nclass OrderConsumer:\n    def __init__(self, db_handler: DatabaseHandler):\n        self.consumer = None\n        self.db = db_handler\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.consumer = KafkaConsumer(\n                    settings.kafka_topic,\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    group_id=settings.kafka_consumer_group,\n                    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n                    auto_offset_reset='earliest',\n                    enable_auto_commit=False,\n                    max_poll_records=settings.batch_size,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"topic\": settings.kafka_topic,\n                        \"consumer_group\": settings.kafka_consumer_group,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                CONSUMER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"kafka_init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def process_messages(self):\n        \"\"\"Main processing loop with tracing\"\"\"\n        batch: list[dict] = []\n        \n        logger.info(\"Starting message processing loop\", extra={\"event\": \"processing_started\"})\n        \n        try:\n            while True:\n                # Poll for messages\n                with tracer.start_as_current_span(\n                    \"kafka_poll\",\n                    kind=SpanKind.CONSUMER\n                ) as poll_span:\n                    poll_start = time.time()\n                    records = self.consumer.poll(\n                        timeout_ms=settings.poll_timeout_ms,\n                        max_records=settings.batch_size\n                    )\n                    poll_duration = time.time() - poll_start\n                    \n                    poll_span.set_attribute(\"messaging.system\", \"kafka\")\n                    poll_span.set_attribute(\"messaging.operation\", \"poll\")\n                    poll_span.set_attribute(\"duration_ms\", round(poll_duration * 1000, 2))\n                    \n                    KAFKA_POLL_DURATION.observe(poll_duration)\n                \n                if not records:\n                    if batch:\n                        self._process_batch(batch)\n                        batch = []\n                    continue\n                \n                # Process received messages\n                for topic_partition, messages in records.items():\n                    for message in messages:\n                        try:\n                            msg_start = time.time()\n                            order = message.value\n                            category = order.get(\"category\", \"unknown\")\n                            \n                            # Create span for consuming\n                            with tracer.start_as_current_span(\n                                \"consume_message\",\n                                kind=SpanKind.CONSUMER\n                            ) as msg_span:\n                                # Add span attributes\n                                msg_span.set_attribute(\"messaging.system\", \"kafka\")\n                                msg_span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n                                msg_span.set_attribute(\"messaging.kafka.partition\", topic_partition.partition)\n                                msg_span.set_attribute(\"messaging.kafka.offset\", message.offset)\n                                msg_span.set_attribute(\"order.id\", order.get(\"order_id\", \"\"))\n                                msg_span.set_attribute(\"order.category\", category)\n                                \n                                # Link to producer trace if available\n                                if \"trace_id\" in order:\n                                    msg_span.set_attribute(\"producer.trace_id\", order[\"trace_id\"])\n                                \n                                batch.append(order)\n                                \n                                MESSAGES_CONSUMED.labels(\n                                    topic=settings.kafka_topic,\n                                    status=\"success\",\n                                    category=category\n                                ).inc()\n                                \n                                msg_duration = time.time() - msg_start\n                                MESSAGE_PROCESS_DURATION.observe(msg_duration)\n                                \n                                # Calculate end-to-end latency\n                                if 'created_at' in order:\n                                    try:\n                                        created_at = datetime.fromisoformat(\n                                            order['created_at'].replace('Z', '+00:00')\n                                        )\n                                        e2e_latency = (datetime.now(timezone.utc) - created_at).total_seconds()\n                                        END_TO_END_LATENCY.observe(e2e_latency)\n                                        msg_span.set_attribute(\"e2e_latency_seconds\", e2e_latency)\n                                    except Exception:\n                                        pass\n                                \n                                CONSUMER_LAG.labels(\n                                    topic=topic_partition.topic,\n                                    partition=str(topic_partition.partition)\n                                ).set(message.offset)\n                                \n                                msg_span.set_status(Status(StatusCode.OK))\n                                \n                                logger.info(\n                                    \"Message consumed\",\n                                    extra={\n                                        \"event\": \"message_consumed\",\n                                        \"order_id\": order.get(\"order_id\"),\n                                        \"trace_id\": order.get(\"trace_id\", \"\"),\n                                        \"category\": category,\n                                        \"partition\": topic_partition.partition,\n                                        \"offset\": message.offset\n                                    }\n                                )\n                                \n                        except Exception as e:\n                            MESSAGES_CONSUMED.labels(\n                                topic=settings.kafka_topic,\n                                status=\"failed\",\n                                category=\"unknown\"\n                            ).inc()\n                            \n                            ERRORS.labels(\n                                error_type=type(e).__name__,\n                                stage=\"message_parse\"\n                            ).inc()\n                            \n                            logger.error(\n                                \"Failed to process message\",\n                                extra={\n                                    \"event\": \"message_parse_error\",\n                                    \"error\": str(e),\n                                    \"error_type\": type(e).__name__,\n                                    \"partition\": topic_partition.partition,\n                                    \"offset\": message.offset\n                                }\n                            )\n\n                \n                CURRENT_BATCH_SIZE.set(len(batch))\n                \n                if len(batch) >= settings.batch_size:\n                    self._process_batch(batch)\n                    batch = []\n                    \n        except KeyboardInterrupt:\n            logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n            if batch:\n                self._process_batch(batch)\n    \n    def _process_batch(self, batch: list[dict]):\n        \"\"\"Process and commit a batch of messages with tracing\"\"\"\n        if not batch:\n            return\n        \n        with tracer.start_as_current_span(\n            \"process_batch\",\n            kind=SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", len(batch))\n            \n            start_time = time.time()\n            \n            # Insert to database\n            inserted = self.db.insert_orders(batch)\n            \n            # Commit Kafka offsets\n            self.consumer.commit()\n            LAST_COMMIT_TIMESTAMP.set(time.time())\n            \n            duration = time.time() - start_time\n            \n            batch_span.set_attribute(\"batch.inserted\", inserted)\n            batch_span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n            \n            BATCH_PROCESS_DURATION.observe(duration)\n            \n            if inserted == len(batch):\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"success\").inc()\n            elif inserted == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"No records inserted\"))\n                BATCHES_PROCESSED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"partial\").inc()\n            \n            # Business metrics\n            for order in batch:\n                category = order.get(\"category\", \"unknown\")\n                ORDERS_PROCESSED.labels(category=category).inc()\n                REVENUE_PROCESSED.labels(category=category).inc(order.get(\"total_amount\", 0))\n            \n            LAST_PROCESS_TIMESTAMP.set(time.time())\n            CURRENT_BATCH_SIZE.set(0)\n            \n            logger.info(\n                \"Batch processed\",\n                extra={\n                    \"event\": \"batch_processed\",\n                    \"batch_size\": len(batch),\n                    \"inserted\": inserted,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(len(batch) / duration, 2) if duration > 0 else 0\n                }\n            )\n    \n    def close(self):\n        \"\"\"Close the consumer\"\"\"\n        if self.consumer:\n            self.consumer.close()\n            CONSUMER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Consumer closed\", extra={\"event\": \"consumer_closed\"})\n\n\n# ===========================================\n# Main\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Consumer\",\n        extra={\n            \"event\": \"consumer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"consumer_group\": settings.kafka_consumer_group,\n                \"postgres_host\": settings.postgres_host,\n                \"postgres_db\": settings.postgres_db,\n                \"batch_size\": settings.batch_size,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create handlers\n    db_handler = DatabaseHandler()\n    consumer = OrderConsumer(db_handler)\n    \n    try:\n        consumer.process_messages()\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in consumer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        consumer.close()\n        db_handler.close()\n        logger.info(\"Consumer stopped\", extra={\"event\": \"consumer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()\n````\n\n## File: applications/consumer/requirements.txt\n````\n# Kafka\nkafka-python-ng==2.2.2\n\n# PostgreSQL\npsycopg2-binary==2.9.9\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0\n````\n\n## File: applications/consumer/tracing.py\n````python\n# applications/consumer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()\n````\n\n## File: applications/producer/config.py\n````python\n# applications/producer/config.py\n\nfrom pydantic_settings import BaseSettings\nfrom typing import Optional\n\n\nclass Settings(BaseSettings):\n    \"\"\"Producer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    \n    # Producer settings\n    produce_interval_seconds: float = 1.0  # Produce every N seconds\n    batch_size: int = 5  # Messages per batch\n    \n    # Metrics server\n    metrics_port: int = 8000\n\n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-producer\"\n    otel_enabled: bool = True\n\n    # Application\n    app_name: str = \"data-producer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n\n\nsettings = Settings()\n````\n\n## File: applications/producer/Dockerfile\n````\n# applications/producer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8000\n\n# Run the producer\nCMD [\"python\", \"-u\", \"main.py\"]\n````\n\n## File: applications/producer/logger.py\n````python\n# applications/producer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()\n````\n\n## File: applications/producer/main.py\n````python\n# applications/producer/main.py\n\nimport json\nimport random\nimport time\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_producer_info',\n    'Producer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'pattern': 'RED'\n})\n\nMESSAGES_PRODUCED = Counter(\n    'pipeline_producer_messages_total',\n    'Total number of messages produced',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PRODUCED = Counter(\n    'pipeline_producer_batches_total',\n    'Total number of batches produced',\n    ['status']\n)\n\nERRORS = Counter(\n    'pipeline_producer_errors_total',\n    'Total number of errors by type',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_message_duration_seconds',\n    'Time to produce a single message to Kafka',\n    ['topic'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_batch_duration_seconds',\n    'Time to produce a batch of messages',\n    ['topic'],\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nMESSAGE_SIZE = Histogram(\n    'pipeline_producer_message_size_bytes',\n    'Size of produced messages in bytes',\n    ['topic'],\n    buckets=(100, 500, 1000, 2500, 5000, 10000, 25000)\n)\n\nORDERS_BY_CATEGORY = Counter(\n    'pipeline_business_orders_total',\n    'Total orders by category',\n    ['category']\n)\n\nREVENUE = Counter(\n    'pipeline_business_revenue_total',\n    'Total revenue generated',\n    ['category']\n)\n\nORDER_VALUE = Histogram(\n    'pipeline_business_order_value',\n    'Distribution of order values',\n    ['category'],\n    buckets=(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\n)\n\nITEMS_PER_ORDER = Histogram(\n    'pipeline_business_items_per_order',\n    'Distribution of items per order',\n    buckets=(1, 2, 3, 4, 5, 10)\n)\n\nPRODUCER_UP = Gauge(\n    'pipeline_producer_up',\n    'Producer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_producer_kafka_connected',\n    'Kafka connection status (1=connected, 0=disconnected)'\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_producer_current_batch_size',\n    'Current configured batch size'\n)\n\nLAST_PRODUCE_TIMESTAMP = Gauge(\n    'pipeline_producer_last_produce_timestamp',\n    'Timestamp of last successful produce'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\n\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order(trace_id: str = None) -> dict[str, Any]:\n    \"\"\"Generate a fake e-commerce order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    order = {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n    }\n    \n    # Add trace_id for correlation\n    if trace_id:\n        order[\"trace_id\"] = trace_id\n    \n    return order\n\n\n# ===========================================\n# Kafka Producer\n# ===========================================\n\nclass OrderProducer:\n    def __init__(self):\n        self.producer = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks='all',\n                    retries=3,\n                    max_in_flight_requests_per_connection=1,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                PRODUCER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def produce(self, order: dict) -> bool:\n        \"\"\"Produce a single order to Kafka with tracing\"\"\"\n        category = order.get(\"category\", \"unknown\")\n        \n        # Create span for this operation\n        with tracer.start_as_current_span(\n            \"produce_order\",\n            kind=trace.SpanKind.PRODUCER\n        ) as span:\n            # Add span attributes\n            span.set_attribute(\"messaging.system\", \"kafka\")\n            span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n            span.set_attribute(\"order.id\", order[\"order_id\"])\n            span.set_attribute(\"order.category\", category)\n            span.set_attribute(\"order.amount\", order[\"total_amount\"])\n            \n            # Get trace_id and add to order for correlation\n            trace_id = format(span.get_span_context().trace_id, '032x')\n            order[\"trace_id\"] = trace_id\n            \n            message_bytes = json.dumps(order).encode('utf-8')\n            span.set_attribute(\"messaging.message.payload_size_bytes\", len(message_bytes))\n            \n            try:\n                start_time = time.time()\n                \n                future = self.producer.send(\n                    settings.kafka_topic,\n                    key=order[\"order_id\"],\n                    value=order\n                )\n                future.get(timeout=10)\n                \n                duration = time.time() - start_time\n                \n                # Record success in span\n                span.set_status(Status(StatusCode.OK))\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                \n                # Metrics\n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"success\",\n                    category=category\n                ).inc()\n                \n                MESSAGE_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n                MESSAGE_SIZE.labels(topic=settings.kafka_topic).observe(len(message_bytes))\n                \n                ORDERS_BY_CATEGORY.labels(category=category).inc()\n                REVENUE.labels(category=category).inc(order[\"total_amount\"])\n                ORDER_VALUE.labels(category=category).observe(order[\"total_amount\"])\n                ITEMS_PER_ORDER.observe(order[\"quantity\"])\n                \n                LAST_PRODUCE_TIMESTAMP.set(time.time())\n                \n                logger.debug(\n                    \"Order produced successfully\",\n                    extra={\n                        \"event\": \"order_produced\",\n                        \"order_id\": order[\"order_id\"],\n                        \"trace_id\": trace_id,\n                        \"category\": category,\n                        \"amount\": order[\"total_amount\"],\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return True\n                \n            except Exception as e:\n                # Record error in span\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"failed\",\n                    category=category\n                ).inc()\n                \n                ERRORS.labels(\n                    error_type=type(e).__name__,\n                    stage=\"produce\"\n                ).inc()\n                \n                logger.error(\n                    \"Failed to produce order\",\n                    extra={\n                        \"event\": \"produce_error\",\n                        \"order_id\": order.get(\"order_id\"),\n                        \"trace_id\": trace_id,\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__\n                    }\n                )\n                return False\n    \n    def produce_batch(self, batch_size: int) -> tuple[int, int]:\n        \"\"\"Produce a batch of orders with tracing\"\"\"\n        \n        # Create parent span for batch\n        with tracer.start_as_current_span(\n            \"produce_batch\",\n            kind=trace.SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", batch_size)\n            \n            success_count = 0\n            failed_count = 0\n            batch_orders = []\n            \n            CURRENT_BATCH_SIZE.set(batch_size)\n            \n            start_time = time.time()\n            \n            for _ in range(batch_size):\n                order = generate_order()\n                if self.produce(order):\n                    success_count += 1\n                    batch_orders.append(order[\"order_id\"])\n                else:\n                    failed_count += 1\n            \n            self.producer.flush()\n            \n            duration = time.time() - start_time\n            \n            # Record batch results in span\n            batch_span.set_attribute(\"batch.success_count\", success_count)\n            batch_span.set_attribute(\"batch.failed_count\", failed_count)\n            batch_span.set_attribute(\"batch.duration_ms\", round(duration * 1000, 2))\n            \n            if failed_count == 0:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"success\").inc()\n            elif success_count == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"All messages failed\"))\n                BATCHES_PRODUCED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"partial\").inc()\n            \n            BATCH_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n            \n            logger.info(\n                \"Batch produced\",\n                extra={\n                    \"event\": \"batch_produced\",\n                    \"batch_size\": batch_size,\n                    \"success_count\": success_count,\n                    \"failed_count\": failed_count,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(batch_size / duration, 2) if duration > 0 else 0\n                }\n            )\n            \n            return success_count, failed_count\n    \n    def close(self):\n        \"\"\"Close the producer\"\"\"\n        if self.producer:\n            self.producer.close()\n            PRODUCER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Producer closed\", extra={\"event\": \"producer_closed\"})\n\n\n# ===========================================\n# Main Loop\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Producer\",\n        extra={\n            \"event\": \"producer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"batch_size\": settings.batch_size,\n                \"interval_seconds\": settings.produce_interval_seconds,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create producer\n    producer = OrderProducer()\n    \n    try:\n        batch_number = 0\n        while True:\n            batch_number += 1\n            \n            # Create span for each iteration\n            with tracer.start_as_current_span(f\"batch_iteration_{batch_number}\"):\n                success, failed = producer.produce_batch(settings.batch_size)\n            \n            time.sleep(settings.produce_interval_seconds)\n            \n    except KeyboardInterrupt:\n        logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in producer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        producer.close()\n        logger.info(\"Producer stopped\", extra={\"event\": \"producer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()\n````\n\n## File: applications/producer/requirements.txt\n````\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data generation\nfaker==24.4.0\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0\n````\n\n## File: applications/producer/tracing.py\n````python\n# applications/producer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()\n````\n\n## File: applications/applications-flow.md\n````markdown\n```mermaid\ngraph LR\n    subgraph \"applications/docker-compose.yml\"\n        subgraph \"Producer\"\n            P[Data Producer<br/>:8000/metrics]\n            F[Faker Library]\n        end\n        \n        subgraph \"Consumer\"\n            C[Data Consumer<br/>:8001/metrics]\n        end\n    end\n    \n    subgraph \"infrastructure/\"\n        K[Kafka]\n        PG[(PostgreSQL)]\n    end\n    \n    subgraph \"monitoring/\"\n        PR[Prometheus]\n    end\n    \n    F --> P\n    P -->|produce orders| K\n    K -->|consume| C\n    C -->|insert| PG\n    \n    P -.->|metrics| PR\n    C -.->|metrics| PR\n```\n````\n\n## File: applications/docker-compose.yml\n````yaml\n# applications/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Data Producer - Generate fake orders\n  # ===========================================\n  data-producer:\n    build:\n      context: ./producer\n      dockerfile: Dockerfile\n    container_name: data-producer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      \n      # Producer settings\n      PRODUCE_INTERVAL_SECONDS: \"1.0\"\n      BATCH_SIZE: \"5\"\n      \n      # Metrics\n      METRICS_PORT: \"8000\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8000:8000\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8000/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Data Consumer - Process orders to PostgreSQL\n  # ===========================================\n  data-consumer:\n    build:\n      context: ./consumer\n      dockerfile: Dockerfile\n    container_name: data-consumer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      KAFKA_CONSUMER_GROUP: order-processor\n      \n      # PostgreSQL settings\n      POSTGRES_HOST: postgres\n      POSTGRES_PORT: \"5432\"\n      POSTGRES_DB: datawarehouse\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres123\n      \n      # Consumer settings\n      BATCH_SIZE: \"10\"\n      POLL_TIMEOUT_MS: \"1000\"\n      \n      # Metrics\n      METRICS_PORT: \"8001\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8001:8001\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    #   postgres:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8001/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Networks\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true\n````\n\n## File: infrastructure/postgres/init/01-init.sql\n````sql\n-- infrastructure/postgres/init/01-init.sql\n\n-- ===========================================\n-- Database cho Data Pipeline\n-- ===========================================\n\n-- Tạo schema cho e-commerce data\nCREATE SCHEMA IF NOT EXISTS ecommerce;\n\n-- Bảng orders - nơi lưu data từ Kafka consumer\nCREATE TABLE ecommerce.orders (\n    id SERIAL PRIMARY KEY,\n    order_id VARCHAR(50) UNIQUE NOT NULL,\n    customer_id VARCHAR(50) NOT NULL,\n    product_id VARCHAR(50) NOT NULL,\n    product_name VARCHAR(255) NOT NULL,\n    category VARCHAR(100),\n    quantity INTEGER NOT NULL,\n    unit_price DECIMAL(10, 2) NOT NULL,\n    total_amount DECIMAL(10, 2) NOT NULL,\n    order_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    processed_at TIMESTAMP,\n    \n    -- Indexes cho queries thường dùng\n    CONSTRAINT chk_quantity CHECK (quantity > 0),\n    CONSTRAINT chk_price CHECK (unit_price > 0)\n);\n\nCREATE INDEX idx_orders_customer ON ecommerce.orders(customer_id);\nCREATE INDEX idx_orders_created_at ON ecommerce.orders(created_at);\nCREATE INDEX idx_orders_status ON ecommerce.orders(order_status);\nCREATE INDEX idx_orders_category ON ecommerce.orders(category);\n\n-- Bảng để track processing metrics\nCREATE TABLE ecommerce.processing_logs (\n    id SERIAL PRIMARY KEY,\n    batch_id VARCHAR(50) NOT NULL,\n    records_processed INTEGER NOT NULL,\n    records_failed INTEGER DEFAULT 0,\n    processing_time_ms INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- View cho monitoring\nCREATE VIEW ecommerce.orders_summary AS\nSELECT \n    DATE(created_at) as order_date,\n    COUNT(*) as total_orders,\n    SUM(total_amount) as total_revenue,\n    AVG(total_amount) as avg_order_value,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM ecommerce.orders\nGROUP BY DATE(created_at)\nORDER BY order_date DESC;\n\n-- Grant permissions\nGRANT ALL PRIVILEGES ON SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA ecommerce TO postgres;\n\n-- Log initialization\nDO $$\nBEGIN\n    RAISE NOTICE 'Database initialization completed successfully!';\nEND $$;\n````\n\n## File: infrastructure/infrastructure.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"infrastructure/docker-compose.yml\"\n        subgraph \"Message Queue\"\n            ZK[Zookeeper:2181]\n            K[Kafka:9092]\n            KE[Kafka Exporter:9308]\n        end\n        \n        subgraph \"Database\"\n            PG[(PostgreSQL:5432)]\n            PE[Postgres Exporter:9187]\n        end\n        \n        ZK --> K\n        K -.->|metrics| KE\n        PG -.->|metrics| PE\n    end\n    \n    subgraph \"monitoring/\"\n        P[Prometheus]\n    end\n    \n    KE -->|scrape :9308| P\n    PE -->|scrape :9187| P\n```\n````\n\n## File: monitoring/alertmanager/alertmanager.yml\n````yaml\n# monitoring/alertmanager/alertmanager.yml\n\nglobal:\n  # Thời gian chờ trước khi gửi lại alert nếu vẫn còn firing\n  resolve_timeout: 5m\n\n# Route tree - định tuyến alerts\nroute:\n  # Default receiver\n  receiver: 'default-receiver'\n  \n  # Group alerts by these labels\n  group_by: ['alertname', 'severity', 'service']\n  \n  # Thời gian chờ trước khi gửi group đầu tiên\n  group_wait: 30s\n  \n  # Thời gian chờ trước khi gửi alerts mới trong cùng group\n  group_interval: 5m\n  \n  # Thời gian chờ trước khi gửi lại alert đã gửi\n  repeat_interval: 4h\n\n  # Child routes - routing dựa trên labels\n  routes:\n    # Critical alerts - gửi ngay\n    - match:\n        severity: critical\n      receiver: 'critical-receiver'\n      group_wait: 10s\n      repeat_interval: 1h\n\n    # Warning alerts\n    - match:\n        severity: warning\n      receiver: 'warning-receiver'\n      repeat_interval: 4h\n\n    # Info alerts - ít urgent hơn\n    - match:\n        severity: info\n      receiver: 'default-receiver'\n      repeat_interval: 12h\n\n# Inhibition rules - suppress alerts\ninhibit_rules:\n  # Nếu critical đang fire, suppress warning cùng service\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'service']\n\n# Receivers - nơi nhận alerts\nreceivers:\n  - name: 'default-receiver'\n    # Webhook để test (có thể xem trong Alertmanager UI)\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n\n  - name: 'critical-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n    # Có thể thêm Slack, Email, PagerDuty ở đây\n    # slack_configs:\n    #   - api_url: 'https://hooks.slack.com/services/xxx/yyy/zzz'\n    #     channel: '#alerts-critical'\n    #     send_resolved: true\n\n  - name: 'warning-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n````\n\n## File: monitoring/grafana/dashboards/applications/data-pipeline.json\n````json\n{\n  \"uid\": \"data-pipeline-dashboard\",\n  \"title\": \"Data Pipeline Overview\",\n  \"description\": \"Monitoring data pipeline: Producer → Kafka → Consumer → PostgreSQL\",\n  \"tags\": [\"applications\", \"data-pipeline\", \"producer\", \"consumer\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🚀 Data Pipeline Flow\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## Pipeline: **Producer** → **Kafka** → **Consumer** → **PostgreSQL**\\n\\n*Metrics sẽ hiển thị khi Producer và Consumer được khởi động.*\"\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-producer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 6, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-consumer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Messages Produced (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_produced_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Messages Consumed (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_consumed_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Production Rate (msg/s)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_messages_produced_total[1m])\",\n          \"legendFormat\": \"Produced\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_messages_consumed_total[1m])\",\n          \"legendFormat\": \"Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Produced\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Processing Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Errors\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-producer\\\"}[1m])\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-consumer\\\"}[1m])\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"errors/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 0.1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Database Inserts (from Consumer)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_db_inserts_total[1m])\",\n          \"legendFormat\": \"Inserts/s\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"single\", \"sort\": \"none\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"gradientMode\": \"hue\", \"lineWidth\": 1 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Orders by Category (Real-time)\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_orders_by_category)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Revenue Generated\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_total_revenue\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"noValue\": \"$0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Pipeline Health Score\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(1 - (rate(pipeline_errors_total[5m]) / (rate(pipeline_messages_produced_total[5m]) + 0.001))) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"noValue\": \"N/A\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 90 }, { \"color\": \"green\", \"value\": 99 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/applications/red-metrics.json\n````json\n{\n  \"uid\": \"red-metrics-dashboard\",\n  \"title\": \"RED Metrics - Data Pipeline\",\n  \"description\": \"Rate, Errors, Duration metrics for the data pipeline\",\n  \"tags\": [\"red\", \"applications\", \"sre\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 2, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 📊 RED Metrics Overview\\n**R**ate (throughput) | **E**rrors (failures) | **D**uration (latency)   The golden signals for service health\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"🚀 PRODUCER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 2 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Producer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Success\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Failed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Success\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Failed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Producer Latency Percentiles\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 8 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Producer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 10 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_producer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📥 CONSUMER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 17 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency (P95)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 30 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"DB Inserts/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Inserted to DB\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Inserted to DB\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"purple\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 17,\n      \"title\": \"End-to-End Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 18,\n      \"title\": \"Consumer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 25 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_consumer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"💰 BUSINESS METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 35 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Orders/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_orders_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Revenue/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 23,\n      \"title\": \"Avg Order Value\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[5m])) / (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 24,\n      \"title\": \"Orders by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_orders_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"short\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 25,\n      \"title\": \"Revenue by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 18, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_revenue_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"currencyUSD\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 26,\n      \"title\": \"Revenue Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 41 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"legendFormat\": \"{{category}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"lineWidth\": 2, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 27,\n      \"title\": \"Order Value Distribution\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 44 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"Median\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.90, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p90\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p99 (High Value)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/applications/slo-dashboard.json\n````json\n{\n  \"uid\": \"slo-dashboard\",\n  \"title\": \"SLI/SLO Dashboard\",\n  \"description\": \"Service Level Indicators and Objectives monitoring\",\n  \"tags\": [\"slo\", \"sre\", \"reliability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-24h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 🎯 SLI/SLO Dashboard\\n\\n| Service | SLO Target | Error Budget (30d) |\\n|---------|------------|--------------------|\\n| Producer Success Rate | 99.9% | 43.2 min |\\n| Consumer Success Rate | 99.9% | 43.2 min |\\n| Latency P95 | < 100ms (Producer), < 5s (E2E) | - |\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"📊 ERROR BUDGET STATUS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 3 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 6, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Producer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Consumer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 15, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Budget Burn Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 18, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:consumed_percent\",\n          \"legendFormat\": \"Producer Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"error_budget:consumer:consumed_percent\",\n          \"legendFormat\": \"Consumer Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📈 SLI METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 10 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Consumer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"Producer Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"Database Availability\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.95 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Lag\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 500 },\n              { \"color\": \"red\", \"value\": 1000 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"SLI Trends Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"legendFormat\": \"Producer Success Rate\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"legendFormat\": \"Consumer Success Rate\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"99.9\",\n          \"legendFormat\": \"SLO Target (99.9%)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"min\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 99,\n          \"max\": 100\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"SLO Target (99.9%)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Burn Rate Trend\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"legendFormat\": \"Producer (1h)\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"legendFormat\": \"Consumer (1h)\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"1\",\n          \"legendFormat\": \"Normal Rate (1x)\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"14.4\",\n          \"legendFormat\": \"Critical (14.4x)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"x\",\n          \"min\": 0\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Normal Rate (1x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Critical (14.4x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📋 SLO SUMMARY TABLE\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 23 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"SLO Status\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 24 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"E\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true },\n            \"renameByName\": { \n              \"Value #A\": \"Producer Success %\",\n              \"Value #B\": \"Consumer Success %\",\n              \"Value #C\": \"Database Availability %\",\n              \"Value #D\": \"Producer Budget %\",\n              \"Value #E\": \"Consumer Budget %\"\n            }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"md\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"center\", \"displayMode\": \"color-background-solid\" },\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/infrastructure/container-metrics.json\n````json\n{\n  \"uid\": \"container-metrics\",\n  \"title\": \"Container Metrics\",\n  \"description\": \"Docker container resource usage monitoring\",\n  \"tags\": [\"infrastructure\", \"containers\", \"cadvisor\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"container\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(container_cpu_usage_seconds_total{name=~\\\".+\\\"}, name)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Running Containers\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(container_last_seen{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total CPU Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m])) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Memory Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(container_memory_usage_bytes{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 4294967296 }, { \"color\": \"red\", \"value\": 8589934592 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Network In\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_receive_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Network Out\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_transmit_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"CPU Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\"$container\\\"}[5m]) * 100\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Memory Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\"$container\\\"}\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Network I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_network_receive_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Receive\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_network_transmit_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Transmit\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Disk I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_fs_reads_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Read\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_fs_writes_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Write\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Container Resource Table\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m]) * 100\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"container_spec_memory_limit_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true, \"__name__\": true, \"id\": true, \"image\": true, \"instance\": true, \"job\": true },\n            \"renameByName\": { \"name\": \"Container\", \"Value #A\": \"CPU %\", \"Value #B\": \"Memory Used\", \"Value #C\": \"Memory Limit\" }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\",\n        \"footer\": { \"show\": true, \"reducer\": [\"sum\"], \"countRows\": false }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"CPU %\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"percent\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"max\", \"value\": 100 },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 80 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Used\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"bytes\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Limit\" }, \n            \"properties\": [{ \"id\": \"unit\", \"value\": \"bytes\" }] \n          }\n        ]\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/infrastructure/kafka.json\n````json\n{\n  \"uid\": \"kafka-dashboard\",\n  \"title\": \"Kafka Overview\",\n  \"description\": \"Monitoring Apache Kafka cluster health and performance\",\n  \"tags\": [\"infrastructure\", \"kafka\", \"messaging\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"topic\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(kafka_topic_partitions, topic)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Kafka Brokers Up\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_brokers)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total Topics\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Under Replicated Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_under_replicated_partition)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Offline Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_leader{leader=\\\"-1\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Consumer Groups\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(count by (consumergroup) (kafka_consumergroup_current_offset))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Messages In per Second (by Topic)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (topic) (rate(kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}[1m]))\",\n          \"legendFormat\": \"{{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Consumer Group Lag\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (consumergroup, topic) (kafka_consumergroup_lag{topic=~\\\"$topic\\\"})\",\n          \"legendFormat\": \"{{consumergroup}} - {{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1000 }, { \"color\": \"red\", \"value\": 10000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Topic Partitions\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partitions{topic=~\\\"$topic\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"organize\", \"options\": { \"excludeByName\": { \"Time\": true, \"__name__\": true, \"instance\": true, \"job\": true }, \"renameByName\": { \"topic\": \"Topic\", \"Value\": \"Partitions\" } } }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Partitions\" }, \"properties\": [{ \"id\": \"custom.displayMode\", \"value\": \"color-background\" }, { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Current Offset by Partition\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-{{partition}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"stepAfter\", \"fillOpacity\": 0, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Consumer Group Lag Sum\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 8, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"min\": 0,\n          \"max\": 100000,\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5000 }, { \"color\": \"red\", \"value\": 50000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Messages per Partition\",\n      \"type\": \"bargauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 16, \"x\": 8, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-p{{partition}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"orientation\": \"horizontal\",\n        \"displayMode\": \"gradient\",\n        \"showUnfilled\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/infrastructure/postgresql.json\n````json\n{\n  \"uid\": \"postgresql-dashboard\",\n  \"title\": \"PostgreSQL Overview\",\n  \"description\": \"Monitoring PostgreSQL database performance and health\",\n  \"tags\": [\"infrastructure\", \"postgresql\", \"database\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"PostgreSQL Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_up\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Database Size\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_database_size_bytes{datname=\\\"datawarehouse\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1073741824 }, { \"color\": \"red\", \"value\": 5368709120 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Active Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(pg_stat_activity_count{datname=\\\"datawarehouse\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 100 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Max Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_settings_max_connections\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - pg_postmaster_start_time_seconds\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Connections by State\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_activity_count{datname=\\\"datawarehouse\\\"}\",\n          \"legendFormat\": \"{{state}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\", \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Transactions per Second\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_xact_commit{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Commits\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_xact_rollback{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Rollbacks\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Rollbacks\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Rows Operations\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_tup_inserted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Inserted\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_tup_updated{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Updated\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"rate(pg_stat_database_tup_deleted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Deleted\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"rate(pg_stat_database_tup_fetched{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Fetched\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"rowsps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Cache Hit Ratio\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} / (pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} + pg_stat_database_blks_read{datname=\\\"datawarehouse\\\"}) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 80 }, { \"color\": \"green\", \"value\": 95 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Deadlocks\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_deadlocks{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Temp Files Created\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 16 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_temp_files{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 10 }, { \"color\": \"red\", \"value\": 50 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/logs/logs-explorer.json\n````json\n{\n  \"uid\": \"logs-dashboard\",\n  \"title\": \"Logs Explorer\",\n  \"description\": \"Centralized logs from all services\",\n  \"tags\": [\"logs\", \"loki\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"level\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"search\",\n        \"type\": \"textbox\",\n        \"current\": { \"value\": \"\" },\n        \"label\": \"Search\"\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 5, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"fixed\", \"fixedColor\": \"red\" }\n        },\n        \"overrides\": []\n      }\n    },\n     {\n      \"id\": 3,\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | json | level=~\\\".+\\\" | __error__=\\\"\\\" [$__range]))\",\n          \"legendFormat\": \"{{level}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"palette-classic\" },\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"INFO\": { \"color\": \"green\", \"index\": 0 },\n                \"info\": { \"color\": \"green\", \"index\": 1 },\n                \"ERROR\": { \"color\": \"red\", \"index\": 2 },\n                \"error\": { \"color\": \"red\", \"index\": 3 },\n                \"WARNING\": { \"color\": \"yellow\", \"index\": 4 },\n                \"warning\": { \"color\": \"yellow\", \"index\": 5 },\n                \"WARN\": { \"color\": \"yellow\", \"index\": 6 },\n                \"warn\": { \"color\": \"yellow\", \"index\": 7 },\n                \"DEBUG\": { \"color\": \"blue\", \"index\": 8 },\n                \"debug\": { \"color\": \"blue\", \"index\": 9 }\n              }\n            }\n          ]\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 18, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"red\", \"value\": 10 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 11 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"All Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 15, \"w\": 24, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"dedupStrategy\": \"none\",\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 12, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 38 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Order Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 32,\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/logs/test.json\n````json\n{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"description\": \"Centralized logs from all services\",\n  \"editable\": true,\n  \"fiscalYearStartMonth\": 0,\n  \"graphTooltip\": 0,\n  \"id\": 12,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"normal\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 1,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [\n            \"sum\"\n          ],\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"fixedColor\": \"red\",\n            \"mode\": \"fixed\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 70,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"none\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 6\n      },\n      \"id\": 2,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [],\n          \"displayMode\": \"list\",\n          \"placement\": \"bottom\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            }\n          },\n          \"mappings\": [],\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"ERROR\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"red\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"WARNING\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"yellow\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"INFO\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"green\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"DEBUG\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"blue\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          }\n        ]\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 6\n      },\n      \"id\": 3,\n      \"options\": {\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"values\": [\n            \"value\",\n            \"percent\"\n          ]\n        },\n        \"pieType\": \"donut\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"sort\": \"desc\",\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"single\",\n          \"sort\": \"none\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"direction\": \"backward\",\n          \"editorMode\": \"code\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | __error__=\\\"\\\" [$__range]))\",\n          \"instant\": true,\n          \"legendFormat\": \"{{level}}\",\n          \"queryType\": \"range\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [],\n          \"noValue\": \"0\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"yellow\",\n                \"value\": 1\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 10\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 6\n      },\n      \"id\": 4,\n      \"options\": {\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"orientation\": \"auto\",\n        \"percentChangeColorMode\": \"standard\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"showPercentChange\": false,\n        \"textMode\": \"auto\",\n        \"wideLayout\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 11\n      },\n      \"id\": 10,\n      \"panels\": [],\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 15,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"id\": 11,\n      \"options\": {\n        \"dedupStrategy\": \"none\",\n        \"enableInfiniteScrolling\": false,\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showCommonLabels\": false,\n        \"showControls\": false,\n        \"showLabels\": true,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"All Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 27\n      },\n      \"id\": 20,\n      \"panels\": [],\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 28\n      },\n      \"id\": 21,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 28\n      },\n      \"id\": 22,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 38\n      },\n      \"id\": 30,\n      \"panels\": [],\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 39\n      },\n      \"id\": 31,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Order Events\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 39\n      },\n      \"id\": 32,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\"\n    }\n  ],\n  \"preload\": false,\n  \"refresh\": \"10s\",\n  \"schemaVersion\": 42,\n  \"tags\": [\n    \"logs\",\n    \"loki\",\n    \"observability\"\n  ],\n  \"templating\": {\n    \"list\": [\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"service\",\n        \"options\": [],\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"level\",\n        \"options\": [],\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"value\": \"\"\n        },\n        \"label\": \"Search\",\n        \"name\": \"search\",\n        \"type\": \"textbox\"\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {},\n  \"timezone\": \"browser\",\n  \"title\": \"Logs Explorer\",\n  \"uid\": \"logs-dashboard\",\n  \"version\": 3\n}\n````\n\n## File: monitoring/grafana/dashboards/overview/correlation-dashboard.json\n````json\n{\n  \"uid\": \"correlation-dashboard\",\n  \"title\": \"Metrics & Logs Correlation\",\n  \"description\": \"View metrics and related logs side by side\",\n  \"tags\": [\"correlation\", \"metrics\", \"logs\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"name\": \"Errors\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"enable\": true,\n        \"iconColor\": \"red\",\n        \"expr\": \"{service=\\\"$service\\\"} |= \\\"ERROR\\\"\",\n        \"titleFormat\": \"Error\",\n        \"textFormat\": \"{{ __line__ }}\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"📊 Metrics Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Message Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"msg/min\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byRegexp\", \"options\": \"/Error/\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Latency P95\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Producer P95\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Consumer E2E P95\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 8 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Log Volume (matches time range above)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 4, \"w\": 24, \"x\": 0, \"y\": 9 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=\\\"$service\\\"} | json | __error__=\\\"\\\" [$__interval]))\",\n          \"legendFormat\": \"{{level}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"right\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"ERROR\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"INFO\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"WARNING\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Service Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 13 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"}\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔴 Error Logs Only\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 25 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Error Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 26 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"} |~ \\\"(?i)error|ERROR\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/overview/system-overview.json\n````json\n{\n  \"uid\": \"system-overview\",\n  \"title\": \"System Overview\",\n  \"description\": \"Overview of all monitored services\",\n  \"tags\": [\"overview\", \"prometheus\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Services Health\",\n      \"description\": \"Status of all monitored services (1 = UP, 0 = DOWN)\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 4,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up\",\n          \"legendFormat\": \"{{job}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"orientation\": \"horizontal\",\n        \"textMode\": \"auto\",\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"0\": {\n                  \"text\": \"DOWN\",\n                  \"color\": \"red\",\n                  \"index\": 0\n                },\n                \"1\": {\n                  \"text\": \"UP\",\n                  \"color\": \"green\",\n                  \"index\": 1\n                }\n              }\n            }\n          ],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"green\", \"value\": 1 }\n            ]\n          },\n          \"unit\": \"none\"\n        },\n        \"overrides\": []\n      }\n    },\n    \n    {\n      \"id\": 2,\n      \"title\": \"Prometheus Scrape Duration\",\n      \"description\": \"Time taken to scrape each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_duration_seconds\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"line\",\n            \"lineInterpolation\": \"smooth\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"scheme\",\n            \"spanNulls\": false,\n            \"lineWidth\": 2,\n            \"pointSize\": 5,\n            \"showPoints\": \"auto\"\n          },\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 0.5 },\n              { \"color\": \"red\", \"value\": 1 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Samples Scraped\",\n      \"description\": \"Number of metrics scraped from each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_samples_scraped\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"hue\",\n            \"lineWidth\": 1\n          },\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Prometheus Memory Usage\",\n      \"type\": \"gauge\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"process_resident_memory_bytes{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"min\": 0,\n          \"max\": 1073741824,\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 536870912 },\n              { \"color\": \"red\", \"value\": 858993459 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Prometheus Storage Size\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 6,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_storage_blocks_bytes\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 5368709120 },\n              { \"color\": \"red\", \"value\": 10737418240 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Total Time Series\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_head_series\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - process_start_time_seconds{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/tracing/tracing-overview.json\n````json\n{\n  \"uid\": \"tracing-overview\",\n  \"title\": \"Distributed Tracing Overview\",\n  \"description\": \"Jaeger traces visualization and analysis\",\n  \"tags\": [\"tracing\", \"jaeger\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      },\n      {\n        \"name\": \"operation\",\n        \"type\": \"custom\",\n        \"query\": \"all,produce_order,produce_batch,consume_message,process_batch,db_insert_orders\",\n        \"includeAll\": true,\n        \"current\": { \"text\": \"All\", \"value\": \"$__all\" },\n        \"options\": [\n          { \"text\": \"All\", \"value\": \"$__all\", \"selected\": true },\n          { \"text\": \"produce_order\", \"value\": \"produce_order\", \"selected\": false },\n          { \"text\": \"produce_batch\", \"value\": \"produce_batch\", \"selected\": false },\n          { \"text\": \"consume_message\", \"value\": \"consume_message\", \"selected\": false },\n          { \"text\": \"process_batch\", \"value\": \"process_batch\", \"selected\": false },\n          { \"text\": \"db_insert_orders\", \"value\": \"db_insert_orders\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🔍 Tracing Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Trace Count\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[5m])) * 300\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Avg Duration (Producer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Avg Duration (Consumer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"E2E Latency P95\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 10 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Services\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(up{job=~\\\"data-producer|data-consumer\\\"} == 1)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📊 Latency Distribution\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 5 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"End-to-End Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔎 Trace Search\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 14 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Recent Traces\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"jaeger\", \"uid\": \"jaeger\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"queryType\": \"search\",\n          \"service\": \"$service\",\n          \"limit\": 20\n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\" }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Application Logs (with trace_id)\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 24, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} | json | __error__=\\\"\\\" | trace_id =~ \\\".+\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/provisioning/dashboards/dashboards.yml\n````yaml\n# monitoring/grafana/provisioning/dashboards/dashboards.yml\n\napiVersion: 1\n\nproviders:\n  # ===========================================\n  # Overview Dashboards\n  # ===========================================\n  - name: 'overview'\n    orgId: 1\n    folder: 'Overview'\n    folderUid: 'overview'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: true              # ✅  cho edit trên UI\n    options:\n      path: /var/lib/grafana/dashboards/overview\n\n  # ===========================================\n  # Infrastructure Dashboards\n  # ===========================================\n  - name: 'infrastructure'\n    orgId: 1\n    folder: 'Infrastructure'\n    folderUid: 'infrastructure'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/infrastructure\n\n  # ===========================================\n  # Application Dashboards\n  # ===========================================\n  - name: 'applications'\n    orgId: 1\n    folder: 'Applications'\n    folderUid: 'applications'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/applications\n\n  # ===========================================\n  # Logs Dashboards\n  # ===========================================\n  - name: 'logs'\n    orgId: 1\n    folder: 'Logs'\n    folderUid: 'logs'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/logs\n\n  - name: 'tracing'\n    orgId: 1\n    folder: 'Tracing'\n    folderUid: 'tracing'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/tracing\n````\n\n## File: monitoring/grafana/provisioning/datasources/datasources.yml\n````yaml\n# monitoring/grafana/provisioning/datasources/datasources.yml\n\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    uid: prometheus                    # ✅ UID cố định - RẤT QUAN TRỌNG!\n    access: proxy\n    url: http://prometheus:9090\n    isDefault: true\n    editable: true                    # Không cho edit trên UI để tránh drift\n    jsonData:\n      timeInterval: \"15s\"\n      httpMethod: POST                 # POST tốt hơn cho large queries\n\n  - name: Alertmanager\n    type: alertmanager\n    uid: alertmanager\n    access: proxy\n    url: http://alertmanager:9093\n    editable: false\n    jsonData:\n      implementation: prometheus\n\n  - name: Loki\n    type: loki\n    uid: loki\n    access: proxy\n    url: http://loki:3100\n    editable: false\n    jsonData:\n      timeout: 60\n      maxLines: 1000\n      derivedFields:\n        # Link từ logs sang related logs by order_id\n        - name: order_id\n          matcherRegex: '\"order_id\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"loki\",\"queries\":[{\"expr\":\"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"$${__value.raw}\\\"\"}]}'\n          datasourceUid: loki\n          urlDisplayLabel: \"View related logs\"\n        \n        # Link từ logs sang metrics by service\n        - name: service_metrics\n          matcherRegex: '\"service\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"prometheus\",\"queries\":[{\"expr\":\"up{job=\\\"$${__value.raw}\\\"}\"}]}'\n          datasourceUid: prometheus\n          urlDisplayLabel: \"View metrics\"\n\n        - name: TraceID\n          matcherRegex: '\"trace_id\":\\s*\"([a-f0-9]+)\"'\n          url: '$${__value.raw}'\n          datasourceUid: jaeger\n          urlDisplayLabel: \"View Trace\"\n\n  - name: Jaeger\n    type: jaeger\n    uid: jaeger\n    access: proxy\n    url: http://jaeger:16686\n    editable: false\n    jsonData:\n      tracesToLogs:\n        datasourceUid: loki\n        tags: ['service']\n        mappedTags: [{ key: 'service.name', value: 'service' }]\n        mapTagNamesEnabled: true\n        spanStartTimeShift: '-1h'\n        spanEndTimeShift: '1h'\n        filterByTraceID: true\n        filterBySpanID: false\n        lokiSearch: true\n````\n\n## File: monitoring/loki/rules/fake/alerts.yml\n````yaml\n# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"\n````\n\n## File: monitoring/loki/rules/alerts.yml\n````yaml\n# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"\n````\n\n## File: monitoring/loki/loki-config.yml\n````yaml\n# monitoring/loki/loki-config.yml\n\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n  grpc_listen_port: 9096\n  log_level: info\n\ncommon:\n  instance_addr: 127.0.0.1\n  path_prefix: /loki\n  storage:\n    filesystem:\n      chunks_directory: /loki/chunks\n      rules_directory: /loki/rules\n  replication_factor: 1\n  ring:\n    kvstore:\n      store: inmemory\n\nquery_range:\n  results_cache:\n    cache:\n      embedded_cache:\n        enabled: true\n        max_size_mb: 100\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: tsdb\n      object_store: filesystem\n      schema: v13\n      index:\n        prefix: index_\n        period: 24h\n\nstorage_config:\n  filesystem:\n    directory: /loki/storage\n\nlimits_config:\n  retention_period: 168h  # 7 days\n  ingestion_rate_mb: 10\n  ingestion_burst_size_mb: 20\n  max_streams_per_user: 10000\n  max_line_size: 256kb\n\ncompactor:\n  working_directory: /loki/compactor\n  compaction_interval: 10m\n  retention_enabled: true\n  retention_delete_delay: 2h\n  delete_request_store: filesystem\n\nruler:\n  alertmanager_url: http://alertmanager:9093\n  storage:\n    type: local\n    local:\n      directory: /loki/rules\n  rule_path: /loki/rules-temp\n  enable_api: true\n  enable_alertmanager_v2: true\n  ring:\n    kvstore:\n      store: inmemory\n\nanalytics:\n  reporting_enabled: false\n````\n\n## File: monitoring/prometheus/rules/alert_rules.yml\n````yaml\n# monitoring/prometheus/rules/alert_rules.yml\n\ngroups:\n  # ===========================================\n  # Service Health Alerts\n  # ===========================================\n  - name: service_health_alerts\n    rules:\n      # Service Down - Critical\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"{{ $labels.job }} has been down for more than 1 minute.\"\n          runbook_url: \"https://wiki.example.com/runbook/service-down\"\n\n      # Service Flapping - Warning\n      - alert: ServiceFlapping\n        expr: changes(up[10m]) > 3\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Service {{ $labels.job }} is flapping\"\n          description: \"{{ $labels.job }} has changed state {{ $value }} times in the last 10 minutes.\"\n\n  # ===========================================\n  # Data Pipeline Alerts\n  # ===========================================\n  - name: pipeline_alerts\n    rules:\n      # Producer Down\n      - alert: ProducerDown\n        expr: up{job=\"data-producer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n        annotations:\n          summary: \"Data Producer is down\"\n          description: \"Data Producer has been down for more than 1 minute. No data is being produced to Kafka.\"\n\n      # Consumer Down\n      - alert: ConsumerDown\n        expr: up{job=\"data-consumer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: consumer\n        annotations:\n          summary: \"Data Consumer is down\"\n          description: \"Data Consumer has been down for more than 1 minute. No data is being processed.\"\n\n      # Producer Not Producing\n      - alert: ProducerNotProducing\n        expr: rate(pipeline_messages_produced_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer is not producing messages\"\n          description: \"No messages have been produced in the last 5 minutes.\"\n\n      # Consumer Not Consuming\n      - alert: ConsumerNotConsuming\n        expr: rate(pipeline_messages_consumed_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer is not consuming messages\"\n          description: \"No messages have been consumed in the last 5 minutes.\"\n\n      # High Error Rate - Producer\n      - alert: ProducerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_produced_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_produced_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer error rate is high\"\n          description: \"Producer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Error Rate - Consumer\n      - alert: ConsumerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_consumed_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_consumed_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer error rate is high\"\n          description: \"Consumer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Processing Latency\n      - alert: HighProcessingLatency\n        expr: pipeline:processing_latency_p95:seconds > 1\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Processing latency is high\"\n          description: \"P95 processing latency is {{ $value | printf \\\"%.2f\\\" }}s (threshold: 1s).\"\n\n  # ===========================================\n  # Kafka Alerts\n  # ===========================================\n  - name: kafka_alerts\n    rules:\n      # Kafka Down\n      - alert: KafkaDown\n        expr: up{job=\"kafka\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka exporter is down\"\n          description: \"Cannot scrape Kafka metrics. Kafka might be down.\"\n\n      # High Consumer Lag\n      - alert: KafkaHighConsumerLag\n        expr: kafka:consumer_lag:sum > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is high\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 1000).\"\n\n      # Critical Consumer Lag\n      - alert: KafkaCriticalConsumerLag\n        expr: kafka:consumer_lag:sum > 10000\n        for: 5m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is critical\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 10000). Consumer might be stuck.\"\n\n      # No Messages Flowing\n      - alert: KafkaNoMessagesFlowing\n        expr: kafka:messages_in:rate1m:total == 0\n        for: 10m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"No messages flowing through Kafka\"\n          description: \"No messages have been produced to Kafka in the last 10 minutes.\"\n\n  # ===========================================\n  # PostgreSQL Alerts\n  # ===========================================\n  - name: postgresql_alerts\n    rules:\n      # PostgreSQL Down\n      - alert: PostgreSQLDown\n        expr: pg_up == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL is down\"\n          description: \"PostgreSQL database is not responding.\"\n\n      # High Connection Usage\n      - alert: PostgreSQLHighConnections\n        expr: postgresql:connections:usage_percent > 80\n        for: 5m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is high\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical Connection Usage\n      - alert: PostgreSQLCriticalConnections\n        expr: postgresql:connections:usage_percent > 95\n        for: 2m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is critical\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%). New connections may fail.\"\n\n      # Low Cache Hit Ratio\n      - alert: PostgreSQLLowCacheHitRatio\n        expr: postgresql:cache_hit_ratio:percent < 90\n        for: 10m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL cache hit ratio is low\"\n          description: \"Cache hit ratio is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 90%). Consider increasing shared_buffers.\"\n\n      # Deadlocks Detected\n      - alert: PostgreSQLDeadlocks\n        expr: increase(pg_stat_database_deadlocks{datname=\"datawarehouse\"}[5m]) > 0\n        for: 0m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL deadlocks detected\"\n          description: \"{{ $value }} deadlocks detected in the last 5 minutes.\"\n\n  # ===========================================\n  # Container Resource Alerts (Fixed)\n  # ===========================================\n  - name: container_alerts\n    rules:\n      # High CPU Usage\n      - alert: ContainerHighCPU\n        expr: container:cpu_usage_percent:rate5m > 80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical CPU Usage\n      - alert: ContainerCriticalCPU\n        expr: container:cpu_usage_percent:rate5m > 95\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%).\"\n\n      # High Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerHighMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 80\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 80%).\"\n\n      # Critical Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerCriticalMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 95\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 95%). OOM kill imminent.\"\n\n      # High Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerHighMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 1073741824\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} using high memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 1GB).\"\n\n      # Critical Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerCriticalMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 2147483648\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} using critical memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 2GB).\"\n\n      # Container Restarting\n      - alert: ContainerRestarting\n        expr: increase(container_restart_count{name=~\".+\"}[1h]) > 3\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} is restarting frequently\"\n          description: \"Container has restarted {{ $value | printf \\\"%.0f\\\" }} times in the last hour.\"\n````\n\n## File: monitoring/prometheus/rules/recording_rules.yml\n````yaml\n# monitoring/prometheus/rules/recording_rules.yml\n\ngroups:\n  # ===========================================\n  # Container Metrics - Pre-computed\n  # ===========================================\n  - name: container_metrics\n    interval: 15s\n    rules:\n      - record: container:cpu_usage_percent:rate5m\n        expr: rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]) * 100\n\n      - record: container:memory_usage_bytes:current\n        expr: container_memory_usage_bytes{name=~\".+\"}\n\n      - record: container:memory_usage_percent:current\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} / \n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n\n      - record: container:network_receive_bytes:rate5m\n        expr: rate(container_network_receive_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:network_transmit_bytes:rate5m\n        expr: rate(container_network_transmit_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:cpu_usage_percent:total\n        expr: sum(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m])) * 100\n\n      - record: container:memory_usage_bytes:total\n        expr: sum(container_memory_usage_bytes{name=~\".+\"})\n\n      - record: container:count:total\n        expr: count(container_last_seen{name=~\".+\"})\n\n  # ===========================================\n  # RED Metrics - Producer\n  # ===========================================\n  - name: red_producer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:producer:rate_per_second\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:producer:rate_per_minute\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:producer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_producer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:producer:errors_per_minute\n        expr: sum(rate(pipeline_producer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:producer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n  # ===========================================\n  # RED Metrics - Consumer\n  # ===========================================\n  - name: red_consumer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:consumer:rate_per_second\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:consumer:rate_per_minute\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m])) * 60\n\n      - record: red:consumer:db_inserts_per_minute\n        expr: sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:consumer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_consumer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:consumer:errors_per_minute\n        expr: sum(rate(pipeline_consumer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:consumer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      # End-to-End Latency\n      - record: red:e2e:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n  # ===========================================\n  # Business Metrics\n  # ===========================================\n  - name: business_metrics\n    interval: 15s\n    rules:\n      - record: business:orders:rate_per_minute\n        expr: sum(rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue:rate_per_minute\n        expr: sum(rate(pipeline_business_revenue_total[1m])) * 60\n\n      - record: business:avg_order_value\n        expr: |\n          sum(rate(pipeline_business_revenue_total[5m])) /\n          (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\n\n      - record: business:orders_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\n\n  # ===========================================\n  # Kafka Metrics\n  # ===========================================\n  - name: kafka_metrics\n    interval: 15s\n    rules:\n      - record: kafka:messages_in:rate1m\n        expr: sum by (topic) (rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:messages_in:rate1m:total\n        expr: sum(rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:consumer_lag:total\n        expr: sum by (consumergroup, topic) (kafka_consumergroup_lag)\n\n      - record: kafka:consumer_lag:sum\n        expr: sum(kafka_consumergroup_lag)\n\n      - record: kafka:partitions:count\n        expr: sum(kafka_topic_partitions)\n\n  # ===========================================\n  # PostgreSQL Metrics\n  # ===========================================\n  - name: postgresql_metrics\n    interval: 15s\n    rules:\n      - record: postgresql:transactions:rate1m\n        expr: rate(pg_stat_database_xact_commit{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_inserted:rate1m\n        expr: rate(pg_stat_database_tup_inserted{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_fetched:rate1m\n        expr: rate(pg_stat_database_tup_fetched{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:connections:active\n        expr: sum(pg_stat_activity_count{datname=\"datawarehouse\"})\n\n      - record: postgresql:connections:usage_percent\n        expr: |\n          (sum(pg_stat_activity_count{datname=\"datawarehouse\"}) / \n           pg_settings_max_connections) * 100\n\n      - record: postgresql:cache_hit_ratio:percent\n        expr: |\n          (\n            pg_stat_database_blks_hit{datname=\"datawarehouse\"} /\n            (pg_stat_database_blks_hit{datname=\"datawarehouse\"} + \n             pg_stat_database_blks_read{datname=\"datawarehouse\"} + 0.001)\n          ) * 100\n\n      - record: postgresql:database_size:bytes\n        expr: pg_database_size_bytes{datname=\"datawarehouse\"}\n\n  # ===========================================\n  # Service Health\n  # ===========================================\n  - name: service_health\n    interval: 15s\n    rules:\n      - record: service:up:status\n        expr: up\n\n      - record: service:healthy:count\n        expr: count(up == 1)\n\n      - record: service:unhealthy:count\n        expr: count(up == 0)\n\n      - record: service:health:percent\n        expr: (count(up == 1) / count(up)) * 100\n````\n\n## File: monitoring/prometheus/rules/slo_alerts.yml\n````yaml\n# monitoring/prometheus/rules/slo_alerts.yml\n\ngroups:\n  # ===========================================\n  # Error Budget Alerts\n  # ===========================================\n  - name: error_budget_alerts\n    rules:\n      # Producer error budget low\n      - alert: ProducerErrorBudgetLow\n        expr: error_budget:producer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is running low\"\n          description: \"Producer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      # Producer error budget critical\n      - alert: ProducerErrorBudgetCritical\n        expr: error_budget:producer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is critically low\"\n          description: \"Producer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining. Consider freezing deployments.\"\n\n      # Producer error budget exhausted\n      - alert: ProducerErrorBudgetExhausted\n        expr: error_budget:producer:remaining_percent <= 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget exhausted!\"\n          description: \"Producer has exhausted its error budget. SLO is being violated.\"\n\n      # Consumer error budget alerts\n      - alert: ConsumerErrorBudgetLow\n        expr: error_budget:consumer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is running low\"\n          description: \"Consumer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      - alert: ConsumerErrorBudgetCritical\n        expr: error_budget:consumer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is critically low\"\n          description: \"Consumer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining.\"\n\n  # ===========================================\n  # Burn Rate Alerts (Multi-window)\n  # ===========================================\n  - name: burn_rate_alerts\n    rules:\n      # Fast burn - will exhaust budget in ~2 hours\n      # 1h window with 14.4x burn rate\n      - alert: ProducerHighBurnRate\n        expr: |\n          burn_rate:producer:1h > 14.4\n          and\n          burn_rate:producer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer is burning error budget too fast\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. At this rate, error budget will be exhausted in ~2 hours.\"\n\n      # Slow burn - will exhaust budget in ~1 day\n      - alert: ProducerModerateBurnRate\n        expr: |\n          burn_rate:producer:6h > 6\n          and\n          burn_rate:producer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer burn rate is elevated\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. Error budget may be exhausted within a day.\"\n\n      # Consumer burn rate alerts\n      - alert: ConsumerHighBurnRate\n        expr: |\n          burn_rate:consumer:1h > 14.4\n          and\n          burn_rate:consumer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer is burning error budget too fast\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n      - alert: ConsumerModerateBurnRate\n        expr: |\n          burn_rate:consumer:6h > 6\n          and\n          burn_rate:consumer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer burn rate is elevated\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n  # ===========================================\n  # SLO Violation Alerts\n  # ===========================================\n  - name: slo_violation_alerts\n    rules:\n      # Producer SLO violations\n      - alert: ProducerSLOViolation\n        expr: sli:producer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer SLO is being violated\"\n          description: \"Producer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ProducerLatencySLOViolation\n        expr: sli:producer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of requests are under 100ms (target: 99%)\"\n\n      # Consumer SLO violations\n      - alert: ConsumerSLOViolation\n        expr: sli:consumer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer SLO is being violated\"\n          description: \"Consumer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ConsumerLatencySLOViolation\n        expr: sli:consumer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer E2E latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of messages are processed under 5s (target: 99%)\"\n\n      # Data freshness SLO\n      - alert: DataFreshnessSLOViolation\n        expr: sum(kafka_consumergroup_lag) > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: pipeline\n          category: slo\n        annotations:\n          summary: \"Data freshness SLO is being violated\"\n          description: \"Consumer lag is {{ $value }} messages (threshold: 1000)\"\n````\n\n## File: monitoring/prometheus/rules/slo_rules.yml\n````yaml\n# monitoring/prometheus/rules/slo_rules.yml\n\ngroups:\n  # ===========================================\n  # SLI Definitions\n  # ===========================================\n  - name: sli_metrics\n    interval: 30s\n    rules:\n      # --- Producer SLIs ---\n      \n      # Availability SLI: % of time producer is up\n      - record: sli:producer:availability\n        expr: avg_over_time(up{job=\"data-producer\"}[5m])\n\n      # Success Rate SLI: % of successful messages\n      - record: sli:producer:success_rate\n        expr: |\n          sum(rate(pipeline_producer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of requests under threshold (100ms)\n      - record: sli:producer:latency_good\n        expr: |\n          sum(rate(pipeline_producer_message_duration_seconds_bucket{le=\"0.1\"}[5m])) /\n          (sum(rate(pipeline_producer_message_duration_seconds_count[5m])) + 0.001)\n\n      # --- Consumer SLIs ---\n      \n      # Availability SLI\n      - record: sli:consumer:availability\n        expr: avg_over_time(up{job=\"data-consumer\"}[5m])\n\n      # Success Rate SLI\n      - record: sli:consumer:success_rate\n        expr: |\n          sum(rate(pipeline_consumer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of E2E latency under threshold (5s)\n      - record: sli:consumer:latency_good\n        expr: |\n          sum(rate(pipeline_consumer_end_to_end_latency_seconds_bucket{le=\"5.0\"}[5m])) /\n          (sum(rate(pipeline_consumer_end_to_end_latency_seconds_count[5m])) + 0.001)\n\n      # --- Pipeline SLIs ---\n      \n      # Data Freshness SLI: Consumer lag < 1000 messages\n      - record: sli:pipeline:data_freshness\n        expr: |\n          (sum(kafka_consumergroup_lag) < 1000) or vector(0)\n\n      # --- Database SLIs ---\n      \n      # Availability SLI\n      - record: sli:database:availability\n        expr: avg_over_time(pg_up[5m])\n\n      # Query Success Rate\n      - record: sli:database:query_success_rate\n        expr: |\n          sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_db_operations_total[5m])) + 0.001)\n\n  # ===========================================\n  # SLO Calculations (Rolling Windows)\n  # ===========================================\n  - name: slo_calculations\n    interval: 1m\n    rules:\n      # --- Producer SLOs ---\n      \n      # 30-day rolling availability (target: 99.9%)\n      - record: slo:producer:availability_30d\n        expr: avg_over_time(sli:producer:availability[30d])\n\n      # 30-day rolling success rate (target: 99.9%)\n      - record: slo:producer:success_rate_30d\n        expr: avg_over_time(sli:producer:success_rate[30d])\n\n      # 30-day rolling latency compliance (target: 99%)\n      - record: slo:producer:latency_compliance_30d\n        expr: avg_over_time(sli:producer:latency_good[30d])\n\n      # --- Consumer SLOs ---\n      \n      - record: slo:consumer:availability_30d\n        expr: avg_over_time(sli:consumer:availability[30d])\n\n      - record: slo:consumer:success_rate_30d\n        expr: avg_over_time(sli:consumer:success_rate[30d])\n\n      - record: slo:consumer:latency_compliance_30d\n        expr: avg_over_time(sli:consumer:latency_good[30d])\n\n      # --- Database SLOs ---\n      \n      - record: slo:database:availability_30d\n        expr: avg_over_time(sli:database:availability[30d])\n\n      - record: slo:database:query_success_rate_30d\n        expr: avg_over_time(sli:database:query_success_rate[30d])\n\n  # ===========================================\n  # Error Budget Calculations\n  # ===========================================\n  - name: error_budget\n    interval: 1m\n    rules:\n      # --- Producer Error Budget ---\n      \n      # Error budget remaining (target 99.9% = 0.1% budget)\n      # Formula: (SLO - (1 - current_success_rate)) / (1 - SLO)\n      - record: error_budget:producer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:producer:success_rate)) / 0.001\n          ) * 100\n\n      # Error budget consumed\n      - record: error_budget:producer:consumed_percent\n        expr: 100 - error_budget:producer:remaining_percent\n\n      # --- Consumer Error Budget ---\n      \n      - record: error_budget:consumer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:consumer:success_rate)) / 0.001\n          ) * 100\n\n      - record: error_budget:consumer:consumed_percent\n        expr: 100 - error_budget:consumer:remaining_percent\n\n      # --- Combined Pipeline Error Budget ---\n      \n      - record: error_budget:pipeline:remaining_percent\n        expr: |\n          (\n            error_budget:producer:remaining_percent + \n            error_budget:consumer:remaining_percent\n          ) / 2\n\n  # ===========================================\n  # Burn Rate Calculations\n  # ===========================================\n  - name: burn_rate\n    interval: 1m\n    rules:\n      # Burn rate = actual error rate / allowed error rate\n      # If burn rate > 1, we're consuming budget faster than allowed\n      \n      # Producer burn rate (1h window)\n      - record: burn_rate:producer:1h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[1h])) / 0.001\n\n      # Producer burn rate (6h window)\n      - record: burn_rate:producer:6h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[6h])) / 0.001\n\n      # Consumer burn rate (1h window)\n      - record: burn_rate:consumer:1h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[1h])) / 0.001\n\n      # Consumer burn rate (6h window)\n      - record: burn_rate:consumer:6h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[6h])) / 0.001\n````\n\n## File: monitoring/prometheus/prometheus.yml\n````yaml\n# monitoring/prometheus/prometheus.yml\n\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    monitor: 'data-pipeline-monitor'\n\notlp:\n  promote_resource_attributes:\n    - service.name\n    - service.namespace\n    - service.instance.id\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n            - alertmanager:9093\n\nrule_files:\n  - /etc/prometheus/rules/*.yml\n\nscrape_configs:\n  # ===========================================\n  # Monitoring Stack\n  # ===========================================\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n        labels:\n          service: 'prometheus'\n          layer: 'monitoring'\n\n  - job_name: 'grafana'\n    static_configs:\n      - targets: ['grafana:3000']\n        labels:\n          service: 'grafana'\n          layer: 'monitoring'\n\n  - job_name: 'alertmanager'\n    static_configs:\n      - targets: ['alertmanager:9093']\n        labels:\n          service: 'alertmanager'\n          layer: 'monitoring'\n\n  - job_name: 'cadvisor'\n    static_configs:\n      - targets: ['cadvisor:8080']\n        labels:\n          service: 'cadvisor'\n          layer: 'monitoring'\n    metric_relabel_configs:\n      # Giữ lại metrics của containers trong docker-compose project\n      - source_labels: [container_label_com_docker_compose_project]\n        regex: '.+'\n        action: keep\n      # Chỉ drop các metrics không cần thiết (bỏ container_last_seen khỏi list)\n      - source_labels: [__name__]\n        regex: 'container_(tasks_state|memory_failures_total)'\n        action: drop\n\n  # ===========================================\n  # Infrastructure\n  # ===========================================\n  - job_name: 'postgresql'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n        labels:\n          service: 'postgresql'\n          layer: 'infrastructure'\n          database: 'datawarehouse'\n\n  - job_name: 'kafka'\n    static_configs:\n      - targets: ['kafka-exporter:9308']\n        labels:\n          service: 'kafka'\n          layer: 'infrastructure'\n\n  # ===========================================\n  # Applications\n  # ===========================================\n  - job_name: 'data-producer'\n    static_configs:\n      - targets: ['data-producer:8000']\n        labels:\n          service: 'producer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  - job_name: 'data-consumer'\n    static_configs:\n      - targets: ['data-consumer:8001']\n        labels:\n          service: 'consumer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  # ===========================================\n  # stress test\n  # ===========================================\n  - job_name: 'load-test'\n    static_configs:\n      - targets: ['load-test:8002']\n        labels:\n          service: 'load-test'\n          layer: 'testing'\n    scrape_interval: 5s\n````\n\n## File: monitoring/promtail/promtail-config.yml\n````yaml\n# monitoring/promtail/promtail-config.yml\n\nserver:\n  http_listen_port: 9080\n  grpc_listen_port: 0\n  log_level: info\n\npositions:\n  filename: /tmp/positions.yaml\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n    tenant_id: fake\n\nscrape_configs:\n  # ===========================================\n  # Docker Container Logs\n  # ===========================================\n  - job_name: docker\n    docker_sd_configs:\n      - host: unix:///var/run/docker.sock\n        refresh_interval: 5s\n    relabel_configs:\n      # Lấy container name làm label\n      - source_labels: ['__meta_docker_container_name']\n        regex: '/(.*)'\n        target_label: 'container'\n      \n      # Lấy compose service name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'service'\n      \n      # Lấy compose project name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        target_label: 'project'\n      \n      # Lấy container ID\n      - source_labels: ['__meta_docker_container_id']\n        target_label: 'container_id'\n      \n      # Thêm job label\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'job'\n      \n      # Filter: chỉ lấy logs từ containers có compose project\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        regex: '.+'\n        action: keep\n\n    pipeline_stages:\n      # ===========================================\n      # Parse JSON logs (từ Producer/Consumer)\n      # ===========================================\n      - match:\n          selector: '{service=~\"data-producer|data-consumer\"}'\n          stages:\n            - json:\n                expressions:\n                  level: level\n                  message: message\n                  event: event\n                  service: service\n                  timestamp: timestamp\n                  order_id: order_id\n                  category: category\n                  error: error\n                  error_type: error_type\n                  batch_size: batch_size\n                  duration_ms: duration_ms\n                  throughput_per_sec: throughput_per_sec\n            \n            # Set log level as label\n            - labels:\n                level:\n                event:\n            \n            # Set timestamp from log\n            - timestamp:\n                source: timestamp\n                format: RFC3339Nano\n                fallback_formats:\n                  - RFC3339\n            \n            # Extract metrics from logs (optional)\n            - metrics:\n                log_lines_total:\n                  type: Counter\n                  description: \"Total log lines\"\n                  source: message\n                  config:\n                    action: inc\n                    match_all: true\n\n      # ===========================================\n      # Parse Kafka logs\n      # ===========================================\n      - match:\n          selector: '{service=\"kafka\"}'\n          stages:\n            - regex:\n                expression: '^\\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})\\] (?P<level>\\w+) (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Parse PostgreSQL logs\n      # ===========================================\n      - match:\n          selector: '{service=\"postgres\"}'\n          stages:\n            - regex:\n                expression: '^(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3} \\w+) \\[(?P<pid>\\d+)\\] (?P<level>\\w+):  (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Default: keep raw log\n      # ===========================================\n      - match:\n          selector: '{level=\"\"}'\n          stages:\n            - static_labels:\n                level: info\n````\n\n## File: monitoring/stress-testing/load-test/Dockerfile\n````\n# stress-testing/load-test/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY load_test.py .\n\nENTRYPOINT [\"python\", \"load_test.py\"]\nCMD [\"--help\"]\n````\n\n## File: monitoring/stress-testing/load-test/load_test.py\n````python\n# stress-testing/load-test/load_test.py\n\nimport json\nimport random\nimport time\nimport uuid\nimport threading\nimport signal\nimport sys\nfrom datetime import datetime, timezone\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any\n\nimport click\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\n\n# ===========================================\n# Metrics\n# ===========================================\nMESSAGES_SENT = Counter(\n    'loadtest_messages_sent_total',\n    'Total messages sent',\n    ['status']\n)\n\nSEND_DURATION = Histogram(\n    'loadtest_send_duration_seconds',\n    'Time to send message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)\n)\n\nCURRENT_RPS = Gauge(\n    'loadtest_current_rps',\n    'Current requests per second'\n)\n\nACTIVE_THREADS = Gauge(\n    'loadtest_active_threads',\n    'Number of active threads'\n)\n\nTARGET_RPS = Gauge(\n    'loadtest_target_rps',\n    'Target requests per second'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order() -> dict[str, Any]:\n    \"\"\"Generate a fake order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    return {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n        \"load_test\": True,\n    }\n\n\n# ===========================================\n# Load Tester\n# ===========================================\nclass LoadTester:\n    def __init__(self, bootstrap_servers: str, topic: str):\n        self.bootstrap_servers = bootstrap_servers\n        self.topic = topic\n        self.producer = None\n        self.running = False\n        self.stats = {\n            \"sent\": 0,\n            \"failed\": 0,\n            \"start_time\": None,\n        }\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka\"\"\"\n        max_retries = 10\n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=self.bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks=1,  # Faster for load testing\n                    linger_ms=5,\n                    batch_size=16384,\n                    buffer_memory=33554432,\n                )\n                click.echo(f\"✅ Connected to Kafka: {self.bootstrap_servers}\")\n                return\n            except KafkaError as e:\n                click.echo(f\"⏳ Kafka connection attempt {attempt + 1}/{max_retries}: {e}\")\n                time.sleep(2)\n        \n        raise Exception(\"Failed to connect to Kafka\")\n    \n    def send_message(self) -> bool:\n        \"\"\"Send a single message\"\"\"\n        order = generate_order()\n        \n        try:\n            start = time.time()\n            future = self.producer.send(\n                self.topic,\n                key=order[\"order_id\"],\n                value=order\n            )\n            future.get(timeout=10)\n            duration = time.time() - start\n            \n            MESSAGES_SENT.labels(status=\"success\").inc()\n            SEND_DURATION.observe(duration)\n            self.stats[\"sent\"] += 1\n            return True\n            \n        except Exception as e:\n            MESSAGES_SENT.labels(status=\"failed\").inc()\n            self.stats[\"failed\"] += 1\n            return False\n    \n    def run_constant_load(self, rps: int, duration_seconds: int, threads: int = 10):\n        \"\"\"Run constant load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        TARGET_RPS.set(rps)\n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n🚀 Starting Constant Load Test\")\n        click.echo(f\"   Target RPS: {rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        interval = 1.0 / rps if rps > 0 else 1\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                batch_start = time.time()\n                futures = []\n                \n                # Submit batch of requests\n                for _ in range(min(rps, 100)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                # Wait for completion\n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                # Calculate actual RPS\n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                # Print progress\n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Sent: {self.stats['sent']:,} | \"\n                    f\"Failed: {self.stats['failed']:,} | \"\n                    f\"RPS: {actual_rps:.1f}\",\n                    nl=False\n                )\n                \n                # Rate limiting\n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_ramp_up(self, start_rps: int, end_rps: int, duration_seconds: int, threads: int = 20):\n        \"\"\"Run ramp-up load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n📈 Starting Ramp-Up Load Test\")\n        click.echo(f\"   Start RPS: {start_rps}\")\n        click.echo(f\"   End RPS: {end_rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        rps_increment = (end_rps - start_rps) / duration_seconds\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                elapsed = time.time() - self.stats[\"start_time\"]\n                current_target_rps = int(start_rps + (rps_increment * elapsed))\n                TARGET_RPS.set(current_target_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_target_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Target RPS: {current_target_rps:3d} | \"\n                    f\"Actual RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_spike(self, base_rps: int, spike_rps: int, spike_duration: int, total_duration: int, threads: int = 20):\n        \"\"\"Run spike test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n⚡ Starting Spike Test\")\n        click.echo(f\"   Base RPS: {base_rps}\")\n        click.echo(f\"   Spike RPS: {spike_rps}\")\n        click.echo(f\"   Spike Duration: {spike_duration}s\")\n        click.echo(f\"   Total Duration: {total_duration}s\")\n        click.echo(\"-\" * 50)\n        \n        end_time = time.time() + total_duration\n        spike_start = time.time() + (total_duration - spike_duration) / 2\n        spike_end = spike_start + spike_duration\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                current_time = time.time()\n                \n                # Determine current RPS\n                if spike_start <= current_time <= spike_end:\n                    current_rps = spike_rps\n                    phase = \"🔥 SPIKE\"\n                else:\n                    current_rps = base_rps\n                    phase = \"📊 BASE \"\n                \n                TARGET_RPS.set(current_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r{phase} | Remaining: {remaining:3d}s | \"\n                    f\"RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def _print_summary(self):\n        \"\"\"Print test summary\"\"\"\n        elapsed = time.time() - self.stats[\"start_time\"]\n        total = self.stats[\"sent\"] + self.stats[\"failed\"]\n        success_rate = (self.stats[\"sent\"] / total * 100) if total > 0 else 0\n        avg_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n        \n        click.echo(\"\\n\")\n        click.echo(\"=\" * 50)\n        click.echo(\"📊 LOAD TEST SUMMARY\")\n        click.echo(\"=\" * 50)\n        click.echo(f\"   Duration:     {elapsed:.1f}s\")\n        click.echo(f\"   Total Sent:   {self.stats['sent']:,}\")\n        click.echo(f\"   Failed:       {self.stats['failed']:,}\")\n        click.echo(f\"   Success Rate: {success_rate:.2f}%\")\n        click.echo(f\"   Avg RPS:      {avg_rps:.1f}\")\n        click.echo(\"=\" * 50)\n    \n    def stop(self):\n        \"\"\"Stop the test\"\"\"\n        self.running = False\n        if self.producer:\n            self.producer.close()\n\n\n# ===========================================\n# CLI\n# ===========================================\n@click.group()\ndef cli():\n    \"\"\"Load Testing Tool for Data Pipeline\"\"\"\n    pass\n\n\n@cli.command()\n@click.option('--rps', default=50, help='Requests per second')\n@click.option('--duration', default=60, help='Duration in seconds')\n@click.option('--threads', default=10, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef constant(rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run constant load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_constant_load(rps, duration, threads)\n\n\n@cli.command()\n@click.option('--start-rps', default=10, help='Starting RPS')\n@click.option('--end-rps', default=100, help='Ending RPS')\n@click.option('--duration', default=120, help='Duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef rampup(start_rps, end_rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run ramp-up load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_ramp_up(start_rps, end_rps, duration, threads)\n\n\n@cli.command()\n@click.option('--base-rps', default=20, help='Base RPS')\n@click.option('--spike-rps', default=200, help='Spike RPS')\n@click.option('--spike-duration', default=30, help='Spike duration in seconds')\n@click.option('--total-duration', default=120, help='Total duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef spike(base_rps, spike_rps, spike_duration, total_duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run spike test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_spike(base_rps, spike_rps, spike_duration, total_duration, threads)\n\n\nif __name__ == \"__main__\":\n    cli()\n````\n\n## File: monitoring/stress-testing/load-test/requirements.txt\n````\n# stress-testing/load-test/requirements.txt\n\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data\nfaker==24.4.0\n\n# CLI\nclick==8.1.7\n\n# Metrics\nprometheus-client==0.20.0\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n````\n\n## File: monitoring/stress-testing/scripts/chaos_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/chaos_test.sh\n\necho \"💥 Starting Chaos Test\"\necho \"======================\"\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\ncd \"$(dirname \"$0\")/../../\"\n\n# Function to check service health\ncheck_health() {\n    local service=$1\n    local url=$2\n    if curl -s \"$url\" > /dev/null 2>&1; then\n        echo -e \"${GREEN}✅ $service is UP${NC}\"\n        return 0\n    else\n        echo -e \"${RED}❌ $service is DOWN${NC}\"\n        return 1\n    fi\n}\n\n# Function to wait for recovery\nwait_for_recovery() {\n    local service=$1\n    local url=$2\n    local max_wait=60\n    local waited=0\n    \n    echo -e \"${YELLOW}⏳ Waiting for $service to recover...${NC}\"\n    \n    while [ $waited -lt $max_wait ]; do\n        if curl -s \"$url\" > /dev/null 2>&1; then\n            echo -e \"${GREEN}✅ $service recovered after ${waited}s${NC}\"\n            return 0\n        fi\n        sleep 2\n        waited=$((waited + 2))\n    done\n    \n    echo -e \"${RED}❌ $service did not recover within ${max_wait}s${NC}\"\n    return 1\n}\n\necho \"\"\necho \"📊 Initial Health Check\"\necho \"-----------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\ncheck_health \"Prometheus\" \"http://localhost:9090/-/healthy\"\ncheck_health \"Grafana\" \"http://localhost:3000/api/health\"\n\necho \"\"\necho \"💥 Test 1: Kill Consumer\"\necho \"------------------------\"\ndocker stop data-consumer\necho -e \"${YELLOW}Consumer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Consumer...\"\ndocker start data-consumer\nwait_for_recovery \"Consumer\" \"http://localhost:8001/metrics\"\n\necho \"\"\necho \"💥 Test 2: Kill Producer\"\necho \"------------------------\"\ndocker stop data-producer\necho -e \"${YELLOW}Producer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Producer...\"\ndocker start data-producer\nwait_for_recovery \"Producer\" \"http://localhost:8000/metrics\"\n\necho \"\"\necho \"💥 Test 3: Kill Kafka (WARNING: This will affect data flow)\"\necho \"------------------------------------------------------------\"\nread -p \"Do you want to proceed? (y/N) \" -n 1 -r\necho\nif [[ $REPLY =~ ^[Yy]$ ]]; then\n    docker stop kafka\n    echo -e \"${YELLOW}Kafka stopped. Check Grafana for cascading failures...${NC}\"\n    echo \"Waiting 60 seconds...\"\n    sleep 60\n    \n    echo \"\"\n    echo \"🔄 Restarting Kafka...\"\n    docker start kafka\n    sleep 10\n    wait_for_recovery \"Kafka\" \"http://localhost:9308/metrics\"\nfi\n\necho \"\"\necho \"📊 Final Health Check\"\necho \"---------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\n\necho \"\"\necho \"✅ Chaos Test Complete!\"\necho \"Check Grafana dashboards and Alertmanager for results.\"\n````\n\n## File: monitoring/stress-testing/scripts/ramp_up_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/ramp_up_test.sh\n\necho \"📈 Starting Ramp-Up Load Test\"\necho \"==============================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test rampup \\\n    --start-rps ${START_RPS:-10} \\\n    --end-rps ${END_RPS:-100} \\\n    --duration ${DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n````\n\n## File: monitoring/stress-testing/scripts/run_load_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/run_load_test.sh\n\necho \"🚀 Starting Constant Load Test\"\necho \"================================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test constant \\\n    --rps ${RPS:-50} \\\n    --duration ${DURATION:-60} \\\n    --threads ${THREADS:-10} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n````\n\n## File: monitoring/stress-testing/scripts/spike_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/spike_test.sh\n\necho \"⚡ Starting Spike Test\"\necho \"======================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test spike \\\n    --base-rps ${BASE_RPS:-20} \\\n    --spike-rps ${SPIKE_RPS:-200} \\\n    --spike-duration ${SPIKE_DURATION:-30} \\\n    --total-duration ${TOTAL_DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n````\n\n## File: monitoring/stress-testing/docker-compose-stress-test.yml\n````yaml\n# stress-testing/docker-compose.yml\n\nservices:\n  load-test:\n    build:\n      context: ./load-test\n      dockerfile: Dockerfile\n    container_name: load-test\n    networks:\n      - monitoring-net\n    ports:\n      - \"8002:8002\"\n    environment:\n      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092\n      - KAFKA_TOPIC=ecommerce.orders\n    # Command sẽ được override khi chạy\n    command: [\"--help\"]\n\nnetworks:\n  monitoring-net:\n    external: true\n````\n\n## File: monitoring/docker-compose-monitoring.yml\n````yaml\n# monitoring/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Prometheus - Metrics Collection & Storage\n  # ===========================================\n  prometheus:\n    image: prom/prometheus:v3.5.0\n    container_name: prometheus\n    restart: unless-stopped\n    \n    # Command line arguments\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention.time=15d'        # Giữ data 15 ngày\n      - '--web.enable-lifecycle'                   # Cho phép reload config via API\n      - '--web.enable-admin-api'                   # Enable admin API\n      - '--web.enable-remote-write-receiver'\n    volumes:\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - ./prometheus/rules:/etc/prometheus/rules:ro\n      - prometheus_data:/prometheus\n    \n    ports:\n      - \"9090:9090\"\n    \n    networks:\n      - monitoring-net\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9090/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Alertmanager\n  # ===========================================\n  alertmanager:\n    image: prom/alertmanager:v0.27.0\n    container_name: alertmanager\n    restart: unless-stopped\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n      - '--web.external-url=http://localhost:9093'\n      - '--cluster.listen-address='\n    volumes:\n      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro\n      - alertmanager_data:/alertmanager\n    ports:\n      - \"9093:9093\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9093/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n\n  # ===========================================\n  # Loki - Log Aggregation\n  # ===========================================\n  loki:\n    image: grafana/loki:3.3.2\n    container_name: loki\n    restart: unless-stopped\n    command: -config.file=/etc/loki/loki-config.yml\n    volumes:\n      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro\n      - ./loki/rules:/loki/rules:ro\n      - loki_data:/loki\n    ports:\n      - \"3100:3100\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3100/ready\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Promtail - Log Collector\n  # ===========================================\n  promtail:\n    image: grafana/promtail:3.3.2\n    container_name: promtail\n    restart: unless-stopped\n    command: -config.file=/etc/promtail/promtail-config.yml\n    volumes:\n      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n    ports:\n      - \"9080:9080\"\n    networks:\n      - monitoring-net\n    depends_on:\n      loki:\n        condition: service_healthy\n\n\n  # ===========================================\n  # Grafana - Visualization & Dashboards\n  # ===========================================\n  grafana:\n    image: grafana/grafana:12.3.1\n    container_name: grafana\n    restart: unless-stopped\n    \n    environment:\n      # Admin credentials\n      - GF_SECURITY_ADMIN_USER=admin\n      - GF_SECURITY_ADMIN_PASSWORD=admin123\n      \n      # Server settings\n      - GF_SERVER_ROOT_URL=http://localhost:3000\n      \n      # Disable analytics\n      - GF_ANALYTICS_REPORTING_ENABLED=false\n      - GF_ANALYTICS_CHECK_FOR_UPDATES=false\n      \n      # Feature toggles (Grafana 12 features)\n      - GF_FEATURE_TOGGLES_ENABLE=nestedFolders\n    volumes:\n      # Provisioning - auto setup datasources & dashboards\n      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro\n      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro\n      \n      # Dashboard JSON files - mount trực tiếp từ local 👇\n      - ./grafana/dashboards/overview:/var/lib/grafana/dashboards/overview:ro\n      - ./grafana/dashboards/infrastructure:/var/lib/grafana/dashboards/infrastructure:ro\n      - ./grafana/dashboards/applications:/var/lib/grafana/dashboards/applications:ro\n      - ./grafana/dashboards/logs:/var/lib/grafana/dashboards/logs:ro\n      - ./grafana/dashboards/tracing:/var/lib/grafana/dashboards/tracing:ro\n      # Persistent storage\n      - grafana_data:/var/lib/grafana\n    \n    ports:\n      - \"3000:3000\"\n    \n    networks:\n      - monitoring-net\n    \n    depends_on:\n      prometheus:\n        condition: service_healthy\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3000/api/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  # ===========================================\n  # cAdvisor - Container Metrics\n  # ===========================================\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor:v0.51.0\n    container_name: cadvisor\n    restart: unless-stopped\n    privileged: true\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    ports:\n      - \"8080:8080\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8080/healthz\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n  # Jaeger - Distributed Tracing\n  # ===========================================\n  jaeger:\n    image: jaegertracing/all-in-one:1.54\n    container_name: jaeger\n    restart: unless-stopped\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n      - LOG_LEVEL=info\n    ports:\n      - \"16686:16686\"   # Jaeger UI\n      - \"4317:4317\"     # OTLP gRPC\n      - \"4318:4318\"     # OTLP HTTP\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:16686/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Volumes - Persistent Storage\n# ===========================================\nvolumes:\n  prometheus_data:\n    name: prometheus_data\n  grafana_data:\n    name: grafana_data\n  grafana_dashboards:\n    name: grafana_dashboards\n  alertmanager_data:\n    name: alertmanager_data\n  loki_data:\n    name: loki_data\n# ===========================================\n# Networks - External Network\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true\n````\n\n## File: monitoring/loki.md\n````markdown\n```mermaid\ngraph LR\n    subgraph \"Applications\"\n        P[Producer]\n        C[Consumer]\n    end\n    \n    subgraph \"Log Collection\"\n        PR[Promtail<br/>Log Collector]\n    end\n    \n    subgraph \"Storage & Query\"\n        L[Loki<br/>Log Aggregation]\n        G[Grafana<br/>Visualization]\n    end\n    \n    P -->|stdout/stderr| PR\n    C -->|stdout/stderr| PR\n    PR -->|push logs| L\n    L -->|query| G\n```\n````\n\n## File: monitoring/SLO.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"Definitions\"\n        SLI[SLI - Service Level Indicator<br/>📊 Metric đo lường]\n        SLO[SLO - Service Level Objective<br/>🎯 Mục tiêu cần đạt]\n        SLA[SLA - Service Level Agreement<br/>📝 Cam kết với khách hàng]\n    end\n    \n    SLI --> SLO --> SLA\n    \n    subgraph \"Example\"\n        E1[SLI: 99.5% requests < 500ms]\n        E2[SLO: 99.9% availability]\n        E3[SLA: Hoàn tiền nếu < 99.5%]\n    end\n```\n````\n\n## File: monitoring/stack.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"Observability Stack\"\n        M[Metrics<br/>Prometheus]\n        L[Logs<br/>Loki]\n        A[Alerts<br/>Alertmanager]\n        V[Visualization<br/>Grafana]\n    end\n    \n    subgraph \"Data Pipeline\"\n        P[Producer]\n        K[Kafka]\n        C[Consumer]\n        DB[(PostgreSQL)]\n    end\n    \n    subgraph \"Infrastructure\"\n        CA[cAdvisor]\n        KE[Kafka Exporter]\n        PE[Postgres Exporter]\n    end\n    \n    P --> K --> C --> DB\n    P & C --> M\n    P & C --> L\n    CA & KE & PE --> M\n    M & L --> A\n    M & L & A --> V\n```\n````\n\n## File: networks/docker-compose-network.yml\n````yaml\nnetworks:\n  monitoring-net:\n    name: monitoring-net\n    driver: bridge\n````\n\n## File: monitoring-overview.md\n````markdown\n```mermaid\ngraph LR\n    subgraph \"Data Source\"\n        A[🐍 Python Producer<br/>Fake e-commerce data]\n    end\n    \n    subgraph \"Message Queue\"\n        B[📨 Apache Kafka<br/>+ Zookeeper]\n    end\n    \n    subgraph \"Processing & Storage\"\n        C[🐍 Python Consumer<br/>Transform data]\n        D[(🐘 PostgreSQL<br/>Data Warehouse)]\n    end\n    \n    subgraph \"Monitoring Stack\"\n        E[📊 Prometheus]\n        F[📈 Grafana]\n    end\n    \n    A -->|produce orders| B\n    B -->|consume| C\n    C -->|insert| D\n    \n    A -.->|metrics| E\n    B -.->|metrics| E\n    C -.->|metrics| E\n    D -.->|metrics| E\n    E -->|visualize| F\n```\n````\n\n## File: structure.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"External Network: monitoring-net\"\n        subgraph \"monitoring/\"\n            P[Prometheus]\n            G[Grafana]\n        end\n        \n        subgraph \"infrastructure/\"\n            K[Kafka]\n            Z[Zookeeper]\n            PG[PostgreSQL]\n            \n            KE[Kafka Exporter]\n            PE[Postgres Exporter]\n        end\n        \n        subgraph \"applications/\"\n            PR[Producer]\n            CO[Consumer]\n        end\n    end\n    \n    P -.->|scrape| KE\n    P -.->|scrape| PE\n    P -.->|scrape| PR\n    P -.->|scrape| CO\n    G -->|query| P\n```\n````",
    "repomix-output.xml": "This file is a merged representation of the entire codebase, combined into a single document by Repomix.\n\n<file_summary>\nThis section contains a summary of this file.\n\n<purpose>\nThis file contains a packed representation of the entire repository's contents.\nIt is designed to be easily consumable by AI systems for analysis, code review,\nor other automated processes.\n</purpose>\n\n<file_format>\nThe content is organized as follows:\n1. This summary section\n2. Repository information\n3. Directory structure\n4. Repository files (if enabled)\n5. Multiple file entries, each consisting of:\n  - File path as an attribute\n  - Full contents of the file\n</file_format>\n\n<usage_guidelines>\n- This file should be treated as read-only. Any changes should be made to the\n  original repository files, not this packed version.\n- When processing this file, use the file path to distinguish\n  between different files in the repository.\n- Be aware that this file may contain sensitive information. Handle it with\n  the same level of security as you would the original repository.\n</usage_guidelines>\n\n<notes>\n- Some files may have been excluded based on .gitignore rules and Repomix's configuration\n- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files\n- Files matching patterns in .gitignore are excluded\n- Files matching default ignore patterns are excluded\n- Files are sorted by Git change count (files with more changes are at the bottom)\n</notes>\n\n</file_summary>\n\n<directory_structure>\napplications/\n  consumer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  producer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  applications-flow.md\n  docker-compose.yml\ninfrastructure/\n  postgres/\n    init/\n      01-init.sql\n  docker-compose-application.yml\n  infrastructure.md\nmonitoring/\n  alertmanager/\n    alertmanager.yml\n  grafana/\n    dashboards/\n      applications/\n        data-pipeline.json\n        red-metrics.json\n        slo-dashboard.json\n      infrastructure/\n        container-metrics.json\n        kafka.json\n        postgresql.json\n      logs/\n        logs-explorer.json\n        test.json\n      overview/\n        correlation-dashboard.json\n        system-overview.json\n      tracing/\n        tracing-overview.json\n    provisioning/\n      dashboards/\n        dashboards.yml\n      datasources/\n        datasources.yml\n  loki/\n    rules/\n      fake/\n        alerts.yml\n      alerts.yml\n    loki-config.yml\n  prometheus/\n    rules/\n      alert_rules.yml\n      recording_rules.yml\n      slo_alerts.yml\n      slo_rules.yml\n    prometheus.yml\n  promtail/\n    promtail-config.yml\n  stress-testing/\n    load-test/\n      Dockerfile\n      load_test.py\n      requirements.txt\n    scripts/\n      chaos_test.sh\n      ramp_up_test.sh\n      run_load_test.sh\n      spike_test.sh\n    docker-compose-stress-test.yml\n  docker-compose-monitoring.yml\n  loki.md\n  SLO.md\n  stack.md\nnetworks/\n  docker-compose-network.yml\nmonitoring-overview.md\nrepomix-output.md\nstructure.md\n</directory_structure>\n\n<files>\nThis section contains the contents of the repository's files.\n\n<file path=\"applications/consumer/config.py\">\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    \"\"\"Consumer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    kafka_consumer_group: str = \"order-processor\"\n    \n    # PostgreSQL settings\n    postgres_host: str = \"postgres\"\n    postgres_port: int = 5432\n    postgres_db: str = \"datawarehouse\"\n    postgres_user: str = \"postgres\"\n    postgres_password: str = \"postgres123\"\n    \n    # Consumer settings\n    batch_size: int = 10\n    poll_timeout_ms: int = 1000\n    \n    # Metrics server\n    metrics_port: int = 8001\n    \n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-consumer\"\n    otel_enabled: bool = True\n    \n    # Application\n    app_name: str = \"data-consumer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n    \n    @property\n    def postgres_dsn(self) -> str:\n        return (\n            f\"host={self.postgres_host} \"\n            f\"port={self.postgres_port} \"\n            f\"dbname={self.postgres_db} \"\n            f\"user={self.postgres_user} \"\n            f\"password={self.postgres_password}\"\n        )\n\n\nsettings = Settings()\n</file>\n\n<file path=\"applications/consumer/Dockerfile\">\n# applications/consumer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8001\n\n# Run the consumer\nCMD [\"python\", \"-u\", \"main.py\"]\n</file>\n\n<file path=\"applications/consumer/logger.py\">\n# applications/consumer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()\n</file>\n\n<file path=\"applications/consumer/main.py\">\n# applications/consumer/main.py\n\nimport json\nimport time\nfrom datetime import datetime, timezone\n\nimport psycopg2\nfrom psycopg2.extras import execute_batch\nfrom kafka import KafkaConsumer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode, SpanKind\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_consumer_info',\n    'Consumer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'consumer_group': settings.kafka_consumer_group,\n    'pattern': 'RED'\n})\n\nMESSAGES_CONSUMED = Counter(\n    'pipeline_consumer_messages_total',\n    'Total number of messages consumed',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PROCESSED = Counter(\n    'pipeline_consumer_batches_total',\n    'Total number of batches processed',\n    ['status']\n)\n\nDB_OPERATIONS = Counter(\n    'pipeline_consumer_db_operations_total',\n    'Total database operations',\n    ['operation', 'status']\n)\n\nERRORS = Counter(\n    'pipeline_consumer_errors_total',\n    'Total number of errors by type and stage',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_message_duration_seconds',\n    'Time to process a single message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_batch_duration_seconds',\n    'Time to process a batch of messages',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nDB_QUERY_DURATION = Histogram(\n    'pipeline_consumer_db_query_duration_seconds',\n    'Database query duration',\n    ['operation'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5)\n)\n\nKAFKA_POLL_DURATION = Histogram(\n    'pipeline_consumer_kafka_poll_duration_seconds',\n    'Kafka poll duration',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0)\n)\n\nEND_TO_END_LATENCY = Histogram(\n    'pipeline_consumer_end_to_end_latency_seconds',\n    'End-to-end latency from order creation to database insert',\n    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0)\n)\n\nORDERS_PROCESSED = Counter(\n    'pipeline_business_orders_processed_total',\n    'Total orders processed by category',\n    ['category']\n)\n\nREVENUE_PROCESSED = Counter(\n    'pipeline_business_revenue_processed_total',\n    'Total revenue processed',\n    ['category']\n)\n\nCONSUMER_UP = Gauge(\n    'pipeline_consumer_up',\n    'Consumer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_kafka_connected',\n    'Kafka connection status'\n)\n\nDB_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_db_connected',\n    'Database connection status'\n)\n\nCONSUMER_LAG = Gauge(\n    'pipeline_consumer_lag_messages',\n    'Consumer lag per partition',\n    ['topic', 'partition']\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_consumer_current_batch_size',\n    'Number of messages in current batch'\n)\n\nLAST_PROCESS_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_process_timestamp',\n    'Timestamp of last successful process'\n)\n\nLAST_COMMIT_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_commit_timestamp',\n    'Timestamp of last offset commit'\n)\n\n\n# ===========================================\n# Database Handler\n# ===========================================\n\nclass DatabaseHandler:\n    def __init__(self):\n        self.conn = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to PostgreSQL with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.conn = psycopg2.connect(settings.postgres_dsn)\n                self.conn.autocommit = False\n                logger.info(\n                    \"Connected to PostgreSQL\",\n                    extra={\n                        \"event\": \"db_connected\",\n                        \"host\": settings.postgres_host,\n                        \"database\": settings.postgres_db,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                DB_CONNECTION_STATUS.set(1)\n                return\n            except psycopg2.Error as e:\n                DB_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"db_init\").inc()\n                logger.warning(\n                    \"PostgreSQL connection failed, retrying...\",\n                    extra={\n                        \"event\": \"db_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to PostgreSQL after maximum retries\")\n    \n    def insert_orders(self, orders: list[dict], parent_span=None) -> int:\n        \"\"\"Insert batch of orders into database with tracing\"\"\"\n        if not orders:\n            return 0\n        \n        with tracer.start_as_current_span(\n            \"db_insert_orders\",\n            kind=SpanKind.CLIENT\n        ) as span:\n            span.set_attribute(\"db.system\", \"postgresql\")\n            span.set_attribute(\"db.name\", settings.postgres_db)\n            span.set_attribute(\"db.operation\", \"INSERT\")\n            span.set_attribute(\"db.batch_size\", len(orders))\n            \n            insert_sql = \"\"\"\n                INSERT INTO ecommerce.orders (\n                    order_id, customer_id, product_id, product_name, \n                    category, quantity, unit_price, total_amount,\n                    order_status, created_at, processed_at\n                ) VALUES (\n                    %(order_id)s, %(customer_id)s, %(product_id)s, %(product_name)s,\n                    %(category)s, %(quantity)s, %(unit_price)s, %(total_amount)s,\n                    %(order_status)s, %(created_at)s, %(processed_at)s\n                )\n                ON CONFLICT (order_id) DO NOTHING\n            \"\"\"\n            \n            try:\n                start_time = time.time()\n                \n                with self.conn.cursor() as cur:\n                    process_time = datetime.now(timezone.utc)\n                    \n                    for order in orders:\n                        order['processed_at'] = process_time.isoformat()\n                        if isinstance(order.get('created_at'), str):\n                            order['created_at'] = datetime.fromisoformat(\n                                order['created_at'].replace('Z', '+00:00')\n                            )\n                    \n                    execute_batch(cur, insert_sql, orders, page_size=100)\n                    inserted = cur.rowcount\n                    self.conn.commit()\n                \n                duration = time.time() - start_time\n                \n                span.set_attribute(\"db.rows_affected\", inserted)\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                span.set_status(Status(StatusCode.OK))\n                \n                DB_QUERY_DURATION.labels(operation=\"insert_batch\").observe(duration)\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"success\").inc(len(orders))\n                \n                logger.debug(\n                    \"Orders inserted into database\",\n                    extra={\n                        \"event\": \"db_insert_success\",\n                        \"count\": inserted,\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return inserted\n                \n            except psycopg2.Error as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                ERRORS.labels(error_type=type(e).__name__, stage=\"db_insert\").inc()\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"failed\").inc(len(orders))\n                \n                logger.error(\n                    \"Database insert failed\",\n                    extra={\n                        \"event\": \"db_insert_error\",\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__,\n                        \"batch_size\": len(orders)\n                    }\n                )\n                self.conn.rollback()\n                \n                try:\n                    self._connect()\n                except Exception:\n                    DB_CONNECTION_STATUS.set(0)\n                \n                return 0\n    \n    def close(self):\n        \"\"\"Close database connection\"\"\"\n        if self.conn:\n            self.conn.close()\n            DB_CONNECTION_STATUS.set(0)\n            logger.info(\"Database connection closed\", extra={\"event\": \"db_closed\"})\n\n\n# ===========================================\n# Kafka Consumer\n# ===========================================\n\nclass OrderConsumer:\n    def __init__(self, db_handler: DatabaseHandler):\n        self.consumer = None\n        self.db = db_handler\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.consumer = KafkaConsumer(\n                    settings.kafka_topic,\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    group_id=settings.kafka_consumer_group,\n                    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n                    auto_offset_reset='earliest',\n                    enable_auto_commit=False,\n                    max_poll_records=settings.batch_size,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"topic\": settings.kafka_topic,\n                        \"consumer_group\": settings.kafka_consumer_group,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                CONSUMER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"kafka_init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def process_messages(self):\n        \"\"\"Main processing loop with tracing\"\"\"\n        batch: list[dict] = []\n        \n        logger.info(\"Starting message processing loop\", extra={\"event\": \"processing_started\"})\n        \n        try:\n            while True:\n                # Poll for messages\n                with tracer.start_as_current_span(\n                    \"kafka_poll\",\n                    kind=SpanKind.CONSUMER\n                ) as poll_span:\n                    poll_start = time.time()\n                    records = self.consumer.poll(\n                        timeout_ms=settings.poll_timeout_ms,\n                        max_records=settings.batch_size\n                    )\n                    poll_duration = time.time() - poll_start\n                    \n                    poll_span.set_attribute(\"messaging.system\", \"kafka\")\n                    poll_span.set_attribute(\"messaging.operation\", \"poll\")\n                    poll_span.set_attribute(\"duration_ms\", round(poll_duration * 1000, 2))\n                    \n                    KAFKA_POLL_DURATION.observe(poll_duration)\n                \n                if not records:\n                    if batch:\n                        self._process_batch(batch)\n                        batch = []\n                    continue\n                \n                # Process received messages\n                for topic_partition, messages in records.items():\n                    for message in messages:\n                        try:\n                            msg_start = time.time()\n                            order = message.value\n                            category = order.get(\"category\", \"unknown\")\n                            \n                            # Create span for consuming\n                            with tracer.start_as_current_span(\n                                \"consume_message\",\n                                kind=SpanKind.CONSUMER\n                            ) as msg_span:\n                                # Add span attributes\n                                msg_span.set_attribute(\"messaging.system\", \"kafka\")\n                                msg_span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n                                msg_span.set_attribute(\"messaging.kafka.partition\", topic_partition.partition)\n                                msg_span.set_attribute(\"messaging.kafka.offset\", message.offset)\n                                msg_span.set_attribute(\"order.id\", order.get(\"order_id\", \"\"))\n                                msg_span.set_attribute(\"order.category\", category)\n                                \n                                # Link to producer trace if available\n                                if \"trace_id\" in order:\n                                    msg_span.set_attribute(\"producer.trace_id\", order[\"trace_id\"])\n                                \n                                batch.append(order)\n                                \n                                MESSAGES_CONSUMED.labels(\n                                    topic=settings.kafka_topic,\n                                    status=\"success\",\n                                    category=category\n                                ).inc()\n                                \n                                msg_duration = time.time() - msg_start\n                                MESSAGE_PROCESS_DURATION.observe(msg_duration)\n                                \n                                # Calculate end-to-end latency\n                                if 'created_at' in order:\n                                    try:\n                                        created_at = datetime.fromisoformat(\n                                            order['created_at'].replace('Z', '+00:00')\n                                        )\n                                        e2e_latency = (datetime.now(timezone.utc) - created_at).total_seconds()\n                                        END_TO_END_LATENCY.observe(e2e_latency)\n                                        msg_span.set_attribute(\"e2e_latency_seconds\", e2e_latency)\n                                    except Exception:\n                                        pass\n                                \n                                CONSUMER_LAG.labels(\n                                    topic=topic_partition.topic,\n                                    partition=str(topic_partition.partition)\n                                ).set(message.offset)\n                                \n                                msg_span.set_status(Status(StatusCode.OK))\n                                \n                                logger.info(\n                                    \"Message consumed\",\n                                    extra={\n                                        \"event\": \"message_consumed\",\n                                        \"order_id\": order.get(\"order_id\"),\n                                        \"trace_id\": order.get(\"trace_id\", \"\"),\n                                        \"category\": category,\n                                        \"partition\": topic_partition.partition,\n                                        \"offset\": message.offset\n                                    }\n                                )\n                                \n                        except Exception as e:\n                            MESSAGES_CONSUMED.labels(\n                                topic=settings.kafka_topic,\n                                status=\"failed\",\n                                category=\"unknown\"\n                            ).inc()\n                            \n                            ERRORS.labels(\n                                error_type=type(e).__name__,\n                                stage=\"message_parse\"\n                            ).inc()\n                            \n                            logger.error(\n                                \"Failed to process message\",\n                                extra={\n                                    \"event\": \"message_parse_error\",\n                                    \"error\": str(e),\n                                    \"error_type\": type(e).__name__,\n                                    \"partition\": topic_partition.partition,\n                                    \"offset\": message.offset\n                                }\n                            )\n\n                \n                CURRENT_BATCH_SIZE.set(len(batch))\n                \n                if len(batch) >= settings.batch_size:\n                    self._process_batch(batch)\n                    batch = []\n                    \n        except KeyboardInterrupt:\n            logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n            if batch:\n                self._process_batch(batch)\n    \n    def _process_batch(self, batch: list[dict]):\n        \"\"\"Process and commit a batch of messages with tracing\"\"\"\n        if not batch:\n            return\n        \n        with tracer.start_as_current_span(\n            \"process_batch\",\n            kind=SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", len(batch))\n            \n            start_time = time.time()\n            \n            # Insert to database\n            inserted = self.db.insert_orders(batch)\n            \n            # Commit Kafka offsets\n            self.consumer.commit()\n            LAST_COMMIT_TIMESTAMP.set(time.time())\n            \n            duration = time.time() - start_time\n            \n            batch_span.set_attribute(\"batch.inserted\", inserted)\n            batch_span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n            \n            BATCH_PROCESS_DURATION.observe(duration)\n            \n            if inserted == len(batch):\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"success\").inc()\n            elif inserted == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"No records inserted\"))\n                BATCHES_PROCESSED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"partial\").inc()\n            \n            # Business metrics\n            for order in batch:\n                category = order.get(\"category\", \"unknown\")\n                ORDERS_PROCESSED.labels(category=category).inc()\n                REVENUE_PROCESSED.labels(category=category).inc(order.get(\"total_amount\", 0))\n            \n            LAST_PROCESS_TIMESTAMP.set(time.time())\n            CURRENT_BATCH_SIZE.set(0)\n            \n            logger.info(\n                \"Batch processed\",\n                extra={\n                    \"event\": \"batch_processed\",\n                    \"batch_size\": len(batch),\n                    \"inserted\": inserted,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(len(batch) / duration, 2) if duration > 0 else 0\n                }\n            )\n    \n    def close(self):\n        \"\"\"Close the consumer\"\"\"\n        if self.consumer:\n            self.consumer.close()\n            CONSUMER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Consumer closed\", extra={\"event\": \"consumer_closed\"})\n\n\n# ===========================================\n# Main\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Consumer\",\n        extra={\n            \"event\": \"consumer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"consumer_group\": settings.kafka_consumer_group,\n                \"postgres_host\": settings.postgres_host,\n                \"postgres_db\": settings.postgres_db,\n                \"batch_size\": settings.batch_size,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create handlers\n    db_handler = DatabaseHandler()\n    consumer = OrderConsumer(db_handler)\n    \n    try:\n        consumer.process_messages()\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in consumer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        consumer.close()\n        db_handler.close()\n        logger.info(\"Consumer stopped\", extra={\"event\": \"consumer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n\n<file path=\"applications/consumer/requirements.txt\">\n# Kafka\nkafka-python-ng==2.2.2\n\n# PostgreSQL\npsycopg2-binary==2.9.9\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0\n</file>\n\n<file path=\"applications/consumer/tracing.py\">\n# applications/consumer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()\n</file>\n\n<file path=\"applications/producer/config.py\">\n# applications/producer/config.py\n\nfrom pydantic_settings import BaseSettings\nfrom typing import Optional\n\n\nclass Settings(BaseSettings):\n    \"\"\"Producer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    \n    # Producer settings\n    produce_interval_seconds: float = 1.0  # Produce every N seconds\n    batch_size: int = 5  # Messages per batch\n    \n    # Metrics server\n    metrics_port: int = 8000\n\n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-producer\"\n    otel_enabled: bool = True\n\n    # Application\n    app_name: str = \"data-producer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n\n\nsettings = Settings()\n</file>\n\n<file path=\"applications/producer/Dockerfile\">\n# applications/producer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8000\n\n# Run the producer\nCMD [\"python\", \"-u\", \"main.py\"]\n</file>\n\n<file path=\"applications/producer/logger.py\">\n# applications/producer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()\n</file>\n\n<file path=\"applications/producer/main.py\">\n# applications/producer/main.py\n\nimport json\nimport random\nimport time\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_producer_info',\n    'Producer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'pattern': 'RED'\n})\n\nMESSAGES_PRODUCED = Counter(\n    'pipeline_producer_messages_total',\n    'Total number of messages produced',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PRODUCED = Counter(\n    'pipeline_producer_batches_total',\n    'Total number of batches produced',\n    ['status']\n)\n\nERRORS = Counter(\n    'pipeline_producer_errors_total',\n    'Total number of errors by type',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_message_duration_seconds',\n    'Time to produce a single message to Kafka',\n    ['topic'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_batch_duration_seconds',\n    'Time to produce a batch of messages',\n    ['topic'],\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nMESSAGE_SIZE = Histogram(\n    'pipeline_producer_message_size_bytes',\n    'Size of produced messages in bytes',\n    ['topic'],\n    buckets=(100, 500, 1000, 2500, 5000, 10000, 25000)\n)\n\nORDERS_BY_CATEGORY = Counter(\n    'pipeline_business_orders_total',\n    'Total orders by category',\n    ['category']\n)\n\nREVENUE = Counter(\n    'pipeline_business_revenue_total',\n    'Total revenue generated',\n    ['category']\n)\n\nORDER_VALUE = Histogram(\n    'pipeline_business_order_value',\n    'Distribution of order values',\n    ['category'],\n    buckets=(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\n)\n\nITEMS_PER_ORDER = Histogram(\n    'pipeline_business_items_per_order',\n    'Distribution of items per order',\n    buckets=(1, 2, 3, 4, 5, 10)\n)\n\nPRODUCER_UP = Gauge(\n    'pipeline_producer_up',\n    'Producer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_producer_kafka_connected',\n    'Kafka connection status (1=connected, 0=disconnected)'\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_producer_current_batch_size',\n    'Current configured batch size'\n)\n\nLAST_PRODUCE_TIMESTAMP = Gauge(\n    'pipeline_producer_last_produce_timestamp',\n    'Timestamp of last successful produce'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\n\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order(trace_id: str = None) -> dict[str, Any]:\n    \"\"\"Generate a fake e-commerce order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    order = {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n    }\n    \n    # Add trace_id for correlation\n    if trace_id:\n        order[\"trace_id\"] = trace_id\n    \n    return order\n\n\n# ===========================================\n# Kafka Producer\n# ===========================================\n\nclass OrderProducer:\n    def __init__(self):\n        self.producer = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks='all',\n                    retries=3,\n                    max_in_flight_requests_per_connection=1,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                PRODUCER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def produce(self, order: dict) -> bool:\n        \"\"\"Produce a single order to Kafka with tracing\"\"\"\n        category = order.get(\"category\", \"unknown\")\n        \n        # Create span for this operation\n        with tracer.start_as_current_span(\n            \"produce_order\",\n            kind=trace.SpanKind.PRODUCER\n        ) as span:\n            # Add span attributes\n            span.set_attribute(\"messaging.system\", \"kafka\")\n            span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n            span.set_attribute(\"order.id\", order[\"order_id\"])\n            span.set_attribute(\"order.category\", category)\n            span.set_attribute(\"order.amount\", order[\"total_amount\"])\n            \n            # Get trace_id and add to order for correlation\n            trace_id = format(span.get_span_context().trace_id, '032x')\n            order[\"trace_id\"] = trace_id\n            \n            message_bytes = json.dumps(order).encode('utf-8')\n            span.set_attribute(\"messaging.message.payload_size_bytes\", len(message_bytes))\n            \n            try:\n                start_time = time.time()\n                \n                future = self.producer.send(\n                    settings.kafka_topic,\n                    key=order[\"order_id\"],\n                    value=order\n                )\n                future.get(timeout=10)\n                \n                duration = time.time() - start_time\n                \n                # Record success in span\n                span.set_status(Status(StatusCode.OK))\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                \n                # Metrics\n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"success\",\n                    category=category\n                ).inc()\n                \n                MESSAGE_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n                MESSAGE_SIZE.labels(topic=settings.kafka_topic).observe(len(message_bytes))\n                \n                ORDERS_BY_CATEGORY.labels(category=category).inc()\n                REVENUE.labels(category=category).inc(order[\"total_amount\"])\n                ORDER_VALUE.labels(category=category).observe(order[\"total_amount\"])\n                ITEMS_PER_ORDER.observe(order[\"quantity\"])\n                \n                LAST_PRODUCE_TIMESTAMP.set(time.time())\n                \n                logger.debug(\n                    \"Order produced successfully\",\n                    extra={\n                        \"event\": \"order_produced\",\n                        \"order_id\": order[\"order_id\"],\n                        \"trace_id\": trace_id,\n                        \"category\": category,\n                        \"amount\": order[\"total_amount\"],\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return True\n                \n            except Exception as e:\n                # Record error in span\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"failed\",\n                    category=category\n                ).inc()\n                \n                ERRORS.labels(\n                    error_type=type(e).__name__,\n                    stage=\"produce\"\n                ).inc()\n                \n                logger.error(\n                    \"Failed to produce order\",\n                    extra={\n                        \"event\": \"produce_error\",\n                        \"order_id\": order.get(\"order_id\"),\n                        \"trace_id\": trace_id,\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__\n                    }\n                )\n                return False\n    \n    def produce_batch(self, batch_size: int) -> tuple[int, int]:\n        \"\"\"Produce a batch of orders with tracing\"\"\"\n        \n        # Create parent span for batch\n        with tracer.start_as_current_span(\n            \"produce_batch\",\n            kind=trace.SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", batch_size)\n            \n            success_count = 0\n            failed_count = 0\n            batch_orders = []\n            \n            CURRENT_BATCH_SIZE.set(batch_size)\n            \n            start_time = time.time()\n            \n            for _ in range(batch_size):\n                order = generate_order()\n                if self.produce(order):\n                    success_count += 1\n                    batch_orders.append(order[\"order_id\"])\n                else:\n                    failed_count += 1\n            \n            self.producer.flush()\n            \n            duration = time.time() - start_time\n            \n            # Record batch results in span\n            batch_span.set_attribute(\"batch.success_count\", success_count)\n            batch_span.set_attribute(\"batch.failed_count\", failed_count)\n            batch_span.set_attribute(\"batch.duration_ms\", round(duration * 1000, 2))\n            \n            if failed_count == 0:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"success\").inc()\n            elif success_count == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"All messages failed\"))\n                BATCHES_PRODUCED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"partial\").inc()\n            \n            BATCH_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n            \n            logger.info(\n                \"Batch produced\",\n                extra={\n                    \"event\": \"batch_produced\",\n                    \"batch_size\": batch_size,\n                    \"success_count\": success_count,\n                    \"failed_count\": failed_count,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(batch_size / duration, 2) if duration > 0 else 0\n                }\n            )\n            \n            return success_count, failed_count\n    \n    def close(self):\n        \"\"\"Close the producer\"\"\"\n        if self.producer:\n            self.producer.close()\n            PRODUCER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Producer closed\", extra={\"event\": \"producer_closed\"})\n\n\n# ===========================================\n# Main Loop\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Producer\",\n        extra={\n            \"event\": \"producer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"batch_size\": settings.batch_size,\n                \"interval_seconds\": settings.produce_interval_seconds,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create producer\n    producer = OrderProducer()\n    \n    try:\n        batch_number = 0\n        while True:\n            batch_number += 1\n            \n            # Create span for each iteration\n            with tracer.start_as_current_span(f\"batch_iteration_{batch_number}\"):\n                success, failed = producer.produce_batch(settings.batch_size)\n            \n            time.sleep(settings.produce_interval_seconds)\n            \n    except KeyboardInterrupt:\n        logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in producer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        producer.close()\n        logger.info(\"Producer stopped\", extra={\"event\": \"producer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()\n</file>\n\n<file path=\"applications/producer/requirements.txt\">\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data generation\nfaker==24.4.0\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0\n</file>\n\n<file path=\"applications/producer/tracing.py\">\n# applications/producer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()\n</file>\n\n<file path=\"applications/applications-flow.md\">\n```mermaid\ngraph LR\n    subgraph \"applications/docker-compose.yml\"\n        subgraph \"Producer\"\n            P[Data Producer<br/>:8000/metrics]\n            F[Faker Library]\n        end\n        \n        subgraph \"Consumer\"\n            C[Data Consumer<br/>:8001/metrics]\n        end\n    end\n    \n    subgraph \"infrastructure/\"\n        K[Kafka]\n        PG[(PostgreSQL)]\n    end\n    \n    subgraph \"monitoring/\"\n        PR[Prometheus]\n    end\n    \n    F --> P\n    P -->|produce orders| K\n    K -->|consume| C\n    C -->|insert| PG\n    \n    P -.->|metrics| PR\n    C -.->|metrics| PR\n```\n</file>\n\n<file path=\"applications/docker-compose.yml\">\n# applications/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Data Producer - Generate fake orders\n  # ===========================================\n  data-producer:\n    build:\n      context: ./producer\n      dockerfile: Dockerfile\n    container_name: data-producer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      \n      # Producer settings\n      PRODUCE_INTERVAL_SECONDS: \"1.0\"\n      BATCH_SIZE: \"5\"\n      \n      # Metrics\n      METRICS_PORT: \"8000\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8000:8000\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8000/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Data Consumer - Process orders to PostgreSQL\n  # ===========================================\n  data-consumer:\n    build:\n      context: ./consumer\n      dockerfile: Dockerfile\n    container_name: data-consumer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      KAFKA_CONSUMER_GROUP: order-processor\n      \n      # PostgreSQL settings\n      POSTGRES_HOST: postgres\n      POSTGRES_PORT: \"5432\"\n      POSTGRES_DB: datawarehouse\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres123\n      \n      # Consumer settings\n      BATCH_SIZE: \"10\"\n      POLL_TIMEOUT_MS: \"1000\"\n      \n      # Metrics\n      METRICS_PORT: \"8001\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8001:8001\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    #   postgres:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8001/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Networks\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true\n</file>\n\n<file path=\"infrastructure/postgres/init/01-init.sql\">\n-- infrastructure/postgres/init/01-init.sql\n\n-- ===========================================\n-- Database cho Data Pipeline\n-- ===========================================\n\n-- Tạo schema cho e-commerce data\nCREATE SCHEMA IF NOT EXISTS ecommerce;\n\n-- Bảng orders - nơi lưu data từ Kafka consumer\nCREATE TABLE ecommerce.orders (\n    id SERIAL PRIMARY KEY,\n    order_id VARCHAR(50) UNIQUE NOT NULL,\n    customer_id VARCHAR(50) NOT NULL,\n    product_id VARCHAR(50) NOT NULL,\n    product_name VARCHAR(255) NOT NULL,\n    category VARCHAR(100),\n    quantity INTEGER NOT NULL,\n    unit_price DECIMAL(10, 2) NOT NULL,\n    total_amount DECIMAL(10, 2) NOT NULL,\n    order_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    processed_at TIMESTAMP,\n    \n    -- Indexes cho queries thường dùng\n    CONSTRAINT chk_quantity CHECK (quantity > 0),\n    CONSTRAINT chk_price CHECK (unit_price > 0)\n);\n\nCREATE INDEX idx_orders_customer ON ecommerce.orders(customer_id);\nCREATE INDEX idx_orders_created_at ON ecommerce.orders(created_at);\nCREATE INDEX idx_orders_status ON ecommerce.orders(order_status);\nCREATE INDEX idx_orders_category ON ecommerce.orders(category);\n\n-- Bảng để track processing metrics\nCREATE TABLE ecommerce.processing_logs (\n    id SERIAL PRIMARY KEY,\n    batch_id VARCHAR(50) NOT NULL,\n    records_processed INTEGER NOT NULL,\n    records_failed INTEGER DEFAULT 0,\n    processing_time_ms INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- View cho monitoring\nCREATE VIEW ecommerce.orders_summary AS\nSELECT \n    DATE(created_at) as order_date,\n    COUNT(*) as total_orders,\n    SUM(total_amount) as total_revenue,\n    AVG(total_amount) as avg_order_value,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM ecommerce.orders\nGROUP BY DATE(created_at)\nORDER BY order_date DESC;\n\n-- Grant permissions\nGRANT ALL PRIVILEGES ON SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA ecommerce TO postgres;\n\n-- Log initialization\nDO $$\nBEGIN\n    RAISE NOTICE 'Database initialization completed successfully!';\nEND $$;\n</file>\n\n<file path=\"infrastructure/infrastructure.md\">\n```mermaid\ngraph TB\n    subgraph \"infrastructure/docker-compose.yml\"\n        subgraph \"Message Queue\"\n            ZK[Zookeeper:2181]\n            K[Kafka:9092]\n            KE[Kafka Exporter:9308]\n        end\n        \n        subgraph \"Database\"\n            PG[(PostgreSQL:5432)]\n            PE[Postgres Exporter:9187]\n        end\n        \n        ZK --> K\n        K -.->|metrics| KE\n        PG -.->|metrics| PE\n    end\n    \n    subgraph \"monitoring/\"\n        P[Prometheus]\n    end\n    \n    KE -->|scrape :9308| P\n    PE -->|scrape :9187| P\n```\n</file>\n\n<file path=\"monitoring/alertmanager/alertmanager.yml\">\n# monitoring/alertmanager/alertmanager.yml\n\nglobal:\n  # Thời gian chờ trước khi gửi lại alert nếu vẫn còn firing\n  resolve_timeout: 5m\n\n# Route tree - định tuyến alerts\nroute:\n  # Default receiver\n  receiver: 'default-receiver'\n  \n  # Group alerts by these labels\n  group_by: ['alertname', 'severity', 'service']\n  \n  # Thời gian chờ trước khi gửi group đầu tiên\n  group_wait: 30s\n  \n  # Thời gian chờ trước khi gửi alerts mới trong cùng group\n  group_interval: 5m\n  \n  # Thời gian chờ trước khi gửi lại alert đã gửi\n  repeat_interval: 4h\n\n  # Child routes - routing dựa trên labels\n  routes:\n    # Critical alerts - gửi ngay\n    - match:\n        severity: critical\n      receiver: 'critical-receiver'\n      group_wait: 10s\n      repeat_interval: 1h\n\n    # Warning alerts\n    - match:\n        severity: warning\n      receiver: 'warning-receiver'\n      repeat_interval: 4h\n\n    # Info alerts - ít urgent hơn\n    - match:\n        severity: info\n      receiver: 'default-receiver'\n      repeat_interval: 12h\n\n# Inhibition rules - suppress alerts\ninhibit_rules:\n  # Nếu critical đang fire, suppress warning cùng service\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'service']\n\n# Receivers - nơi nhận alerts\nreceivers:\n  - name: 'default-receiver'\n    # Webhook để test (có thể xem trong Alertmanager UI)\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n\n  - name: 'critical-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n    # Có thể thêm Slack, Email, PagerDuty ở đây\n    # slack_configs:\n    #   - api_url: 'https://hooks.slack.com/services/xxx/yyy/zzz'\n    #     channel: '#alerts-critical'\n    #     send_resolved: true\n\n  - name: 'warning-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n</file>\n\n<file path=\"monitoring/grafana/dashboards/applications/data-pipeline.json\">\n{\n  \"uid\": \"data-pipeline-dashboard\",\n  \"title\": \"Data Pipeline Overview\",\n  \"description\": \"Monitoring data pipeline: Producer → Kafka → Consumer → PostgreSQL\",\n  \"tags\": [\"applications\", \"data-pipeline\", \"producer\", \"consumer\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🚀 Data Pipeline Flow\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## Pipeline: **Producer** → **Kafka** → **Consumer** → **PostgreSQL**\\n\\n*Metrics sẽ hiển thị khi Producer và Consumer được khởi động.*\"\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-producer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 6, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-consumer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Messages Produced (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_produced_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Messages Consumed (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_consumed_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Production Rate (msg/s)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_messages_produced_total[1m])\",\n          \"legendFormat\": \"Produced\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_messages_consumed_total[1m])\",\n          \"legendFormat\": \"Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Produced\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Processing Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Errors\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-producer\\\"}[1m])\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-consumer\\\"}[1m])\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"errors/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 0.1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Database Inserts (from Consumer)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_db_inserts_total[1m])\",\n          \"legendFormat\": \"Inserts/s\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"single\", \"sort\": \"none\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"gradientMode\": \"hue\", \"lineWidth\": 1 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Orders by Category (Real-time)\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_orders_by_category)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Revenue Generated\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_total_revenue\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"noValue\": \"$0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Pipeline Health Score\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(1 - (rate(pipeline_errors_total[5m]) / (rate(pipeline_messages_produced_total[5m]) + 0.001))) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"noValue\": \"N/A\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 90 }, { \"color\": \"green\", \"value\": 99 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/applications/red-metrics.json\">\n{\n  \"uid\": \"red-metrics-dashboard\",\n  \"title\": \"RED Metrics - Data Pipeline\",\n  \"description\": \"Rate, Errors, Duration metrics for the data pipeline\",\n  \"tags\": [\"red\", \"applications\", \"sre\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 2, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 📊 RED Metrics Overview\\n**R**ate (throughput) | **E**rrors (failures) | **D**uration (latency)   The golden signals for service health\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"🚀 PRODUCER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 2 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Producer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Success\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Failed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Success\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Failed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Producer Latency Percentiles\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 8 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Producer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 10 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_producer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📥 CONSUMER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 17 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency (P95)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 30 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"DB Inserts/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Inserted to DB\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Inserted to DB\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"purple\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 17,\n      \"title\": \"End-to-End Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 18,\n      \"title\": \"Consumer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 25 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_consumer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"💰 BUSINESS METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 35 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Orders/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_orders_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Revenue/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 23,\n      \"title\": \"Avg Order Value\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[5m])) / (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 24,\n      \"title\": \"Orders by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_orders_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"short\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 25,\n      \"title\": \"Revenue by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 18, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_revenue_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"currencyUSD\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 26,\n      \"title\": \"Revenue Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 41 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"legendFormat\": \"{{category}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"lineWidth\": 2, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 27,\n      \"title\": \"Order Value Distribution\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 44 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"Median\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.90, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p90\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p99 (High Value)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/applications/slo-dashboard.json\">\n{\n  \"uid\": \"slo-dashboard\",\n  \"title\": \"SLI/SLO Dashboard\",\n  \"description\": \"Service Level Indicators and Objectives monitoring\",\n  \"tags\": [\"slo\", \"sre\", \"reliability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-24h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 🎯 SLI/SLO Dashboard\\n\\n| Service | SLO Target | Error Budget (30d) |\\n|---------|------------|--------------------|\\n| Producer Success Rate | 99.9% | 43.2 min |\\n| Consumer Success Rate | 99.9% | 43.2 min |\\n| Latency P95 | < 100ms (Producer), < 5s (E2E) | - |\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"📊 ERROR BUDGET STATUS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 3 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 6, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Producer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Consumer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 15, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Budget Burn Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 18, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:consumed_percent\",\n          \"legendFormat\": \"Producer Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"error_budget:consumer:consumed_percent\",\n          \"legendFormat\": \"Consumer Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📈 SLI METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 10 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Consumer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"Producer Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"Database Availability\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.95 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Lag\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 500 },\n              { \"color\": \"red\", \"value\": 1000 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"SLI Trends Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"legendFormat\": \"Producer Success Rate\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"legendFormat\": \"Consumer Success Rate\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"99.9\",\n          \"legendFormat\": \"SLO Target (99.9%)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"min\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 99,\n          \"max\": 100\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"SLO Target (99.9%)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Burn Rate Trend\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"legendFormat\": \"Producer (1h)\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"legendFormat\": \"Consumer (1h)\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"1\",\n          \"legendFormat\": \"Normal Rate (1x)\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"14.4\",\n          \"legendFormat\": \"Critical (14.4x)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"x\",\n          \"min\": 0\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Normal Rate (1x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Critical (14.4x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📋 SLO SUMMARY TABLE\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 23 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"SLO Status\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 24 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"E\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true },\n            \"renameByName\": { \n              \"Value #A\": \"Producer Success %\",\n              \"Value #B\": \"Consumer Success %\",\n              \"Value #C\": \"Database Availability %\",\n              \"Value #D\": \"Producer Budget %\",\n              \"Value #E\": \"Consumer Budget %\"\n            }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"md\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"center\", \"displayMode\": \"color-background-solid\" },\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/infrastructure/container-metrics.json\">\n{\n  \"uid\": \"container-metrics\",\n  \"title\": \"Container Metrics\",\n  \"description\": \"Docker container resource usage monitoring\",\n  \"tags\": [\"infrastructure\", \"containers\", \"cadvisor\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"container\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(container_cpu_usage_seconds_total{name=~\\\".+\\\"}, name)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Running Containers\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(container_last_seen{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total CPU Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m])) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Memory Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(container_memory_usage_bytes{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 4294967296 }, { \"color\": \"red\", \"value\": 8589934592 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Network In\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_receive_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Network Out\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_transmit_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"CPU Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\"$container\\\"}[5m]) * 100\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Memory Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\"$container\\\"}\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Network I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_network_receive_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Receive\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_network_transmit_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Transmit\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Disk I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_fs_reads_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Read\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_fs_writes_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Write\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Container Resource Table\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m]) * 100\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"container_spec_memory_limit_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true, \"__name__\": true, \"id\": true, \"image\": true, \"instance\": true, \"job\": true },\n            \"renameByName\": { \"name\": \"Container\", \"Value #A\": \"CPU %\", \"Value #B\": \"Memory Used\", \"Value #C\": \"Memory Limit\" }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\",\n        \"footer\": { \"show\": true, \"reducer\": [\"sum\"], \"countRows\": false }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"CPU %\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"percent\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"max\", \"value\": 100 },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 80 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Used\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"bytes\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Limit\" }, \n            \"properties\": [{ \"id\": \"unit\", \"value\": \"bytes\" }] \n          }\n        ]\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/infrastructure/kafka.json\">\n{\n  \"uid\": \"kafka-dashboard\",\n  \"title\": \"Kafka Overview\",\n  \"description\": \"Monitoring Apache Kafka cluster health and performance\",\n  \"tags\": [\"infrastructure\", \"kafka\", \"messaging\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"topic\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(kafka_topic_partitions, topic)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Kafka Brokers Up\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_brokers)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total Topics\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Under Replicated Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_under_replicated_partition)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Offline Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_leader{leader=\\\"-1\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Consumer Groups\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(count by (consumergroup) (kafka_consumergroup_current_offset))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Messages In per Second (by Topic)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (topic) (rate(kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}[1m]))\",\n          \"legendFormat\": \"{{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Consumer Group Lag\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (consumergroup, topic) (kafka_consumergroup_lag{topic=~\\\"$topic\\\"})\",\n          \"legendFormat\": \"{{consumergroup}} - {{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1000 }, { \"color\": \"red\", \"value\": 10000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Topic Partitions\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partitions{topic=~\\\"$topic\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"organize\", \"options\": { \"excludeByName\": { \"Time\": true, \"__name__\": true, \"instance\": true, \"job\": true }, \"renameByName\": { \"topic\": \"Topic\", \"Value\": \"Partitions\" } } }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Partitions\" }, \"properties\": [{ \"id\": \"custom.displayMode\", \"value\": \"color-background\" }, { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Current Offset by Partition\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-{{partition}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"stepAfter\", \"fillOpacity\": 0, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Consumer Group Lag Sum\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 8, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"min\": 0,\n          \"max\": 100000,\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5000 }, { \"color\": \"red\", \"value\": 50000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Messages per Partition\",\n      \"type\": \"bargauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 16, \"x\": 8, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-p{{partition}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"orientation\": \"horizontal\",\n        \"displayMode\": \"gradient\",\n        \"showUnfilled\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/infrastructure/postgresql.json\">\n{\n  \"uid\": \"postgresql-dashboard\",\n  \"title\": \"PostgreSQL Overview\",\n  \"description\": \"Monitoring PostgreSQL database performance and health\",\n  \"tags\": [\"infrastructure\", \"postgresql\", \"database\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"PostgreSQL Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_up\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Database Size\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_database_size_bytes{datname=\\\"datawarehouse\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1073741824 }, { \"color\": \"red\", \"value\": 5368709120 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Active Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(pg_stat_activity_count{datname=\\\"datawarehouse\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 100 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Max Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_settings_max_connections\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - pg_postmaster_start_time_seconds\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Connections by State\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_activity_count{datname=\\\"datawarehouse\\\"}\",\n          \"legendFormat\": \"{{state}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\", \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Transactions per Second\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_xact_commit{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Commits\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_xact_rollback{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Rollbacks\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Rollbacks\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Rows Operations\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_tup_inserted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Inserted\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_tup_updated{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Updated\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"rate(pg_stat_database_tup_deleted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Deleted\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"rate(pg_stat_database_tup_fetched{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Fetched\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"rowsps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Cache Hit Ratio\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} / (pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} + pg_stat_database_blks_read{datname=\\\"datawarehouse\\\"}) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 80 }, { \"color\": \"green\", \"value\": 95 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Deadlocks\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_deadlocks{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Temp Files Created\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 16 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_temp_files{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 10 }, { \"color\": \"red\", \"value\": 50 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/logs/logs-explorer.json\">\n{\n  \"uid\": \"logs-dashboard\",\n  \"title\": \"Logs Explorer\",\n  \"description\": \"Centralized logs from all services\",\n  \"tags\": [\"logs\", \"loki\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"level\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"search\",\n        \"type\": \"textbox\",\n        \"current\": { \"value\": \"\" },\n        \"label\": \"Search\"\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 5, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"fixed\", \"fixedColor\": \"red\" }\n        },\n        \"overrides\": []\n      }\n    },\n     {\n      \"id\": 3,\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | json | level=~\\\".+\\\" | __error__=\\\"\\\" [$__range]))\",\n          \"legendFormat\": \"{{level}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"palette-classic\" },\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"INFO\": { \"color\": \"green\", \"index\": 0 },\n                \"info\": { \"color\": \"green\", \"index\": 1 },\n                \"ERROR\": { \"color\": \"red\", \"index\": 2 },\n                \"error\": { \"color\": \"red\", \"index\": 3 },\n                \"WARNING\": { \"color\": \"yellow\", \"index\": 4 },\n                \"warning\": { \"color\": \"yellow\", \"index\": 5 },\n                \"WARN\": { \"color\": \"yellow\", \"index\": 6 },\n                \"warn\": { \"color\": \"yellow\", \"index\": 7 },\n                \"DEBUG\": { \"color\": \"blue\", \"index\": 8 },\n                \"debug\": { \"color\": \"blue\", \"index\": 9 }\n              }\n            }\n          ]\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 18, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"red\", \"value\": 10 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 11 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"All Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 15, \"w\": 24, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"dedupStrategy\": \"none\",\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 12, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 38 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Order Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 32,\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/logs/test.json\">\n{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"description\": \"Centralized logs from all services\",\n  \"editable\": true,\n  \"fiscalYearStartMonth\": 0,\n  \"graphTooltip\": 0,\n  \"id\": 12,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"normal\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 1,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [\n            \"sum\"\n          ],\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"fixedColor\": \"red\",\n            \"mode\": \"fixed\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 70,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"none\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 6\n      },\n      \"id\": 2,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [],\n          \"displayMode\": \"list\",\n          \"placement\": \"bottom\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            }\n          },\n          \"mappings\": [],\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"ERROR\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"red\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"WARNING\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"yellow\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"INFO\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"green\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"DEBUG\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"blue\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          }\n        ]\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 6\n      },\n      \"id\": 3,\n      \"options\": {\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"values\": [\n            \"value\",\n            \"percent\"\n          ]\n        },\n        \"pieType\": \"donut\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"sort\": \"desc\",\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"single\",\n          \"sort\": \"none\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"direction\": \"backward\",\n          \"editorMode\": \"code\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | __error__=\\\"\\\" [$__range]))\",\n          \"instant\": true,\n          \"legendFormat\": \"{{level}}\",\n          \"queryType\": \"range\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [],\n          \"noValue\": \"0\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"yellow\",\n                \"value\": 1\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 10\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 6\n      },\n      \"id\": 4,\n      \"options\": {\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"orientation\": \"auto\",\n        \"percentChangeColorMode\": \"standard\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"showPercentChange\": false,\n        \"textMode\": \"auto\",\n        \"wideLayout\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 11\n      },\n      \"id\": 10,\n      \"panels\": [],\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 15,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"id\": 11,\n      \"options\": {\n        \"dedupStrategy\": \"none\",\n        \"enableInfiniteScrolling\": false,\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showCommonLabels\": false,\n        \"showControls\": false,\n        \"showLabels\": true,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"All Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 27\n      },\n      \"id\": 20,\n      \"panels\": [],\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 28\n      },\n      \"id\": 21,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 28\n      },\n      \"id\": 22,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 38\n      },\n      \"id\": 30,\n      \"panels\": [],\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 39\n      },\n      \"id\": 31,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Order Events\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 39\n      },\n      \"id\": 32,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\"\n    }\n  ],\n  \"preload\": false,\n  \"refresh\": \"10s\",\n  \"schemaVersion\": 42,\n  \"tags\": [\n    \"logs\",\n    \"loki\",\n    \"observability\"\n  ],\n  \"templating\": {\n    \"list\": [\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"service\",\n        \"options\": [],\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"level\",\n        \"options\": [],\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"value\": \"\"\n        },\n        \"label\": \"Search\",\n        \"name\": \"search\",\n        \"type\": \"textbox\"\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {},\n  \"timezone\": \"browser\",\n  \"title\": \"Logs Explorer\",\n  \"uid\": \"logs-dashboard\",\n  \"version\": 3\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/overview/correlation-dashboard.json\">\n{\n  \"uid\": \"correlation-dashboard\",\n  \"title\": \"Metrics & Logs Correlation\",\n  \"description\": \"View metrics and related logs side by side\",\n  \"tags\": [\"correlation\", \"metrics\", \"logs\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"name\": \"Errors\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"enable\": true,\n        \"iconColor\": \"red\",\n        \"expr\": \"{service=\\\"$service\\\"} |= \\\"ERROR\\\"\",\n        \"titleFormat\": \"Error\",\n        \"textFormat\": \"{{ __line__ }}\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"📊 Metrics Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Message Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"msg/min\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byRegexp\", \"options\": \"/Error/\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Latency P95\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Producer P95\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Consumer E2E P95\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 8 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Log Volume (matches time range above)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 4, \"w\": 24, \"x\": 0, \"y\": 9 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=\\\"$service\\\"} | json | __error__=\\\"\\\" [$__interval]))\",\n          \"legendFormat\": \"{{level}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"right\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"ERROR\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"INFO\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"WARNING\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Service Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 13 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"}\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔴 Error Logs Only\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 25 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Error Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 26 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"} |~ \\\"(?i)error|ERROR\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/overview/system-overview.json\">\n{\n  \"uid\": \"system-overview\",\n  \"title\": \"System Overview\",\n  \"description\": \"Overview of all monitored services\",\n  \"tags\": [\"overview\", \"prometheus\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Services Health\",\n      \"description\": \"Status of all monitored services (1 = UP, 0 = DOWN)\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 4,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up\",\n          \"legendFormat\": \"{{job}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"orientation\": \"horizontal\",\n        \"textMode\": \"auto\",\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"0\": {\n                  \"text\": \"DOWN\",\n                  \"color\": \"red\",\n                  \"index\": 0\n                },\n                \"1\": {\n                  \"text\": \"UP\",\n                  \"color\": \"green\",\n                  \"index\": 1\n                }\n              }\n            }\n          ],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"green\", \"value\": 1 }\n            ]\n          },\n          \"unit\": \"none\"\n        },\n        \"overrides\": []\n      }\n    },\n    \n    {\n      \"id\": 2,\n      \"title\": \"Prometheus Scrape Duration\",\n      \"description\": \"Time taken to scrape each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_duration_seconds\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"line\",\n            \"lineInterpolation\": \"smooth\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"scheme\",\n            \"spanNulls\": false,\n            \"lineWidth\": 2,\n            \"pointSize\": 5,\n            \"showPoints\": \"auto\"\n          },\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 0.5 },\n              { \"color\": \"red\", \"value\": 1 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Samples Scraped\",\n      \"description\": \"Number of metrics scraped from each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_samples_scraped\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"hue\",\n            \"lineWidth\": 1\n          },\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Prometheus Memory Usage\",\n      \"type\": \"gauge\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"process_resident_memory_bytes{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"min\": 0,\n          \"max\": 1073741824,\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 536870912 },\n              { \"color\": \"red\", \"value\": 858993459 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Prometheus Storage Size\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 6,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_storage_blocks_bytes\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 5368709120 },\n              { \"color\": \"red\", \"value\": 10737418240 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Total Time Series\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_head_series\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - process_start_time_seconds{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/dashboards/tracing/tracing-overview.json\">\n{\n  \"uid\": \"tracing-overview\",\n  \"title\": \"Distributed Tracing Overview\",\n  \"description\": \"Jaeger traces visualization and analysis\",\n  \"tags\": [\"tracing\", \"jaeger\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      },\n      {\n        \"name\": \"operation\",\n        \"type\": \"custom\",\n        \"query\": \"all,produce_order,produce_batch,consume_message,process_batch,db_insert_orders\",\n        \"includeAll\": true,\n        \"current\": { \"text\": \"All\", \"value\": \"$__all\" },\n        \"options\": [\n          { \"text\": \"All\", \"value\": \"$__all\", \"selected\": true },\n          { \"text\": \"produce_order\", \"value\": \"produce_order\", \"selected\": false },\n          { \"text\": \"produce_batch\", \"value\": \"produce_batch\", \"selected\": false },\n          { \"text\": \"consume_message\", \"value\": \"consume_message\", \"selected\": false },\n          { \"text\": \"process_batch\", \"value\": \"process_batch\", \"selected\": false },\n          { \"text\": \"db_insert_orders\", \"value\": \"db_insert_orders\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🔍 Tracing Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Trace Count\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[5m])) * 300\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Avg Duration (Producer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Avg Duration (Consumer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"E2E Latency P95\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 10 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Services\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(up{job=~\\\"data-producer|data-consumer\\\"} == 1)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📊 Latency Distribution\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 5 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"End-to-End Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔎 Trace Search\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 14 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Recent Traces\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"jaeger\", \"uid\": \"jaeger\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"queryType\": \"search\",\n          \"service\": \"$service\",\n          \"limit\": 20\n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\" }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Application Logs (with trace_id)\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 24, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} | json | __error__=\\\"\\\" | trace_id =~ \\\".+\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n</file>\n\n<file path=\"monitoring/grafana/provisioning/dashboards/dashboards.yml\">\n# monitoring/grafana/provisioning/dashboards/dashboards.yml\n\napiVersion: 1\n\nproviders:\n  # ===========================================\n  # Overview Dashboards\n  # ===========================================\n  - name: 'overview'\n    orgId: 1\n    folder: 'Overview'\n    folderUid: 'overview'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: true              # ✅  cho edit trên UI\n    options:\n      path: /var/lib/grafana/dashboards/overview\n\n  # ===========================================\n  # Infrastructure Dashboards\n  # ===========================================\n  - name: 'infrastructure'\n    orgId: 1\n    folder: 'Infrastructure'\n    folderUid: 'infrastructure'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/infrastructure\n\n  # ===========================================\n  # Application Dashboards\n  # ===========================================\n  - name: 'applications'\n    orgId: 1\n    folder: 'Applications'\n    folderUid: 'applications'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/applications\n\n  # ===========================================\n  # Logs Dashboards\n  # ===========================================\n  - name: 'logs'\n    orgId: 1\n    folder: 'Logs'\n    folderUid: 'logs'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/logs\n\n  - name: 'tracing'\n    orgId: 1\n    folder: 'Tracing'\n    folderUid: 'tracing'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/tracing\n</file>\n\n<file path=\"monitoring/grafana/provisioning/datasources/datasources.yml\">\n# monitoring/grafana/provisioning/datasources/datasources.yml\n\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    uid: prometheus                    # ✅ UID cố định - RẤT QUAN TRỌNG!\n    access: proxy\n    url: http://prometheus:9090\n    isDefault: true\n    editable: true                    # Không cho edit trên UI để tránh drift\n    jsonData:\n      timeInterval: \"15s\"\n      httpMethod: POST                 # POST tốt hơn cho large queries\n\n  - name: Alertmanager\n    type: alertmanager\n    uid: alertmanager\n    access: proxy\n    url: http://alertmanager:9093\n    editable: false\n    jsonData:\n      implementation: prometheus\n\n  - name: Loki\n    type: loki\n    uid: loki\n    access: proxy\n    url: http://loki:3100\n    editable: false\n    jsonData:\n      timeout: 60\n      maxLines: 1000\n      derivedFields:\n        # Link từ logs sang related logs by order_id\n        - name: order_id\n          matcherRegex: '\"order_id\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"loki\",\"queries\":[{\"expr\":\"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"$${__value.raw}\\\"\"}]}'\n          datasourceUid: loki\n          urlDisplayLabel: \"View related logs\"\n        \n        # Link từ logs sang metrics by service\n        - name: service_metrics\n          matcherRegex: '\"service\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"prometheus\",\"queries\":[{\"expr\":\"up{job=\\\"$${__value.raw}\\\"}\"}]}'\n          datasourceUid: prometheus\n          urlDisplayLabel: \"View metrics\"\n\n        - name: TraceID\n          matcherRegex: '\"trace_id\":\\s*\"([a-f0-9]+)\"'\n          url: '$${__value.raw}'\n          datasourceUid: jaeger\n          urlDisplayLabel: \"View Trace\"\n\n  - name: Jaeger\n    type: jaeger\n    uid: jaeger\n    access: proxy\n    url: http://jaeger:16686\n    editable: false\n    jsonData:\n      tracesToLogs:\n        datasourceUid: loki\n        tags: ['service']\n        mappedTags: [{ key: 'service.name', value: 'service' }]\n        mapTagNamesEnabled: true\n        spanStartTimeShift: '-1h'\n        spanEndTimeShift: '1h'\n        filterByTraceID: true\n        filterBySpanID: false\n        lokiSearch: true\n</file>\n\n<file path=\"monitoring/loki/rules/fake/alerts.yml\">\n# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"\n</file>\n\n<file path=\"monitoring/loki/rules/alerts.yml\">\n# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"\n</file>\n\n<file path=\"monitoring/loki/loki-config.yml\">\n# monitoring/loki/loki-config.yml\n\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n  grpc_listen_port: 9096\n  log_level: info\n\ncommon:\n  instance_addr: 127.0.0.1\n  path_prefix: /loki\n  storage:\n    filesystem:\n      chunks_directory: /loki/chunks\n      rules_directory: /loki/rules\n  replication_factor: 1\n  ring:\n    kvstore:\n      store: inmemory\n\nquery_range:\n  results_cache:\n    cache:\n      embedded_cache:\n        enabled: true\n        max_size_mb: 100\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: tsdb\n      object_store: filesystem\n      schema: v13\n      index:\n        prefix: index_\n        period: 24h\n\nstorage_config:\n  filesystem:\n    directory: /loki/storage\n\nlimits_config:\n  retention_period: 168h  # 7 days\n  ingestion_rate_mb: 10\n  ingestion_burst_size_mb: 20\n  max_streams_per_user: 10000\n  max_line_size: 256kb\n\ncompactor:\n  working_directory: /loki/compactor\n  compaction_interval: 10m\n  retention_enabled: true\n  retention_delete_delay: 2h\n  delete_request_store: filesystem\n\nruler:\n  alertmanager_url: http://alertmanager:9093\n  storage:\n    type: local\n    local:\n      directory: /loki/rules\n  rule_path: /loki/rules-temp\n  enable_api: true\n  enable_alertmanager_v2: true\n  ring:\n    kvstore:\n      store: inmemory\n\nanalytics:\n  reporting_enabled: false\n</file>\n\n<file path=\"monitoring/prometheus/rules/alert_rules.yml\">\n# monitoring/prometheus/rules/alert_rules.yml\n\ngroups:\n  # ===========================================\n  # Service Health Alerts\n  # ===========================================\n  - name: service_health_alerts\n    rules:\n      # Service Down - Critical\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"{{ $labels.job }} has been down for more than 1 minute.\"\n          runbook_url: \"https://wiki.example.com/runbook/service-down\"\n\n      # Service Flapping - Warning\n      - alert: ServiceFlapping\n        expr: changes(up[10m]) > 3\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Service {{ $labels.job }} is flapping\"\n          description: \"{{ $labels.job }} has changed state {{ $value }} times in the last 10 minutes.\"\n\n  # ===========================================\n  # Data Pipeline Alerts\n  # ===========================================\n  - name: pipeline_alerts\n    rules:\n      # Producer Down\n      - alert: ProducerDown\n        expr: up{job=\"data-producer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n        annotations:\n          summary: \"Data Producer is down\"\n          description: \"Data Producer has been down for more than 1 minute. No data is being produced to Kafka.\"\n\n      # Consumer Down\n      - alert: ConsumerDown\n        expr: up{job=\"data-consumer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: consumer\n        annotations:\n          summary: \"Data Consumer is down\"\n          description: \"Data Consumer has been down for more than 1 minute. No data is being processed.\"\n\n      # Producer Not Producing\n      - alert: ProducerNotProducing\n        expr: rate(pipeline_messages_produced_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer is not producing messages\"\n          description: \"No messages have been produced in the last 5 minutes.\"\n\n      # Consumer Not Consuming\n      - alert: ConsumerNotConsuming\n        expr: rate(pipeline_messages_consumed_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer is not consuming messages\"\n          description: \"No messages have been consumed in the last 5 minutes.\"\n\n      # High Error Rate - Producer\n      - alert: ProducerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_produced_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_produced_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer error rate is high\"\n          description: \"Producer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Error Rate - Consumer\n      - alert: ConsumerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_consumed_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_consumed_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer error rate is high\"\n          description: \"Consumer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Processing Latency\n      - alert: HighProcessingLatency\n        expr: pipeline:processing_latency_p95:seconds > 1\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Processing latency is high\"\n          description: \"P95 processing latency is {{ $value | printf \\\"%.2f\\\" }}s (threshold: 1s).\"\n\n  # ===========================================\n  # Kafka Alerts\n  # ===========================================\n  - name: kafka_alerts\n    rules:\n      # Kafka Down\n      - alert: KafkaDown\n        expr: up{job=\"kafka\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka exporter is down\"\n          description: \"Cannot scrape Kafka metrics. Kafka might be down.\"\n\n      # High Consumer Lag\n      - alert: KafkaHighConsumerLag\n        expr: kafka:consumer_lag:sum > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is high\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 1000).\"\n\n      # Critical Consumer Lag\n      - alert: KafkaCriticalConsumerLag\n        expr: kafka:consumer_lag:sum > 10000\n        for: 5m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is critical\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 10000). Consumer might be stuck.\"\n\n      # No Messages Flowing\n      - alert: KafkaNoMessagesFlowing\n        expr: kafka:messages_in:rate1m:total == 0\n        for: 10m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"No messages flowing through Kafka\"\n          description: \"No messages have been produced to Kafka in the last 10 minutes.\"\n\n  # ===========================================\n  # PostgreSQL Alerts\n  # ===========================================\n  - name: postgresql_alerts\n    rules:\n      # PostgreSQL Down\n      - alert: PostgreSQLDown\n        expr: pg_up == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL is down\"\n          description: \"PostgreSQL database is not responding.\"\n\n      # High Connection Usage\n      - alert: PostgreSQLHighConnections\n        expr: postgresql:connections:usage_percent > 80\n        for: 5m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is high\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical Connection Usage\n      - alert: PostgreSQLCriticalConnections\n        expr: postgresql:connections:usage_percent > 95\n        for: 2m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is critical\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%). New connections may fail.\"\n\n      # Low Cache Hit Ratio\n      - alert: PostgreSQLLowCacheHitRatio\n        expr: postgresql:cache_hit_ratio:percent < 90\n        for: 10m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL cache hit ratio is low\"\n          description: \"Cache hit ratio is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 90%). Consider increasing shared_buffers.\"\n\n      # Deadlocks Detected\n      - alert: PostgreSQLDeadlocks\n        expr: increase(pg_stat_database_deadlocks{datname=\"datawarehouse\"}[5m]) > 0\n        for: 0m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL deadlocks detected\"\n          description: \"{{ $value }} deadlocks detected in the last 5 minutes.\"\n\n  # ===========================================\n  # Container Resource Alerts (Fixed)\n  # ===========================================\n  - name: container_alerts\n    rules:\n      # High CPU Usage\n      - alert: ContainerHighCPU\n        expr: container:cpu_usage_percent:rate5m > 80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical CPU Usage\n      - alert: ContainerCriticalCPU\n        expr: container:cpu_usage_percent:rate5m > 95\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%).\"\n\n      # High Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerHighMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 80\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 80%).\"\n\n      # Critical Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerCriticalMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 95\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 95%). OOM kill imminent.\"\n\n      # High Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerHighMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 1073741824\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} using high memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 1GB).\"\n\n      # Critical Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerCriticalMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 2147483648\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} using critical memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 2GB).\"\n\n      # Container Restarting\n      - alert: ContainerRestarting\n        expr: increase(container_restart_count{name=~\".+\"}[1h]) > 3\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} is restarting frequently\"\n          description: \"Container has restarted {{ $value | printf \\\"%.0f\\\" }} times in the last hour.\"\n</file>\n\n<file path=\"monitoring/prometheus/rules/recording_rules.yml\">\n# monitoring/prometheus/rules/recording_rules.yml\n\ngroups:\n  # ===========================================\n  # Container Metrics - Pre-computed\n  # ===========================================\n  - name: container_metrics\n    interval: 15s\n    rules:\n      - record: container:cpu_usage_percent:rate5m\n        expr: rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]) * 100\n\n      - record: container:memory_usage_bytes:current\n        expr: container_memory_usage_bytes{name=~\".+\"}\n\n      - record: container:memory_usage_percent:current\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} / \n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n\n      - record: container:network_receive_bytes:rate5m\n        expr: rate(container_network_receive_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:network_transmit_bytes:rate5m\n        expr: rate(container_network_transmit_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:cpu_usage_percent:total\n        expr: sum(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m])) * 100\n\n      - record: container:memory_usage_bytes:total\n        expr: sum(container_memory_usage_bytes{name=~\".+\"})\n\n      - record: container:count:total\n        expr: count(container_last_seen{name=~\".+\"})\n\n  # ===========================================\n  # RED Metrics - Producer\n  # ===========================================\n  - name: red_producer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:producer:rate_per_second\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:producer:rate_per_minute\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:producer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_producer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:producer:errors_per_minute\n        expr: sum(rate(pipeline_producer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:producer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n  # ===========================================\n  # RED Metrics - Consumer\n  # ===========================================\n  - name: red_consumer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:consumer:rate_per_second\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:consumer:rate_per_minute\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m])) * 60\n\n      - record: red:consumer:db_inserts_per_minute\n        expr: sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:consumer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_consumer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:consumer:errors_per_minute\n        expr: sum(rate(pipeline_consumer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:consumer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      # End-to-End Latency\n      - record: red:e2e:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n  # ===========================================\n  # Business Metrics\n  # ===========================================\n  - name: business_metrics\n    interval: 15s\n    rules:\n      - record: business:orders:rate_per_minute\n        expr: sum(rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue:rate_per_minute\n        expr: sum(rate(pipeline_business_revenue_total[1m])) * 60\n\n      - record: business:avg_order_value\n        expr: |\n          sum(rate(pipeline_business_revenue_total[5m])) /\n          (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\n\n      - record: business:orders_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\n\n  # ===========================================\n  # Kafka Metrics\n  # ===========================================\n  - name: kafka_metrics\n    interval: 15s\n    rules:\n      - record: kafka:messages_in:rate1m\n        expr: sum by (topic) (rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:messages_in:rate1m:total\n        expr: sum(rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:consumer_lag:total\n        expr: sum by (consumergroup, topic) (kafka_consumergroup_lag)\n\n      - record: kafka:consumer_lag:sum\n        expr: sum(kafka_consumergroup_lag)\n\n      - record: kafka:partitions:count\n        expr: sum(kafka_topic_partitions)\n\n  # ===========================================\n  # PostgreSQL Metrics\n  # ===========================================\n  - name: postgresql_metrics\n    interval: 15s\n    rules:\n      - record: postgresql:transactions:rate1m\n        expr: rate(pg_stat_database_xact_commit{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_inserted:rate1m\n        expr: rate(pg_stat_database_tup_inserted{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_fetched:rate1m\n        expr: rate(pg_stat_database_tup_fetched{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:connections:active\n        expr: sum(pg_stat_activity_count{datname=\"datawarehouse\"})\n\n      - record: postgresql:connections:usage_percent\n        expr: |\n          (sum(pg_stat_activity_count{datname=\"datawarehouse\"}) / \n           pg_settings_max_connections) * 100\n\n      - record: postgresql:cache_hit_ratio:percent\n        expr: |\n          (\n            pg_stat_database_blks_hit{datname=\"datawarehouse\"} /\n            (pg_stat_database_blks_hit{datname=\"datawarehouse\"} + \n             pg_stat_database_blks_read{datname=\"datawarehouse\"} + 0.001)\n          ) * 100\n\n      - record: postgresql:database_size:bytes\n        expr: pg_database_size_bytes{datname=\"datawarehouse\"}\n\n  # ===========================================\n  # Service Health\n  # ===========================================\n  - name: service_health\n    interval: 15s\n    rules:\n      - record: service:up:status\n        expr: up\n\n      - record: service:healthy:count\n        expr: count(up == 1)\n\n      - record: service:unhealthy:count\n        expr: count(up == 0)\n\n      - record: service:health:percent\n        expr: (count(up == 1) / count(up)) * 100\n</file>\n\n<file path=\"monitoring/prometheus/rules/slo_alerts.yml\">\n# monitoring/prometheus/rules/slo_alerts.yml\n\ngroups:\n  # ===========================================\n  # Error Budget Alerts\n  # ===========================================\n  - name: error_budget_alerts\n    rules:\n      # Producer error budget low\n      - alert: ProducerErrorBudgetLow\n        expr: error_budget:producer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is running low\"\n          description: \"Producer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      # Producer error budget critical\n      - alert: ProducerErrorBudgetCritical\n        expr: error_budget:producer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is critically low\"\n          description: \"Producer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining. Consider freezing deployments.\"\n\n      # Producer error budget exhausted\n      - alert: ProducerErrorBudgetExhausted\n        expr: error_budget:producer:remaining_percent <= 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget exhausted!\"\n          description: \"Producer has exhausted its error budget. SLO is being violated.\"\n\n      # Consumer error budget alerts\n      - alert: ConsumerErrorBudgetLow\n        expr: error_budget:consumer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is running low\"\n          description: \"Consumer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      - alert: ConsumerErrorBudgetCritical\n        expr: error_budget:consumer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is critically low\"\n          description: \"Consumer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining.\"\n\n  # ===========================================\n  # Burn Rate Alerts (Multi-window)\n  # ===========================================\n  - name: burn_rate_alerts\n    rules:\n      # Fast burn - will exhaust budget in ~2 hours\n      # 1h window with 14.4x burn rate\n      - alert: ProducerHighBurnRate\n        expr: |\n          burn_rate:producer:1h > 14.4\n          and\n          burn_rate:producer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer is burning error budget too fast\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. At this rate, error budget will be exhausted in ~2 hours.\"\n\n      # Slow burn - will exhaust budget in ~1 day\n      - alert: ProducerModerateBurnRate\n        expr: |\n          burn_rate:producer:6h > 6\n          and\n          burn_rate:producer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer burn rate is elevated\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. Error budget may be exhausted within a day.\"\n\n      # Consumer burn rate alerts\n      - alert: ConsumerHighBurnRate\n        expr: |\n          burn_rate:consumer:1h > 14.4\n          and\n          burn_rate:consumer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer is burning error budget too fast\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n      - alert: ConsumerModerateBurnRate\n        expr: |\n          burn_rate:consumer:6h > 6\n          and\n          burn_rate:consumer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer burn rate is elevated\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n  # ===========================================\n  # SLO Violation Alerts\n  # ===========================================\n  - name: slo_violation_alerts\n    rules:\n      # Producer SLO violations\n      - alert: ProducerSLOViolation\n        expr: sli:producer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer SLO is being violated\"\n          description: \"Producer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ProducerLatencySLOViolation\n        expr: sli:producer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of requests are under 100ms (target: 99%)\"\n\n      # Consumer SLO violations\n      - alert: ConsumerSLOViolation\n        expr: sli:consumer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer SLO is being violated\"\n          description: \"Consumer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ConsumerLatencySLOViolation\n        expr: sli:consumer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer E2E latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of messages are processed under 5s (target: 99%)\"\n\n      # Data freshness SLO\n      - alert: DataFreshnessSLOViolation\n        expr: sum(kafka_consumergroup_lag) > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: pipeline\n          category: slo\n        annotations:\n          summary: \"Data freshness SLO is being violated\"\n          description: \"Consumer lag is {{ $value }} messages (threshold: 1000)\"\n</file>\n\n<file path=\"monitoring/prometheus/rules/slo_rules.yml\">\n# monitoring/prometheus/rules/slo_rules.yml\n\ngroups:\n  # ===========================================\n  # SLI Definitions\n  # ===========================================\n  - name: sli_metrics\n    interval: 30s\n    rules:\n      # --- Producer SLIs ---\n      \n      # Availability SLI: % of time producer is up\n      - record: sli:producer:availability\n        expr: avg_over_time(up{job=\"data-producer\"}[5m])\n\n      # Success Rate SLI: % of successful messages\n      - record: sli:producer:success_rate\n        expr: |\n          sum(rate(pipeline_producer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of requests under threshold (100ms)\n      - record: sli:producer:latency_good\n        expr: |\n          sum(rate(pipeline_producer_message_duration_seconds_bucket{le=\"0.1\"}[5m])) /\n          (sum(rate(pipeline_producer_message_duration_seconds_count[5m])) + 0.001)\n\n      # --- Consumer SLIs ---\n      \n      # Availability SLI\n      - record: sli:consumer:availability\n        expr: avg_over_time(up{job=\"data-consumer\"}[5m])\n\n      # Success Rate SLI\n      - record: sli:consumer:success_rate\n        expr: |\n          sum(rate(pipeline_consumer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of E2E latency under threshold (5s)\n      - record: sli:consumer:latency_good\n        expr: |\n          sum(rate(pipeline_consumer_end_to_end_latency_seconds_bucket{le=\"5.0\"}[5m])) /\n          (sum(rate(pipeline_consumer_end_to_end_latency_seconds_count[5m])) + 0.001)\n\n      # --- Pipeline SLIs ---\n      \n      # Data Freshness SLI: Consumer lag < 1000 messages\n      - record: sli:pipeline:data_freshness\n        expr: |\n          (sum(kafka_consumergroup_lag) < 1000) or vector(0)\n\n      # --- Database SLIs ---\n      \n      # Availability SLI\n      - record: sli:database:availability\n        expr: avg_over_time(pg_up[5m])\n\n      # Query Success Rate\n      - record: sli:database:query_success_rate\n        expr: |\n          sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_db_operations_total[5m])) + 0.001)\n\n  # ===========================================\n  # SLO Calculations (Rolling Windows)\n  # ===========================================\n  - name: slo_calculations\n    interval: 1m\n    rules:\n      # --- Producer SLOs ---\n      \n      # 30-day rolling availability (target: 99.9%)\n      - record: slo:producer:availability_30d\n        expr: avg_over_time(sli:producer:availability[30d])\n\n      # 30-day rolling success rate (target: 99.9%)\n      - record: slo:producer:success_rate_30d\n        expr: avg_over_time(sli:producer:success_rate[30d])\n\n      # 30-day rolling latency compliance (target: 99%)\n      - record: slo:producer:latency_compliance_30d\n        expr: avg_over_time(sli:producer:latency_good[30d])\n\n      # --- Consumer SLOs ---\n      \n      - record: slo:consumer:availability_30d\n        expr: avg_over_time(sli:consumer:availability[30d])\n\n      - record: slo:consumer:success_rate_30d\n        expr: avg_over_time(sli:consumer:success_rate[30d])\n\n      - record: slo:consumer:latency_compliance_30d\n        expr: avg_over_time(sli:consumer:latency_good[30d])\n\n      # --- Database SLOs ---\n      \n      - record: slo:database:availability_30d\n        expr: avg_over_time(sli:database:availability[30d])\n\n      - record: slo:database:query_success_rate_30d\n        expr: avg_over_time(sli:database:query_success_rate[30d])\n\n  # ===========================================\n  # Error Budget Calculations\n  # ===========================================\n  - name: error_budget\n    interval: 1m\n    rules:\n      # --- Producer Error Budget ---\n      \n      # Error budget remaining (target 99.9% = 0.1% budget)\n      # Formula: (SLO - (1 - current_success_rate)) / (1 - SLO)\n      - record: error_budget:producer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:producer:success_rate)) / 0.001\n          ) * 100\n\n      # Error budget consumed\n      - record: error_budget:producer:consumed_percent\n        expr: 100 - error_budget:producer:remaining_percent\n\n      # --- Consumer Error Budget ---\n      \n      - record: error_budget:consumer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:consumer:success_rate)) / 0.001\n          ) * 100\n\n      - record: error_budget:consumer:consumed_percent\n        expr: 100 - error_budget:consumer:remaining_percent\n\n      # --- Combined Pipeline Error Budget ---\n      \n      - record: error_budget:pipeline:remaining_percent\n        expr: |\n          (\n            error_budget:producer:remaining_percent + \n            error_budget:consumer:remaining_percent\n          ) / 2\n\n  # ===========================================\n  # Burn Rate Calculations\n  # ===========================================\n  - name: burn_rate\n    interval: 1m\n    rules:\n      # Burn rate = actual error rate / allowed error rate\n      # If burn rate > 1, we're consuming budget faster than allowed\n      \n      # Producer burn rate (1h window)\n      - record: burn_rate:producer:1h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[1h])) / 0.001\n\n      # Producer burn rate (6h window)\n      - record: burn_rate:producer:6h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[6h])) / 0.001\n\n      # Consumer burn rate (1h window)\n      - record: burn_rate:consumer:1h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[1h])) / 0.001\n\n      # Consumer burn rate (6h window)\n      - record: burn_rate:consumer:6h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[6h])) / 0.001\n</file>\n\n<file path=\"monitoring/prometheus/prometheus.yml\">\n# monitoring/prometheus/prometheus.yml\n\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    monitor: 'data-pipeline-monitor'\n\notlp:\n  promote_resource_attributes:\n    - service.name\n    - service.namespace\n    - service.instance.id\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n            - alertmanager:9093\n\nrule_files:\n  - /etc/prometheus/rules/*.yml\n\nscrape_configs:\n  # ===========================================\n  # Monitoring Stack\n  # ===========================================\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n        labels:\n          service: 'prometheus'\n          layer: 'monitoring'\n\n  - job_name: 'grafana'\n    static_configs:\n      - targets: ['grafana:3000']\n        labels:\n          service: 'grafana'\n          layer: 'monitoring'\n\n  - job_name: 'alertmanager'\n    static_configs:\n      - targets: ['alertmanager:9093']\n        labels:\n          service: 'alertmanager'\n          layer: 'monitoring'\n\n  - job_name: 'cadvisor'\n    static_configs:\n      - targets: ['cadvisor:8080']\n        labels:\n          service: 'cadvisor'\n          layer: 'monitoring'\n    metric_relabel_configs:\n      # Giữ lại metrics của containers trong docker-compose project\n      - source_labels: [container_label_com_docker_compose_project]\n        regex: '.+'\n        action: keep\n      # Chỉ drop các metrics không cần thiết (bỏ container_last_seen khỏi list)\n      - source_labels: [__name__]\n        regex: 'container_(tasks_state|memory_failures_total)'\n        action: drop\n\n  # ===========================================\n  # Infrastructure\n  # ===========================================\n  - job_name: 'postgresql'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n        labels:\n          service: 'postgresql'\n          layer: 'infrastructure'\n          database: 'datawarehouse'\n\n  - job_name: 'kafka'\n    static_configs:\n      - targets: ['kafka-exporter:9308']\n        labels:\n          service: 'kafka'\n          layer: 'infrastructure'\n\n  # ===========================================\n  # Applications\n  # ===========================================\n  - job_name: 'data-producer'\n    static_configs:\n      - targets: ['data-producer:8000']\n        labels:\n          service: 'producer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  - job_name: 'data-consumer'\n    static_configs:\n      - targets: ['data-consumer:8001']\n        labels:\n          service: 'consumer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  # ===========================================\n  # stress test\n  # ===========================================\n  - job_name: 'load-test'\n    static_configs:\n      - targets: ['load-test:8002']\n        labels:\n          service: 'load-test'\n          layer: 'testing'\n    scrape_interval: 5s\n</file>\n\n<file path=\"monitoring/promtail/promtail-config.yml\">\n# monitoring/promtail/promtail-config.yml\n\nserver:\n  http_listen_port: 9080\n  grpc_listen_port: 0\n  log_level: info\n\npositions:\n  filename: /tmp/positions.yaml\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n    tenant_id: fake\n\nscrape_configs:\n  # ===========================================\n  # Docker Container Logs\n  # ===========================================\n  - job_name: docker\n    docker_sd_configs:\n      - host: unix:///var/run/docker.sock\n        refresh_interval: 5s\n    relabel_configs:\n      # Lấy container name làm label\n      - source_labels: ['__meta_docker_container_name']\n        regex: '/(.*)'\n        target_label: 'container'\n      \n      # Lấy compose service name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'service'\n      \n      # Lấy compose project name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        target_label: 'project'\n      \n      # Lấy container ID\n      - source_labels: ['__meta_docker_container_id']\n        target_label: 'container_id'\n      \n      # Thêm job label\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'job'\n      \n      # Filter: chỉ lấy logs từ containers có compose project\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        regex: '.+'\n        action: keep\n\n    pipeline_stages:\n      # ===========================================\n      # Parse JSON logs (từ Producer/Consumer)\n      # ===========================================\n      - match:\n          selector: '{service=~\"data-producer|data-consumer\"}'\n          stages:\n            - json:\n                expressions:\n                  level: level\n                  message: message\n                  event: event\n                  service: service\n                  timestamp: timestamp\n                  order_id: order_id\n                  category: category\n                  error: error\n                  error_type: error_type\n                  batch_size: batch_size\n                  duration_ms: duration_ms\n                  throughput_per_sec: throughput_per_sec\n            \n            # Set log level as label\n            - labels:\n                level:\n                event:\n            \n            # Set timestamp from log\n            - timestamp:\n                source: timestamp\n                format: RFC3339Nano\n                fallback_formats:\n                  - RFC3339\n            \n            # Extract metrics from logs (optional)\n            - metrics:\n                log_lines_total:\n                  type: Counter\n                  description: \"Total log lines\"\n                  source: message\n                  config:\n                    action: inc\n                    match_all: true\n\n      # ===========================================\n      # Parse Kafka logs\n      # ===========================================\n      - match:\n          selector: '{service=\"kafka\"}'\n          stages:\n            - regex:\n                expression: '^\\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})\\] (?P<level>\\w+) (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Parse PostgreSQL logs\n      # ===========================================\n      - match:\n          selector: '{service=\"postgres\"}'\n          stages:\n            - regex:\n                expression: '^(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3} \\w+) \\[(?P<pid>\\d+)\\] (?P<level>\\w+):  (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Default: keep raw log\n      # ===========================================\n      - match:\n          selector: '{level=\"\"}'\n          stages:\n            - static_labels:\n                level: info\n</file>\n\n<file path=\"monitoring/stress-testing/load-test/Dockerfile\">\n# stress-testing/load-test/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY load_test.py .\n\nENTRYPOINT [\"python\", \"load_test.py\"]\nCMD [\"--help\"]\n</file>\n\n<file path=\"monitoring/stress-testing/load-test/load_test.py\">\n# stress-testing/load-test/load_test.py\n\nimport json\nimport random\nimport time\nimport uuid\nimport threading\nimport signal\nimport sys\nfrom datetime import datetime, timezone\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any\n\nimport click\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\n\n# ===========================================\n# Metrics\n# ===========================================\nMESSAGES_SENT = Counter(\n    'loadtest_messages_sent_total',\n    'Total messages sent',\n    ['status']\n)\n\nSEND_DURATION = Histogram(\n    'loadtest_send_duration_seconds',\n    'Time to send message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)\n)\n\nCURRENT_RPS = Gauge(\n    'loadtest_current_rps',\n    'Current requests per second'\n)\n\nACTIVE_THREADS = Gauge(\n    'loadtest_active_threads',\n    'Number of active threads'\n)\n\nTARGET_RPS = Gauge(\n    'loadtest_target_rps',\n    'Target requests per second'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order() -> dict[str, Any]:\n    \"\"\"Generate a fake order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    return {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n        \"load_test\": True,\n    }\n\n\n# ===========================================\n# Load Tester\n# ===========================================\nclass LoadTester:\n    def __init__(self, bootstrap_servers: str, topic: str):\n        self.bootstrap_servers = bootstrap_servers\n        self.topic = topic\n        self.producer = None\n        self.running = False\n        self.stats = {\n            \"sent\": 0,\n            \"failed\": 0,\n            \"start_time\": None,\n        }\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka\"\"\"\n        max_retries = 10\n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=self.bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks=1,  # Faster for load testing\n                    linger_ms=5,\n                    batch_size=16384,\n                    buffer_memory=33554432,\n                )\n                click.echo(f\"✅ Connected to Kafka: {self.bootstrap_servers}\")\n                return\n            except KafkaError as e:\n                click.echo(f\"⏳ Kafka connection attempt {attempt + 1}/{max_retries}: {e}\")\n                time.sleep(2)\n        \n        raise Exception(\"Failed to connect to Kafka\")\n    \n    def send_message(self) -> bool:\n        \"\"\"Send a single message\"\"\"\n        order = generate_order()\n        \n        try:\n            start = time.time()\n            future = self.producer.send(\n                self.topic,\n                key=order[\"order_id\"],\n                value=order\n            )\n            future.get(timeout=10)\n            duration = time.time() - start\n            \n            MESSAGES_SENT.labels(status=\"success\").inc()\n            SEND_DURATION.observe(duration)\n            self.stats[\"sent\"] += 1\n            return True\n            \n        except Exception as e:\n            MESSAGES_SENT.labels(status=\"failed\").inc()\n            self.stats[\"failed\"] += 1\n            return False\n    \n    def run_constant_load(self, rps: int, duration_seconds: int, threads: int = 10):\n        \"\"\"Run constant load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        TARGET_RPS.set(rps)\n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n🚀 Starting Constant Load Test\")\n        click.echo(f\"   Target RPS: {rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        interval = 1.0 / rps if rps > 0 else 1\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                batch_start = time.time()\n                futures = []\n                \n                # Submit batch of requests\n                for _ in range(min(rps, 100)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                # Wait for completion\n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                # Calculate actual RPS\n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                # Print progress\n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Sent: {self.stats['sent']:,} | \"\n                    f\"Failed: {self.stats['failed']:,} | \"\n                    f\"RPS: {actual_rps:.1f}\",\n                    nl=False\n                )\n                \n                # Rate limiting\n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_ramp_up(self, start_rps: int, end_rps: int, duration_seconds: int, threads: int = 20):\n        \"\"\"Run ramp-up load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n📈 Starting Ramp-Up Load Test\")\n        click.echo(f\"   Start RPS: {start_rps}\")\n        click.echo(f\"   End RPS: {end_rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        rps_increment = (end_rps - start_rps) / duration_seconds\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                elapsed = time.time() - self.stats[\"start_time\"]\n                current_target_rps = int(start_rps + (rps_increment * elapsed))\n                TARGET_RPS.set(current_target_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_target_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Target RPS: {current_target_rps:3d} | \"\n                    f\"Actual RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_spike(self, base_rps: int, spike_rps: int, spike_duration: int, total_duration: int, threads: int = 20):\n        \"\"\"Run spike test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n⚡ Starting Spike Test\")\n        click.echo(f\"   Base RPS: {base_rps}\")\n        click.echo(f\"   Spike RPS: {spike_rps}\")\n        click.echo(f\"   Spike Duration: {spike_duration}s\")\n        click.echo(f\"   Total Duration: {total_duration}s\")\n        click.echo(\"-\" * 50)\n        \n        end_time = time.time() + total_duration\n        spike_start = time.time() + (total_duration - spike_duration) / 2\n        spike_end = spike_start + spike_duration\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                current_time = time.time()\n                \n                # Determine current RPS\n                if spike_start <= current_time <= spike_end:\n                    current_rps = spike_rps\n                    phase = \"🔥 SPIKE\"\n                else:\n                    current_rps = base_rps\n                    phase = \"📊 BASE \"\n                \n                TARGET_RPS.set(current_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r{phase} | Remaining: {remaining:3d}s | \"\n                    f\"RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def _print_summary(self):\n        \"\"\"Print test summary\"\"\"\n        elapsed = time.time() - self.stats[\"start_time\"]\n        total = self.stats[\"sent\"] + self.stats[\"failed\"]\n        success_rate = (self.stats[\"sent\"] / total * 100) if total > 0 else 0\n        avg_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n        \n        click.echo(\"\\n\")\n        click.echo(\"=\" * 50)\n        click.echo(\"📊 LOAD TEST SUMMARY\")\n        click.echo(\"=\" * 50)\n        click.echo(f\"   Duration:     {elapsed:.1f}s\")\n        click.echo(f\"   Total Sent:   {self.stats['sent']:,}\")\n        click.echo(f\"   Failed:       {self.stats['failed']:,}\")\n        click.echo(f\"   Success Rate: {success_rate:.2f}%\")\n        click.echo(f\"   Avg RPS:      {avg_rps:.1f}\")\n        click.echo(\"=\" * 50)\n    \n    def stop(self):\n        \"\"\"Stop the test\"\"\"\n        self.running = False\n        if self.producer:\n            self.producer.close()\n\n\n# ===========================================\n# CLI\n# ===========================================\n@click.group()\ndef cli():\n    \"\"\"Load Testing Tool for Data Pipeline\"\"\"\n    pass\n\n\n@cli.command()\n@click.option('--rps', default=50, help='Requests per second')\n@click.option('--duration', default=60, help='Duration in seconds')\n@click.option('--threads', default=10, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef constant(rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run constant load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_constant_load(rps, duration, threads)\n\n\n@cli.command()\n@click.option('--start-rps', default=10, help='Starting RPS')\n@click.option('--end-rps', default=100, help='Ending RPS')\n@click.option('--duration', default=120, help='Duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef rampup(start_rps, end_rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run ramp-up load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_ramp_up(start_rps, end_rps, duration, threads)\n\n\n@cli.command()\n@click.option('--base-rps', default=20, help='Base RPS')\n@click.option('--spike-rps', default=200, help='Spike RPS')\n@click.option('--spike-duration', default=30, help='Spike duration in seconds')\n@click.option('--total-duration', default=120, help='Total duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef spike(base_rps, spike_rps, spike_duration, total_duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run spike test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_spike(base_rps, spike_rps, spike_duration, total_duration, threads)\n\n\nif __name__ == \"__main__\":\n    cli()\n</file>\n\n<file path=\"monitoring/stress-testing/load-test/requirements.txt\">\n# stress-testing/load-test/requirements.txt\n\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data\nfaker==24.4.0\n\n# CLI\nclick==8.1.7\n\n# Metrics\nprometheus-client==0.20.0\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n</file>\n\n<file path=\"monitoring/stress-testing/scripts/chaos_test.sh\">\n#!/bin/bash\n# stress-testing/scripts/chaos_test.sh\n\necho \"💥 Starting Chaos Test\"\necho \"======================\"\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\ncd \"$(dirname \"$0\")/../../\"\n\n# Function to check service health\ncheck_health() {\n    local service=$1\n    local url=$2\n    if curl -s \"$url\" > /dev/null 2>&1; then\n        echo -e \"${GREEN}✅ $service is UP${NC}\"\n        return 0\n    else\n        echo -e \"${RED}❌ $service is DOWN${NC}\"\n        return 1\n    fi\n}\n\n# Function to wait for recovery\nwait_for_recovery() {\n    local service=$1\n    local url=$2\n    local max_wait=60\n    local waited=0\n    \n    echo -e \"${YELLOW}⏳ Waiting for $service to recover...${NC}\"\n    \n    while [ $waited -lt $max_wait ]; do\n        if curl -s \"$url\" > /dev/null 2>&1; then\n            echo -e \"${GREEN}✅ $service recovered after ${waited}s${NC}\"\n            return 0\n        fi\n        sleep 2\n        waited=$((waited + 2))\n    done\n    \n    echo -e \"${RED}❌ $service did not recover within ${max_wait}s${NC}\"\n    return 1\n}\n\necho \"\"\necho \"📊 Initial Health Check\"\necho \"-----------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\ncheck_health \"Prometheus\" \"http://localhost:9090/-/healthy\"\ncheck_health \"Grafana\" \"http://localhost:3000/api/health\"\n\necho \"\"\necho \"💥 Test 1: Kill Consumer\"\necho \"------------------------\"\ndocker stop data-consumer\necho -e \"${YELLOW}Consumer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Consumer...\"\ndocker start data-consumer\nwait_for_recovery \"Consumer\" \"http://localhost:8001/metrics\"\n\necho \"\"\necho \"💥 Test 2: Kill Producer\"\necho \"------------------------\"\ndocker stop data-producer\necho -e \"${YELLOW}Producer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Producer...\"\ndocker start data-producer\nwait_for_recovery \"Producer\" \"http://localhost:8000/metrics\"\n\necho \"\"\necho \"💥 Test 3: Kill Kafka (WARNING: This will affect data flow)\"\necho \"------------------------------------------------------------\"\nread -p \"Do you want to proceed? (y/N) \" -n 1 -r\necho\nif [[ $REPLY =~ ^[Yy]$ ]]; then\n    docker stop kafka\n    echo -e \"${YELLOW}Kafka stopped. Check Grafana for cascading failures...${NC}\"\n    echo \"Waiting 60 seconds...\"\n    sleep 60\n    \n    echo \"\"\n    echo \"🔄 Restarting Kafka...\"\n    docker start kafka\n    sleep 10\n    wait_for_recovery \"Kafka\" \"http://localhost:9308/metrics\"\nfi\n\necho \"\"\necho \"📊 Final Health Check\"\necho \"---------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\n\necho \"\"\necho \"✅ Chaos Test Complete!\"\necho \"Check Grafana dashboards and Alertmanager for results.\"\n</file>\n\n<file path=\"monitoring/stress-testing/scripts/ramp_up_test.sh\">\n#!/bin/bash\n# stress-testing/scripts/ramp_up_test.sh\n\necho \"📈 Starting Ramp-Up Load Test\"\necho \"==============================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test rampup \\\n    --start-rps ${START_RPS:-10} \\\n    --end-rps ${END_RPS:-100} \\\n    --duration ${DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n</file>\n\n<file path=\"monitoring/stress-testing/scripts/run_load_test.sh\">\n#!/bin/bash\n# stress-testing/scripts/run_load_test.sh\n\necho \"🚀 Starting Constant Load Test\"\necho \"================================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test constant \\\n    --rps ${RPS:-50} \\\n    --duration ${DURATION:-60} \\\n    --threads ${THREADS:-10} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n</file>\n\n<file path=\"monitoring/stress-testing/scripts/spike_test.sh\">\n#!/bin/bash\n# stress-testing/scripts/spike_test.sh\n\necho \"⚡ Starting Spike Test\"\necho \"======================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test spike \\\n    --base-rps ${BASE_RPS:-20} \\\n    --spike-rps ${SPIKE_RPS:-200} \\\n    --spike-duration ${SPIKE_DURATION:-30} \\\n    --total-duration ${TOTAL_DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n</file>\n\n<file path=\"monitoring/stress-testing/docker-compose-stress-test.yml\">\n# stress-testing/docker-compose.yml\n\nservices:\n  load-test:\n    build:\n      context: ./load-test\n      dockerfile: Dockerfile\n    container_name: load-test\n    networks:\n      - monitoring-net\n    ports:\n      - \"8002:8002\"\n    environment:\n      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092\n      - KAFKA_TOPIC=ecommerce.orders\n    # Command sẽ được override khi chạy\n    command: [\"--help\"]\n\nnetworks:\n  monitoring-net:\n    external: true\n</file>\n\n<file path=\"monitoring/docker-compose-monitoring.yml\">\n# monitoring/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Prometheus - Metrics Collection & Storage\n  # ===========================================\n  prometheus:\n    image: prom/prometheus:v3.5.0\n    container_name: prometheus\n    restart: unless-stopped\n    \n    # Command line arguments\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention.time=15d'        # Giữ data 15 ngày\n      - '--web.enable-lifecycle'                   # Cho phép reload config via API\n      - '--web.enable-admin-api'                   # Enable admin API\n      - '--web.enable-remote-write-receiver'\n    volumes:\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - ./prometheus/rules:/etc/prometheus/rules:ro\n      - prometheus_data:/prometheus\n    \n    ports:\n      - \"9090:9090\"\n    \n    networks:\n      - monitoring-net\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9090/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Alertmanager\n  # ===========================================\n  alertmanager:\n    image: prom/alertmanager:v0.27.0\n    container_name: alertmanager\n    restart: unless-stopped\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n      - '--web.external-url=http://localhost:9093'\n      - '--cluster.listen-address='\n    volumes:\n      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro\n      - alertmanager_data:/alertmanager\n    ports:\n      - \"9093:9093\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9093/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n\n  # ===========================================\n  # Loki - Log Aggregation\n  # ===========================================\n  loki:\n    image: grafana/loki:3.3.2\n    container_name: loki\n    restart: unless-stopped\n    command: -config.file=/etc/loki/loki-config.yml\n    volumes:\n      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro\n      - ./loki/rules:/loki/rules:ro\n      - loki_data:/loki\n    ports:\n      - \"3100:3100\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3100/ready\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Promtail - Log Collector\n  # ===========================================\n  promtail:\n    image: grafana/promtail:3.3.2\n    container_name: promtail\n    restart: unless-stopped\n    command: -config.file=/etc/promtail/promtail-config.yml\n    volumes:\n      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n    ports:\n      - \"9080:9080\"\n    networks:\n      - monitoring-net\n    depends_on:\n      loki:\n        condition: service_healthy\n\n\n  # ===========================================\n  # Grafana - Visualization & Dashboards\n  # ===========================================\n  grafana:\n    image: grafana/grafana:12.3.1\n    container_name: grafana\n    restart: unless-stopped\n    \n    environment:\n      # Admin credentials\n      - GF_SECURITY_ADMIN_USER=admin\n      - GF_SECURITY_ADMIN_PASSWORD=admin123\n      \n      # Server settings\n      - GF_SERVER_ROOT_URL=http://localhost:3000\n      \n      # Disable analytics\n      - GF_ANALYTICS_REPORTING_ENABLED=false\n      - GF_ANALYTICS_CHECK_FOR_UPDATES=false\n      \n      # Feature toggles (Grafana 12 features)\n      - GF_FEATURE_TOGGLES_ENABLE=nestedFolders\n    volumes:\n      # Provisioning - auto setup datasources & dashboards\n      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro\n      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro\n      \n      # Dashboard JSON files - mount trực tiếp từ local 👇\n      - ./grafana/dashboards/overview:/var/lib/grafana/dashboards/overview:ro\n      - ./grafana/dashboards/infrastructure:/var/lib/grafana/dashboards/infrastructure:ro\n      - ./grafana/dashboards/applications:/var/lib/grafana/dashboards/applications:ro\n      - ./grafana/dashboards/logs:/var/lib/grafana/dashboards/logs:ro\n      - ./grafana/dashboards/tracing:/var/lib/grafana/dashboards/tracing:ro\n      # Persistent storage\n      - grafana_data:/var/lib/grafana\n    \n    ports:\n      - \"3000:3000\"\n    \n    networks:\n      - monitoring-net\n    \n    depends_on:\n      prometheus:\n        condition: service_healthy\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3000/api/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  # ===========================================\n  # cAdvisor - Container Metrics\n  # ===========================================\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor:v0.51.0\n    container_name: cadvisor\n    restart: unless-stopped\n    privileged: true\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    ports:\n      - \"8080:8080\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8080/healthz\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n  # Jaeger - Distributed Tracing\n  # ===========================================\n  jaeger:\n    image: jaegertracing/all-in-one:1.54\n    container_name: jaeger\n    restart: unless-stopped\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n      - LOG_LEVEL=info\n    ports:\n      - \"16686:16686\"   # Jaeger UI\n      - \"4317:4317\"     # OTLP gRPC\n      - \"4318:4318\"     # OTLP HTTP\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:16686/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Volumes - Persistent Storage\n# ===========================================\nvolumes:\n  prometheus_data:\n    name: prometheus_data\n  grafana_data:\n    name: grafana_data\n  grafana_dashboards:\n    name: grafana_dashboards\n  alertmanager_data:\n    name: alertmanager_data\n  loki_data:\n    name: loki_data\n# ===========================================\n# Networks - External Network\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true\n</file>\n\n<file path=\"monitoring/loki.md\">\n```mermaid\ngraph LR\n    subgraph \"Applications\"\n        P[Producer]\n        C[Consumer]\n    end\n    \n    subgraph \"Log Collection\"\n        PR[Promtail<br/>Log Collector]\n    end\n    \n    subgraph \"Storage & Query\"\n        L[Loki<br/>Log Aggregation]\n        G[Grafana<br/>Visualization]\n    end\n    \n    P -->|stdout/stderr| PR\n    C -->|stdout/stderr| PR\n    PR -->|push logs| L\n    L -->|query| G\n```\n</file>\n\n<file path=\"monitoring/SLO.md\">\n```mermaid\ngraph TB\n    subgraph \"Definitions\"\n        SLI[SLI - Service Level Indicator<br/>📊 Metric đo lường]\n        SLO[SLO - Service Level Objective<br/>🎯 Mục tiêu cần đạt]\n        SLA[SLA - Service Level Agreement<br/>📝 Cam kết với khách hàng]\n    end\n    \n    SLI --> SLO --> SLA\n    \n    subgraph \"Example\"\n        E1[SLI: 99.5% requests < 500ms]\n        E2[SLO: 99.9% availability]\n        E3[SLA: Hoàn tiền nếu < 99.5%]\n    end\n```\n</file>\n\n<file path=\"monitoring/stack.md\">\n```mermaid\ngraph TB\n    subgraph \"Observability Stack\"\n        M[Metrics<br/>Prometheus]\n        L[Logs<br/>Loki]\n        A[Alerts<br/>Alertmanager]\n        V[Visualization<br/>Grafana]\n    end\n    \n    subgraph \"Data Pipeline\"\n        P[Producer]\n        K[Kafka]\n        C[Consumer]\n        DB[(PostgreSQL)]\n    end\n    \n    subgraph \"Infrastructure\"\n        CA[cAdvisor]\n        KE[Kafka Exporter]\n        PE[Postgres Exporter]\n    end\n    \n    P --> K --> C --> DB\n    P & C --> M\n    P & C --> L\n    CA & KE & PE --> M\n    M & L --> A\n    M & L & A --> V\n```\n</file>\n\n<file path=\"networks/docker-compose-network.yml\">\nnetworks:\n  monitoring-net:\n    name: monitoring-net\n    driver: bridge\n</file>\n\n<file path=\"monitoring-overview.md\">\n```mermaid\ngraph LR\n    subgraph \"Data Source\"\n        A[🐍 Python Producer<br/>Fake e-commerce data]\n    end\n    \n    subgraph \"Message Queue\"\n        B[📨 Apache Kafka<br/>+ Zookeeper]\n    end\n    \n    subgraph \"Processing & Storage\"\n        C[🐍 Python Consumer<br/>Transform data]\n        D[(🐘 PostgreSQL<br/>Data Warehouse)]\n    end\n    \n    subgraph \"Monitoring Stack\"\n        E[📊 Prometheus]\n        F[📈 Grafana]\n    end\n    \n    A -->|produce orders| B\n    B -->|consume| C\n    C -->|insert| D\n    \n    A -.->|metrics| E\n    B -.->|metrics| E\n    C -.->|metrics| E\n    D -.->|metrics| E\n    E -->|visualize| F\n```\n</file>\n\n<file path=\"repomix-output.md\">\nThis file is a merged representation of the entire codebase, combined into a single document by Repomix.\n\n# File Summary\n\n## Purpose\nThis file contains a packed representation of the entire repository's contents.\nIt is designed to be easily consumable by AI systems for analysis, code review,\nor other automated processes.\n\n## File Format\nThe content is organized as follows:\n1. This summary section\n2. Repository information\n3. Directory structure\n4. Repository files (if enabled)\n5. Multiple file entries, each consisting of:\n  a. A header with the file path (## File: path/to/file)\n  b. The full contents of the file in a code block\n\n## Usage Guidelines\n- This file should be treated as read-only. Any changes should be made to the\n  original repository files, not this packed version.\n- When processing this file, use the file path to distinguish\n  between different files in the repository.\n- Be aware that this file may contain sensitive information. Handle it with\n  the same level of security as you would the original repository.\n\n## Notes\n- Some files may have been excluded based on .gitignore rules and Repomix's configuration\n- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files\n- Files matching patterns in .gitignore are excluded\n- Files matching default ignore patterns are excluded\n- Files are sorted by Git change count (files with more changes are at the bottom)\n\n# Directory Structure\n```\napplications/\n  consumer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  producer/\n    config.py\n    Dockerfile\n    logger.py\n    main.py\n    requirements.txt\n    tracing.py\n  applications-flow.md\n  docker-compose.yml\ninfrastructure/\n  postgres/\n    init/\n      01-init.sql\n  docker-compose-application.yml\n  infrastructure.md\nmonitoring/\n  alertmanager/\n    alertmanager.yml\n  grafana/\n    dashboards/\n      applications/\n        data-pipeline.json\n        red-metrics.json\n        slo-dashboard.json\n      infrastructure/\n        container-metrics.json\n        kafka.json\n        postgresql.json\n      logs/\n        logs-explorer.json\n        test.json\n      overview/\n        correlation-dashboard.json\n        system-overview.json\n      tracing/\n        tracing-overview.json\n    provisioning/\n      dashboards/\n        dashboards.yml\n      datasources/\n        datasources.yml\n  loki/\n    rules/\n      fake/\n        alerts.yml\n      alerts.yml\n    loki-config.yml\n  prometheus/\n    rules/\n      alert_rules.yml\n      recording_rules.yml\n      slo_alerts.yml\n      slo_rules.yml\n    prometheus.yml\n  promtail/\n    promtail-config.yml\n  stress-testing/\n    load-test/\n      Dockerfile\n      load_test.py\n      requirements.txt\n    scripts/\n      chaos_test.sh\n      ramp_up_test.sh\n      run_load_test.sh\n      spike_test.sh\n    docker-compose-stress-test.yml\n  docker-compose-monitoring.yml\n  loki.md\n  SLO.md\n  stack.md\nnetworks/\n  docker-compose-network.yml\nmonitoring-overview.md\nstructure.md\n```\n\n# Files\n\n## File: applications/consumer/config.py\n````python\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    \"\"\"Consumer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    kafka_consumer_group: str = \"order-processor\"\n    \n    # PostgreSQL settings\n    postgres_host: str = \"postgres\"\n    postgres_port: int = 5432\n    postgres_db: str = \"datawarehouse\"\n    postgres_user: str = \"postgres\"\n    postgres_password: str = \"postgres123\"\n    \n    # Consumer settings\n    batch_size: int = 10\n    poll_timeout_ms: int = 1000\n    \n    # Metrics server\n    metrics_port: int = 8001\n    \n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-consumer\"\n    otel_enabled: bool = True\n    \n    # Application\n    app_name: str = \"data-consumer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n    \n    @property\n    def postgres_dsn(self) -> str:\n        return (\n            f\"host={self.postgres_host} \"\n            f\"port={self.postgres_port} \"\n            f\"dbname={self.postgres_db} \"\n            f\"user={self.postgres_user} \"\n            f\"password={self.postgres_password}\"\n        )\n\n\nsettings = Settings()\n````\n\n## File: applications/consumer/Dockerfile\n````\n# applications/consumer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8001\n\n# Run the consumer\nCMD [\"python\", \"-u\", \"main.py\"]\n````\n\n## File: applications/consumer/logger.py\n````python\n# applications/consumer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()\n````\n\n## File: applications/consumer/main.py\n````python\n# applications/consumer/main.py\n\nimport json\nimport time\nfrom datetime import datetime, timezone\n\nimport psycopg2\nfrom psycopg2.extras import execute_batch\nfrom kafka import KafkaConsumer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode, SpanKind\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_consumer_info',\n    'Consumer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'consumer_group': settings.kafka_consumer_group,\n    'pattern': 'RED'\n})\n\nMESSAGES_CONSUMED = Counter(\n    'pipeline_consumer_messages_total',\n    'Total number of messages consumed',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PROCESSED = Counter(\n    'pipeline_consumer_batches_total',\n    'Total number of batches processed',\n    ['status']\n)\n\nDB_OPERATIONS = Counter(\n    'pipeline_consumer_db_operations_total',\n    'Total database operations',\n    ['operation', 'status']\n)\n\nERRORS = Counter(\n    'pipeline_consumer_errors_total',\n    'Total number of errors by type and stage',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_message_duration_seconds',\n    'Time to process a single message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PROCESS_DURATION = Histogram(\n    'pipeline_consumer_batch_duration_seconds',\n    'Time to process a batch of messages',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nDB_QUERY_DURATION = Histogram(\n    'pipeline_consumer_db_query_duration_seconds',\n    'Database query duration',\n    ['operation'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5)\n)\n\nKAFKA_POLL_DURATION = Histogram(\n    'pipeline_consumer_kafka_poll_duration_seconds',\n    'Kafka poll duration',\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0)\n)\n\nEND_TO_END_LATENCY = Histogram(\n    'pipeline_consumer_end_to_end_latency_seconds',\n    'End-to-end latency from order creation to database insert',\n    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0)\n)\n\nORDERS_PROCESSED = Counter(\n    'pipeline_business_orders_processed_total',\n    'Total orders processed by category',\n    ['category']\n)\n\nREVENUE_PROCESSED = Counter(\n    'pipeline_business_revenue_processed_total',\n    'Total revenue processed',\n    ['category']\n)\n\nCONSUMER_UP = Gauge(\n    'pipeline_consumer_up',\n    'Consumer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_kafka_connected',\n    'Kafka connection status'\n)\n\nDB_CONNECTION_STATUS = Gauge(\n    'pipeline_consumer_db_connected',\n    'Database connection status'\n)\n\nCONSUMER_LAG = Gauge(\n    'pipeline_consumer_lag_messages',\n    'Consumer lag per partition',\n    ['topic', 'partition']\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_consumer_current_batch_size',\n    'Number of messages in current batch'\n)\n\nLAST_PROCESS_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_process_timestamp',\n    'Timestamp of last successful process'\n)\n\nLAST_COMMIT_TIMESTAMP = Gauge(\n    'pipeline_consumer_last_commit_timestamp',\n    'Timestamp of last offset commit'\n)\n\n\n# ===========================================\n# Database Handler\n# ===========================================\n\nclass DatabaseHandler:\n    def __init__(self):\n        self.conn = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to PostgreSQL with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.conn = psycopg2.connect(settings.postgres_dsn)\n                self.conn.autocommit = False\n                logger.info(\n                    \"Connected to PostgreSQL\",\n                    extra={\n                        \"event\": \"db_connected\",\n                        \"host\": settings.postgres_host,\n                        \"database\": settings.postgres_db,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                DB_CONNECTION_STATUS.set(1)\n                return\n            except psycopg2.Error as e:\n                DB_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"db_init\").inc()\n                logger.warning(\n                    \"PostgreSQL connection failed, retrying...\",\n                    extra={\n                        \"event\": \"db_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to PostgreSQL after maximum retries\")\n    \n    def insert_orders(self, orders: list[dict], parent_span=None) -> int:\n        \"\"\"Insert batch of orders into database with tracing\"\"\"\n        if not orders:\n            return 0\n        \n        with tracer.start_as_current_span(\n            \"db_insert_orders\",\n            kind=SpanKind.CLIENT\n        ) as span:\n            span.set_attribute(\"db.system\", \"postgresql\")\n            span.set_attribute(\"db.name\", settings.postgres_db)\n            span.set_attribute(\"db.operation\", \"INSERT\")\n            span.set_attribute(\"db.batch_size\", len(orders))\n            \n            insert_sql = \"\"\"\n                INSERT INTO ecommerce.orders (\n                    order_id, customer_id, product_id, product_name, \n                    category, quantity, unit_price, total_amount,\n                    order_status, created_at, processed_at\n                ) VALUES (\n                    %(order_id)s, %(customer_id)s, %(product_id)s, %(product_name)s,\n                    %(category)s, %(quantity)s, %(unit_price)s, %(total_amount)s,\n                    %(order_status)s, %(created_at)s, %(processed_at)s\n                )\n                ON CONFLICT (order_id) DO NOTHING\n            \"\"\"\n            \n            try:\n                start_time = time.time()\n                \n                with self.conn.cursor() as cur:\n                    process_time = datetime.now(timezone.utc)\n                    \n                    for order in orders:\n                        order['processed_at'] = process_time.isoformat()\n                        if isinstance(order.get('created_at'), str):\n                            order['created_at'] = datetime.fromisoformat(\n                                order['created_at'].replace('Z', '+00:00')\n                            )\n                    \n                    execute_batch(cur, insert_sql, orders, page_size=100)\n                    inserted = cur.rowcount\n                    self.conn.commit()\n                \n                duration = time.time() - start_time\n                \n                span.set_attribute(\"db.rows_affected\", inserted)\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                span.set_status(Status(StatusCode.OK))\n                \n                DB_QUERY_DURATION.labels(operation=\"insert_batch\").observe(duration)\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"success\").inc(len(orders))\n                \n                logger.debug(\n                    \"Orders inserted into database\",\n                    extra={\n                        \"event\": \"db_insert_success\",\n                        \"count\": inserted,\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return inserted\n                \n            except psycopg2.Error as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                ERRORS.labels(error_type=type(e).__name__, stage=\"db_insert\").inc()\n                DB_OPERATIONS.labels(operation=\"insert\", status=\"failed\").inc(len(orders))\n                \n                logger.error(\n                    \"Database insert failed\",\n                    extra={\n                        \"event\": \"db_insert_error\",\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__,\n                        \"batch_size\": len(orders)\n                    }\n                )\n                self.conn.rollback()\n                \n                try:\n                    self._connect()\n                except Exception:\n                    DB_CONNECTION_STATUS.set(0)\n                \n                return 0\n    \n    def close(self):\n        \"\"\"Close database connection\"\"\"\n        if self.conn:\n            self.conn.close()\n            DB_CONNECTION_STATUS.set(0)\n            logger.info(\"Database connection closed\", extra={\"event\": \"db_closed\"})\n\n\n# ===========================================\n# Kafka Consumer\n# ===========================================\n\nclass OrderConsumer:\n    def __init__(self, db_handler: DatabaseHandler):\n        self.consumer = None\n        self.db = db_handler\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.consumer = KafkaConsumer(\n                    settings.kafka_topic,\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    group_id=settings.kafka_consumer_group,\n                    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n                    auto_offset_reset='earliest',\n                    enable_auto_commit=False,\n                    max_poll_records=settings.batch_size,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"topic\": settings.kafka_topic,\n                        \"consumer_group\": settings.kafka_consumer_group,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                CONSUMER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"kafka_init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def process_messages(self):\n        \"\"\"Main processing loop with tracing\"\"\"\n        batch: list[dict] = []\n        \n        logger.info(\"Starting message processing loop\", extra={\"event\": \"processing_started\"})\n        \n        try:\n            while True:\n                # Poll for messages\n                with tracer.start_as_current_span(\n                    \"kafka_poll\",\n                    kind=SpanKind.CONSUMER\n                ) as poll_span:\n                    poll_start = time.time()\n                    records = self.consumer.poll(\n                        timeout_ms=settings.poll_timeout_ms,\n                        max_records=settings.batch_size\n                    )\n                    poll_duration = time.time() - poll_start\n                    \n                    poll_span.set_attribute(\"messaging.system\", \"kafka\")\n                    poll_span.set_attribute(\"messaging.operation\", \"poll\")\n                    poll_span.set_attribute(\"duration_ms\", round(poll_duration * 1000, 2))\n                    \n                    KAFKA_POLL_DURATION.observe(poll_duration)\n                \n                if not records:\n                    if batch:\n                        self._process_batch(batch)\n                        batch = []\n                    continue\n                \n                # Process received messages\n                for topic_partition, messages in records.items():\n                    for message in messages:\n                        try:\n                            msg_start = time.time()\n                            order = message.value\n                            category = order.get(\"category\", \"unknown\")\n                            \n                            # Create span for consuming\n                            with tracer.start_as_current_span(\n                                \"consume_message\",\n                                kind=SpanKind.CONSUMER\n                            ) as msg_span:\n                                # Add span attributes\n                                msg_span.set_attribute(\"messaging.system\", \"kafka\")\n                                msg_span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n                                msg_span.set_attribute(\"messaging.kafka.partition\", topic_partition.partition)\n                                msg_span.set_attribute(\"messaging.kafka.offset\", message.offset)\n                                msg_span.set_attribute(\"order.id\", order.get(\"order_id\", \"\"))\n                                msg_span.set_attribute(\"order.category\", category)\n                                \n                                # Link to producer trace if available\n                                if \"trace_id\" in order:\n                                    msg_span.set_attribute(\"producer.trace_id\", order[\"trace_id\"])\n                                \n                                batch.append(order)\n                                \n                                MESSAGES_CONSUMED.labels(\n                                    topic=settings.kafka_topic,\n                                    status=\"success\",\n                                    category=category\n                                ).inc()\n                                \n                                msg_duration = time.time() - msg_start\n                                MESSAGE_PROCESS_DURATION.observe(msg_duration)\n                                \n                                # Calculate end-to-end latency\n                                if 'created_at' in order:\n                                    try:\n                                        created_at = datetime.fromisoformat(\n                                            order['created_at'].replace('Z', '+00:00')\n                                        )\n                                        e2e_latency = (datetime.now(timezone.utc) - created_at).total_seconds()\n                                        END_TO_END_LATENCY.observe(e2e_latency)\n                                        msg_span.set_attribute(\"e2e_latency_seconds\", e2e_latency)\n                                    except Exception:\n                                        pass\n                                \n                                CONSUMER_LAG.labels(\n                                    topic=topic_partition.topic,\n                                    partition=str(topic_partition.partition)\n                                ).set(message.offset)\n                                \n                                msg_span.set_status(Status(StatusCode.OK))\n                                \n                                logger.info(\n                                    \"Message consumed\",\n                                    extra={\n                                        \"event\": \"message_consumed\",\n                                        \"order_id\": order.get(\"order_id\"),\n                                        \"trace_id\": order.get(\"trace_id\", \"\"),\n                                        \"category\": category,\n                                        \"partition\": topic_partition.partition,\n                                        \"offset\": message.offset\n                                    }\n                                )\n                                \n                        except Exception as e:\n                            MESSAGES_CONSUMED.labels(\n                                topic=settings.kafka_topic,\n                                status=\"failed\",\n                                category=\"unknown\"\n                            ).inc()\n                            \n                            ERRORS.labels(\n                                error_type=type(e).__name__,\n                                stage=\"message_parse\"\n                            ).inc()\n                            \n                            logger.error(\n                                \"Failed to process message\",\n                                extra={\n                                    \"event\": \"message_parse_error\",\n                                    \"error\": str(e),\n                                    \"error_type\": type(e).__name__,\n                                    \"partition\": topic_partition.partition,\n                                    \"offset\": message.offset\n                                }\n                            )\n\n                \n                CURRENT_BATCH_SIZE.set(len(batch))\n                \n                if len(batch) >= settings.batch_size:\n                    self._process_batch(batch)\n                    batch = []\n                    \n        except KeyboardInterrupt:\n            logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n            if batch:\n                self._process_batch(batch)\n    \n    def _process_batch(self, batch: list[dict]):\n        \"\"\"Process and commit a batch of messages with tracing\"\"\"\n        if not batch:\n            return\n        \n        with tracer.start_as_current_span(\n            \"process_batch\",\n            kind=SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", len(batch))\n            \n            start_time = time.time()\n            \n            # Insert to database\n            inserted = self.db.insert_orders(batch)\n            \n            # Commit Kafka offsets\n            self.consumer.commit()\n            LAST_COMMIT_TIMESTAMP.set(time.time())\n            \n            duration = time.time() - start_time\n            \n            batch_span.set_attribute(\"batch.inserted\", inserted)\n            batch_span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n            \n            BATCH_PROCESS_DURATION.observe(duration)\n            \n            if inserted == len(batch):\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"success\").inc()\n            elif inserted == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"No records inserted\"))\n                BATCHES_PROCESSED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PROCESSED.labels(status=\"partial\").inc()\n            \n            # Business metrics\n            for order in batch:\n                category = order.get(\"category\", \"unknown\")\n                ORDERS_PROCESSED.labels(category=category).inc()\n                REVENUE_PROCESSED.labels(category=category).inc(order.get(\"total_amount\", 0))\n            \n            LAST_PROCESS_TIMESTAMP.set(time.time())\n            CURRENT_BATCH_SIZE.set(0)\n            \n            logger.info(\n                \"Batch processed\",\n                extra={\n                    \"event\": \"batch_processed\",\n                    \"batch_size\": len(batch),\n                    \"inserted\": inserted,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(len(batch) / duration, 2) if duration > 0 else 0\n                }\n            )\n    \n    def close(self):\n        \"\"\"Close the consumer\"\"\"\n        if self.consumer:\n            self.consumer.close()\n            CONSUMER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Consumer closed\", extra={\"event\": \"consumer_closed\"})\n\n\n# ===========================================\n# Main\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Consumer\",\n        extra={\n            \"event\": \"consumer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"consumer_group\": settings.kafka_consumer_group,\n                \"postgres_host\": settings.postgres_host,\n                \"postgres_db\": settings.postgres_db,\n                \"batch_size\": settings.batch_size,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create handlers\n    db_handler = DatabaseHandler()\n    consumer = OrderConsumer(db_handler)\n    \n    try:\n        consumer.process_messages()\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in consumer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        consumer.close()\n        db_handler.close()\n        logger.info(\"Consumer stopped\", extra={\"event\": \"consumer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()\n````\n\n## File: applications/consumer/requirements.txt\n````\n# Kafka\nkafka-python-ng==2.2.2\n\n# PostgreSQL\npsycopg2-binary==2.9.9\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0\n````\n\n## File: applications/consumer/tracing.py\n````python\n# applications/consumer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()\n````\n\n## File: applications/producer/config.py\n````python\n# applications/producer/config.py\n\nfrom pydantic_settings import BaseSettings\nfrom typing import Optional\n\n\nclass Settings(BaseSettings):\n    \"\"\"Producer configuration from environment variables\"\"\"\n    \n    # Kafka settings\n    kafka_bootstrap_servers: str = \"kafka:29092\"\n    kafka_topic: str = \"ecommerce.orders\"\n    \n    # Producer settings\n    produce_interval_seconds: float = 1.0  # Produce every N seconds\n    batch_size: int = 5  # Messages per batch\n    \n    # Metrics server\n    metrics_port: int = 8000\n\n    # Tracing\n    otel_exporter_otlp_endpoint: str = \"http://jaeger:4317\"\n    otel_service_name: str = \"data-producer\"\n    otel_enabled: bool = True\n\n    # Application\n    app_name: str = \"data-producer\"\n    log_level: str = \"INFO\"\n    \n    class Config:\n        env_prefix = \"\"\n        case_sensitive = False\n\n\nsettings = Settings()\n````\n\n## File: applications/producer/Dockerfile\n````\n# applications/producer/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose metrics port\nEXPOSE 8000\n\n# Run the producer\nCMD [\"python\", \"-u\", \"main.py\"]\n````\n\n## File: applications/producer/logger.py\n````python\n# applications/producer/logger.py\n\nimport logging\nimport sys\nfrom datetime import datetime, timezone\nfrom pythonjsonlogger import jsonlogger\n\nfrom config import settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields\"\"\"\n    \n    def add_fields(self, log_record, record, message_dict):\n        super().add_fields(log_record, record, message_dict)\n        \n        # Timestamp in ISO format\n        log_record['timestamp'] = datetime.now(timezone.utc).isoformat()\n        \n        # Service identification\n        log_record['service'] = settings.app_name\n        \n        # Log level - lấy từ record thay vì log_record\n        log_record['level'] = record.levelname\n        \n        # Source location\n        log_record['logger'] = record.name\n        log_record['module'] = record.module\n        log_record['function'] = record.funcName\n        log_record['line'] = record.lineno\n\n\ndef setup_logger(name: str = None) -> logging.Logger:\n    \"\"\"Setup structured JSON logger\"\"\"\n    \n    logger = logging.getLogger(name or settings.app_name)\n    logger.setLevel(getattr(logging, settings.log_level))\n    \n    # Prevent duplicate handlers\n    if logger.handlers:\n        return logger\n    \n    # Console handler with JSON format\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(getattr(logging, settings.log_level))\n    \n    # JSON formatter - không dùng rename_fields\n    formatter = CustomJsonFormatter(\n        fmt='%(timestamp)s %(level)s %(service)s %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    return logger\n\n\n# Create default logger instance\nlogger = setup_logger()\n````\n\n## File: applications/producer/main.py\n````python\n# applications/producer/main.py\n\nimport json\nimport random\nimport time\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import (\n    Counter,\n    Gauge,\n    Histogram,\n    Info,\n    start_http_server,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\nfrom config import settings\nfrom logger import setup_logger\nfrom tracing import tracer\n\n# ===========================================\n# Logging Setup\n# ===========================================\nlogger = setup_logger()\n\n# ===========================================\n# Prometheus Metrics\n# ===========================================\n\nAPP_INFO = Info(\n    'pipeline_producer_info',\n    'Producer application information'\n)\nAPP_INFO.info({\n    'version': '2.0.0',\n    'kafka_topic': settings.kafka_topic,\n    'pattern': 'RED'\n})\n\nMESSAGES_PRODUCED = Counter(\n    'pipeline_producer_messages_total',\n    'Total number of messages produced',\n    ['topic', 'status', 'category']\n)\n\nBATCHES_PRODUCED = Counter(\n    'pipeline_producer_batches_total',\n    'Total number of batches produced',\n    ['status']\n)\n\nERRORS = Counter(\n    'pipeline_producer_errors_total',\n    'Total number of errors by type',\n    ['error_type', 'stage']\n)\n\nMESSAGE_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_message_duration_seconds',\n    'Time to produce a single message to Kafka',\n    ['topic'],\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1.0)\n)\n\nBATCH_PRODUCE_DURATION = Histogram(\n    'pipeline_producer_batch_duration_seconds',\n    'Time to produce a batch of messages',\n    ['topic'],\n    buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)\n)\n\nMESSAGE_SIZE = Histogram(\n    'pipeline_producer_message_size_bytes',\n    'Size of produced messages in bytes',\n    ['topic'],\n    buckets=(100, 500, 1000, 2500, 5000, 10000, 25000)\n)\n\nORDERS_BY_CATEGORY = Counter(\n    'pipeline_business_orders_total',\n    'Total orders by category',\n    ['category']\n)\n\nREVENUE = Counter(\n    'pipeline_business_revenue_total',\n    'Total revenue generated',\n    ['category']\n)\n\nORDER_VALUE = Histogram(\n    'pipeline_business_order_value',\n    'Distribution of order values',\n    ['category'],\n    buckets=(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\n)\n\nITEMS_PER_ORDER = Histogram(\n    'pipeline_business_items_per_order',\n    'Distribution of items per order',\n    buckets=(1, 2, 3, 4, 5, 10)\n)\n\nPRODUCER_UP = Gauge(\n    'pipeline_producer_up',\n    'Producer is running (1) or not (0)'\n)\n\nKAFKA_CONNECTION_STATUS = Gauge(\n    'pipeline_producer_kafka_connected',\n    'Kafka connection status (1=connected, 0=disconnected)'\n)\n\nCURRENT_BATCH_SIZE = Gauge(\n    'pipeline_producer_current_batch_size',\n    'Current configured batch size'\n)\n\nLAST_PRODUCE_TIMESTAMP = Gauge(\n    'pipeline_producer_last_produce_timestamp',\n    'Timestamp of last successful produce'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\n\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order(trace_id: str = None) -> dict[str, Any]:\n    \"\"\"Generate a fake e-commerce order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    order = {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n    }\n    \n    # Add trace_id for correlation\n    if trace_id:\n        order[\"trace_id\"] = trace_id\n    \n    return order\n\n\n# ===========================================\n# Kafka Producer\n# ===========================================\n\nclass OrderProducer:\n    def __init__(self):\n        self.producer = None\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka with retry logic\"\"\"\n        max_retries = 30\n        retry_interval = 2\n        \n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=settings.kafka_bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks='all',\n                    retries=3,\n                    max_in_flight_requests_per_connection=1,\n                )\n                logger.info(\n                    \"Connected to Kafka\",\n                    extra={\n                        \"event\": \"kafka_connected\",\n                        \"bootstrap_servers\": settings.kafka_bootstrap_servers,\n                        \"attempt\": attempt + 1\n                    }\n                )\n                PRODUCER_UP.set(1)\n                KAFKA_CONNECTION_STATUS.set(1)\n                return\n            except KafkaError as e:\n                KAFKA_CONNECTION_STATUS.set(0)\n                ERRORS.labels(error_type=\"connection\", stage=\"init\").inc()\n                logger.warning(\n                    \"Kafka connection failed, retrying...\",\n                    extra={\n                        \"event\": \"kafka_connection_retry\",\n                        \"attempt\": attempt + 1,\n                        \"max_retries\": max_retries,\n                        \"error\": str(e)\n                    }\n                )\n                time.sleep(retry_interval)\n        \n        raise Exception(\"Failed to connect to Kafka after maximum retries\")\n    \n    def produce(self, order: dict) -> bool:\n        \"\"\"Produce a single order to Kafka with tracing\"\"\"\n        category = order.get(\"category\", \"unknown\")\n        \n        # Create span for this operation\n        with tracer.start_as_current_span(\n            \"produce_order\",\n            kind=trace.SpanKind.PRODUCER\n        ) as span:\n            # Add span attributes\n            span.set_attribute(\"messaging.system\", \"kafka\")\n            span.set_attribute(\"messaging.destination\", settings.kafka_topic)\n            span.set_attribute(\"order.id\", order[\"order_id\"])\n            span.set_attribute(\"order.category\", category)\n            span.set_attribute(\"order.amount\", order[\"total_amount\"])\n            \n            # Get trace_id and add to order for correlation\n            trace_id = format(span.get_span_context().trace_id, '032x')\n            order[\"trace_id\"] = trace_id\n            \n            message_bytes = json.dumps(order).encode('utf-8')\n            span.set_attribute(\"messaging.message.payload_size_bytes\", len(message_bytes))\n            \n            try:\n                start_time = time.time()\n                \n                future = self.producer.send(\n                    settings.kafka_topic,\n                    key=order[\"order_id\"],\n                    value=order\n                )\n                future.get(timeout=10)\n                \n                duration = time.time() - start_time\n                \n                # Record success in span\n                span.set_status(Status(StatusCode.OK))\n                span.set_attribute(\"duration_ms\", round(duration * 1000, 2))\n                \n                # Metrics\n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"success\",\n                    category=category\n                ).inc()\n                \n                MESSAGE_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n                MESSAGE_SIZE.labels(topic=settings.kafka_topic).observe(len(message_bytes))\n                \n                ORDERS_BY_CATEGORY.labels(category=category).inc()\n                REVENUE.labels(category=category).inc(order[\"total_amount\"])\n                ORDER_VALUE.labels(category=category).observe(order[\"total_amount\"])\n                ITEMS_PER_ORDER.observe(order[\"quantity\"])\n                \n                LAST_PRODUCE_TIMESTAMP.set(time.time())\n                \n                logger.debug(\n                    \"Order produced successfully\",\n                    extra={\n                        \"event\": \"order_produced\",\n                        \"order_id\": order[\"order_id\"],\n                        \"trace_id\": trace_id,\n                        \"category\": category,\n                        \"amount\": order[\"total_amount\"],\n                        \"duration_ms\": round(duration * 1000, 2)\n                    }\n                )\n                \n                return True\n                \n            except Exception as e:\n                # Record error in span\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                \n                MESSAGES_PRODUCED.labels(\n                    topic=settings.kafka_topic,\n                    status=\"failed\",\n                    category=category\n                ).inc()\n                \n                ERRORS.labels(\n                    error_type=type(e).__name__,\n                    stage=\"produce\"\n                ).inc()\n                \n                logger.error(\n                    \"Failed to produce order\",\n                    extra={\n                        \"event\": \"produce_error\",\n                        \"order_id\": order.get(\"order_id\"),\n                        \"trace_id\": trace_id,\n                        \"error\": str(e),\n                        \"error_type\": type(e).__name__\n                    }\n                )\n                return False\n    \n    def produce_batch(self, batch_size: int) -> tuple[int, int]:\n        \"\"\"Produce a batch of orders with tracing\"\"\"\n        \n        # Create parent span for batch\n        with tracer.start_as_current_span(\n            \"produce_batch\",\n            kind=trace.SpanKind.INTERNAL\n        ) as batch_span:\n            batch_span.set_attribute(\"batch.size\", batch_size)\n            \n            success_count = 0\n            failed_count = 0\n            batch_orders = []\n            \n            CURRENT_BATCH_SIZE.set(batch_size)\n            \n            start_time = time.time()\n            \n            for _ in range(batch_size):\n                order = generate_order()\n                if self.produce(order):\n                    success_count += 1\n                    batch_orders.append(order[\"order_id\"])\n                else:\n                    failed_count += 1\n            \n            self.producer.flush()\n            \n            duration = time.time() - start_time\n            \n            # Record batch results in span\n            batch_span.set_attribute(\"batch.success_count\", success_count)\n            batch_span.set_attribute(\"batch.failed_count\", failed_count)\n            batch_span.set_attribute(\"batch.duration_ms\", round(duration * 1000, 2))\n            \n            if failed_count == 0:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"success\").inc()\n            elif success_count == 0:\n                batch_span.set_status(Status(StatusCode.ERROR, \"All messages failed\"))\n                BATCHES_PRODUCED.labels(status=\"failed\").inc()\n            else:\n                batch_span.set_status(Status(StatusCode.OK))\n                BATCHES_PRODUCED.labels(status=\"partial\").inc()\n            \n            BATCH_PRODUCE_DURATION.labels(topic=settings.kafka_topic).observe(duration)\n            \n            logger.info(\n                \"Batch produced\",\n                extra={\n                    \"event\": \"batch_produced\",\n                    \"batch_size\": batch_size,\n                    \"success_count\": success_count,\n                    \"failed_count\": failed_count,\n                    \"duration_ms\": round(duration * 1000, 2),\n                    \"throughput_per_sec\": round(batch_size / duration, 2) if duration > 0 else 0\n                }\n            )\n            \n            return success_count, failed_count\n    \n    def close(self):\n        \"\"\"Close the producer\"\"\"\n        if self.producer:\n            self.producer.close()\n            PRODUCER_UP.set(0)\n            KAFKA_CONNECTION_STATUS.set(0)\n            logger.info(\"Producer closed\", extra={\"event\": \"producer_closed\"})\n\n\n# ===========================================\n# Main Loop\n# ===========================================\n\ndef main():\n    logger.info(\n        \"Starting Data Producer\",\n        extra={\n            \"event\": \"producer_starting\",\n            \"config\": {\n                \"kafka_servers\": settings.kafka_bootstrap_servers,\n                \"topic\": settings.kafka_topic,\n                \"batch_size\": settings.batch_size,\n                \"interval_seconds\": settings.produce_interval_seconds,\n                \"metrics_port\": settings.metrics_port,\n                \"tracing_enabled\": settings.otel_enabled,\n                \"tracing_endpoint\": settings.otel_exporter_otlp_endpoint\n            }\n        }\n    )\n    \n    # Start Prometheus metrics server\n    start_http_server(settings.metrics_port)\n    logger.info(\n        \"Metrics server started\",\n        extra={\"event\": \"metrics_server_started\", \"port\": settings.metrics_port}\n    )\n    \n    # Create producer\n    producer = OrderProducer()\n    \n    try:\n        batch_number = 0\n        while True:\n            batch_number += 1\n            \n            # Create span for each iteration\n            with tracer.start_as_current_span(f\"batch_iteration_{batch_number}\"):\n                success, failed = producer.produce_batch(settings.batch_size)\n            \n            time.sleep(settings.produce_interval_seconds)\n            \n    except KeyboardInterrupt:\n        logger.info(\"Shutdown requested\", extra={\"event\": \"shutdown_requested\"})\n    except Exception as e:\n        ERRORS.labels(error_type=\"fatal\", stage=\"main\").inc()\n        logger.error(\n            \"Fatal error in producer\",\n            extra={\"event\": \"fatal_error\", \"error\": str(e), \"error_type\": type(e).__name__}\n        )\n    finally:\n        producer.close()\n        logger.info(\"Producer stopped\", extra={\"event\": \"producer_stopped\"})\n\n\nif __name__ == \"__main__\":\n    main()\n````\n\n## File: applications/producer/requirements.txt\n````\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data generation\nfaker==24.4.0\n\n# Prometheus metrics\nprometheus-client==0.20.0\n\n# Logging\npython-json-logger==2.0.7\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n\n# OpenTelemetry - Tracing\nopentelemetry-api==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-exporter-otlp==1.24.0\nopentelemetry-instrumentation==0.45b0\n````\n\n## File: applications/producer/tracing.py\n````python\n# applications/producer/tracing.py\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nfrom config import settings\n\n\ndef setup_tracing() -> trace.Tracer:\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    \n    if not settings.otel_enabled:\n        return trace.get_tracer(settings.otel_service_name)\n    \n    # Create resource with service info\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: settings.otel_service_name,\n        ResourceAttributes.SERVICE_VERSION: \"2.0.0\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\",\n    })\n    \n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n    \n    # Create OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=settings.otel_exporter_otlp_endpoint,\n        insecure=True\n    )\n    \n    # Add span processor\n    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set global tracer provider\n    trace.set_tracer_provider(provider)\n    \n    return trace.get_tracer(settings.otel_service_name)\n\n\n# Create global tracer\ntracer = setup_tracing()\n````\n\n## File: applications/applications-flow.md\n````markdown\n```mermaid\ngraph LR\n    subgraph \"applications/docker-compose.yml\"\n        subgraph \"Producer\"\n            P[Data Producer<br/>:8000/metrics]\n            F[Faker Library]\n        end\n        \n        subgraph \"Consumer\"\n            C[Data Consumer<br/>:8001/metrics]\n        end\n    end\n    \n    subgraph \"infrastructure/\"\n        K[Kafka]\n        PG[(PostgreSQL)]\n    end\n    \n    subgraph \"monitoring/\"\n        PR[Prometheus]\n    end\n    \n    F --> P\n    P -->|produce orders| K\n    K -->|consume| C\n    C -->|insert| PG\n    \n    P -.->|metrics| PR\n    C -.->|metrics| PR\n```\n````\n\n## File: applications/docker-compose.yml\n````yaml\n# applications/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Data Producer - Generate fake orders\n  # ===========================================\n  data-producer:\n    build:\n      context: ./producer\n      dockerfile: Dockerfile\n    container_name: data-producer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      \n      # Producer settings\n      PRODUCE_INTERVAL_SECONDS: \"1.0\"\n      BATCH_SIZE: \"5\"\n      \n      # Metrics\n      METRICS_PORT: \"8000\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8000:8000\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8000/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Data Consumer - Process orders to PostgreSQL\n  # ===========================================\n  data-consumer:\n    build:\n      context: ./consumer\n      dockerfile: Dockerfile\n    container_name: data-consumer\n    restart: unless-stopped\n    environment:\n      # Kafka settings\n      KAFKA_BOOTSTRAP_SERVERS: kafka:29092\n      KAFKA_TOPIC: ecommerce.orders\n      KAFKA_CONSUMER_GROUP: order-processor\n      \n      # PostgreSQL settings\n      POSTGRES_HOST: postgres\n      POSTGRES_PORT: \"5432\"\n      POSTGRES_DB: datawarehouse\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres123\n      \n      # Consumer settings\n      BATCH_SIZE: \"10\"\n      POLL_TIMEOUT_MS: \"1000\"\n      \n      # Metrics\n      METRICS_PORT: \"8001\"\n      \n      # Logging\n      LOG_LEVEL: INFO\n    ports:\n      - \"8001:8001\"\n    networks:\n      - monitoring-net\n    # depends_on:\n    #   kafka:\n    #     condition: service_healthy\n    #   postgres:\n    #     condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8001/metrics\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Networks\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true\n````\n\n## File: infrastructure/postgres/init/01-init.sql\n````sql\n-- infrastructure/postgres/init/01-init.sql\n\n-- ===========================================\n-- Database cho Data Pipeline\n-- ===========================================\n\n-- Tạo schema cho e-commerce data\nCREATE SCHEMA IF NOT EXISTS ecommerce;\n\n-- Bảng orders - nơi lưu data từ Kafka consumer\nCREATE TABLE ecommerce.orders (\n    id SERIAL PRIMARY KEY,\n    order_id VARCHAR(50) UNIQUE NOT NULL,\n    customer_id VARCHAR(50) NOT NULL,\n    product_id VARCHAR(50) NOT NULL,\n    product_name VARCHAR(255) NOT NULL,\n    category VARCHAR(100),\n    quantity INTEGER NOT NULL,\n    unit_price DECIMAL(10, 2) NOT NULL,\n    total_amount DECIMAL(10, 2) NOT NULL,\n    order_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    processed_at TIMESTAMP,\n    \n    -- Indexes cho queries thường dùng\n    CONSTRAINT chk_quantity CHECK (quantity > 0),\n    CONSTRAINT chk_price CHECK (unit_price > 0)\n);\n\nCREATE INDEX idx_orders_customer ON ecommerce.orders(customer_id);\nCREATE INDEX idx_orders_created_at ON ecommerce.orders(created_at);\nCREATE INDEX idx_orders_status ON ecommerce.orders(order_status);\nCREATE INDEX idx_orders_category ON ecommerce.orders(category);\n\n-- Bảng để track processing metrics\nCREATE TABLE ecommerce.processing_logs (\n    id SERIAL PRIMARY KEY,\n    batch_id VARCHAR(50) NOT NULL,\n    records_processed INTEGER NOT NULL,\n    records_failed INTEGER DEFAULT 0,\n    processing_time_ms INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- View cho monitoring\nCREATE VIEW ecommerce.orders_summary AS\nSELECT \n    DATE(created_at) as order_date,\n    COUNT(*) as total_orders,\n    SUM(total_amount) as total_revenue,\n    AVG(total_amount) as avg_order_value,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM ecommerce.orders\nGROUP BY DATE(created_at)\nORDER BY order_date DESC;\n\n-- Grant permissions\nGRANT ALL PRIVILEGES ON SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA ecommerce TO postgres;\nGRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA ecommerce TO postgres;\n\n-- Log initialization\nDO $$\nBEGIN\n    RAISE NOTICE 'Database initialization completed successfully!';\nEND $$;\n````\n\n## File: infrastructure/infrastructure.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"infrastructure/docker-compose.yml\"\n        subgraph \"Message Queue\"\n            ZK[Zookeeper:2181]\n            K[Kafka:9092]\n            KE[Kafka Exporter:9308]\n        end\n        \n        subgraph \"Database\"\n            PG[(PostgreSQL:5432)]\n            PE[Postgres Exporter:9187]\n        end\n        \n        ZK --> K\n        K -.->|metrics| KE\n        PG -.->|metrics| PE\n    end\n    \n    subgraph \"monitoring/\"\n        P[Prometheus]\n    end\n    \n    KE -->|scrape :9308| P\n    PE -->|scrape :9187| P\n```\n````\n\n## File: monitoring/alertmanager/alertmanager.yml\n````yaml\n# monitoring/alertmanager/alertmanager.yml\n\nglobal:\n  # Thời gian chờ trước khi gửi lại alert nếu vẫn còn firing\n  resolve_timeout: 5m\n\n# Route tree - định tuyến alerts\nroute:\n  # Default receiver\n  receiver: 'default-receiver'\n  \n  # Group alerts by these labels\n  group_by: ['alertname', 'severity', 'service']\n  \n  # Thời gian chờ trước khi gửi group đầu tiên\n  group_wait: 30s\n  \n  # Thời gian chờ trước khi gửi alerts mới trong cùng group\n  group_interval: 5m\n  \n  # Thời gian chờ trước khi gửi lại alert đã gửi\n  repeat_interval: 4h\n\n  # Child routes - routing dựa trên labels\n  routes:\n    # Critical alerts - gửi ngay\n    - match:\n        severity: critical\n      receiver: 'critical-receiver'\n      group_wait: 10s\n      repeat_interval: 1h\n\n    # Warning alerts\n    - match:\n        severity: warning\n      receiver: 'warning-receiver'\n      repeat_interval: 4h\n\n    # Info alerts - ít urgent hơn\n    - match:\n        severity: info\n      receiver: 'default-receiver'\n      repeat_interval: 12h\n\n# Inhibition rules - suppress alerts\ninhibit_rules:\n  # Nếu critical đang fire, suppress warning cùng service\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'service']\n\n# Receivers - nơi nhận alerts\nreceivers:\n  - name: 'default-receiver'\n    # Webhook để test (có thể xem trong Alertmanager UI)\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n\n  - name: 'critical-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n    # Có thể thêm Slack, Email, PagerDuty ở đây\n    # slack_configs:\n    #   - api_url: 'https://hooks.slack.com/services/xxx/yyy/zzz'\n    #     channel: '#alerts-critical'\n    #     send_resolved: true\n\n  - name: 'warning-receiver'\n    webhook_configs:\n      - url: 'http://localhost:5001/webhook'\n        send_resolved: true\n````\n\n## File: monitoring/grafana/dashboards/applications/data-pipeline.json\n````json\n{\n  \"uid\": \"data-pipeline-dashboard\",\n  \"title\": \"Data Pipeline Overview\",\n  \"description\": \"Monitoring data pipeline: Producer → Kafka → Consumer → PostgreSQL\",\n  \"tags\": [\"applications\", \"data-pipeline\", \"producer\", \"consumer\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🚀 Data Pipeline Flow\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## Pipeline: **Producer** → **Kafka** → **Consumer** → **PostgreSQL**\\n\\n*Metrics sẽ hiển thị khi Producer và Consumer được khởi động.*\"\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-producer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 6, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up{job=\\\"data-consumer\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"noValue\": \"WAITING\",\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"gray\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Messages Produced (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_produced_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Messages Consumed (Total)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_messages_consumed_total\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Production Rate (msg/s)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_messages_produced_total[1m])\",\n          \"legendFormat\": \"Produced\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_messages_consumed_total[1m])\",\n          \"legendFormat\": \"Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Produced\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Processing Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 7 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_processing_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Errors\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-producer\\\"}[1m])\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pipeline_errors_total{job=\\\"data-consumer\\\"}[1m])\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"errors/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 0.1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Database Inserts (from Consumer)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pipeline_db_inserts_total[1m])\",\n          \"legendFormat\": \"Inserts/s\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"single\", \"sort\": \"none\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"gradientMode\": \"hue\", \"lineWidth\": 1 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Orders by Category (Real-time)\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_orders_by_category)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Revenue Generated\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pipeline_total_revenue\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"noValue\": \"$0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Pipeline Health Score\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 21 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(1 - (rate(pipeline_errors_total[5m]) / (rate(pipeline_messages_produced_total[5m]) + 0.001))) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"noValue\": \"N/A\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 90 }, { \"color\": \"green\", \"value\": 99 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/applications/red-metrics.json\n````json\n{\n  \"uid\": \"red-metrics-dashboard\",\n  \"title\": \"RED Metrics - Data Pipeline\",\n  \"description\": \"Rate, Errors, Duration metrics for the data pipeline\",\n  \"tags\": [\"red\", \"applications\", \"sre\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 2, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 📊 RED Metrics Overview\\n**R**ate (throughput) | **E**rrors (failures) | **D**uration (latency)   The golden signals for service health\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"🚀 PRODUCER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 2 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Producer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 3 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Success\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Failed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Success\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"blue\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Failed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Producer Latency Percentiles\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 8 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Producer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 10 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_producer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📥 CONSUMER\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 17 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Rate (msg/min)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"P95 Latency\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency (P95)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 30 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"DB Inserts/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 23 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Throughput\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 18 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_db_operations_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Inserted to DB\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Consumed\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Inserted to DB\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"purple\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 17,\n      \"title\": \"End-to-End Latency\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 18,\n      \"title\": \"Consumer Errors by Type\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 12, \"x\": 12, \"y\": 25 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (error_type, stage) (rate(pipeline_consumer_errors_total[5m])) * 60\",\n          \"legendFormat\": \"{{error_type}} ({{stage}})\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"💰 BUSINESS METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 35 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Orders/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 0, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_orders_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Revenue/min\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 4, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 23,\n      \"title\": \"Avg Order Value\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 4, \"x\": 8, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_business_revenue_total[5m])) / (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"currencyUSD\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 24,\n      \"title\": \"Orders by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_orders_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"short\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 25,\n      \"title\": \"Revenue by Category\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 18, \"y\": 36 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (pipeline_business_revenue_total)\",\n          \"legendFormat\": \"{{category}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": { \"unit\": \"currencyUSD\" },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 26,\n      \"title\": \"Revenue Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 41 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\",\n          \"legendFormat\": \"{{category}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 30, \"lineWidth\": 2, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 27,\n      \"title\": \"Order Value Distribution\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 44 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"Median\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.90, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p90\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_business_order_value_bucket[5m]))\",\n          \"legendFormat\": \"p99 (High Value)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"currencyUSD\"\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/applications/slo-dashboard.json\n````json\n{\n  \"uid\": \"slo-dashboard\",\n  \"title\": \"SLI/SLO Dashboard\",\n  \"description\": \"Service Level Indicators and Objectives monitoring\",\n  \"tags\": [\"slo\", \"sre\", \"reliability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-24h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 100,\n      \"title\": \"\",\n      \"type\": \"text\",\n      \"gridPos\": { \"h\": 3, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"options\": {\n        \"mode\": \"markdown\",\n        \"content\": \"## 🎯 SLI/SLO Dashboard\\n\\n| Service | SLO Target | Error Budget (30d) |\\n|---------|------------|--------------------|\\n| Producer Success Rate | 99.9% | 43.2 min |\\n| Consumer Success Rate | 99.9% | 43.2 min |\\n| Latency P95 | < 100ms (Producer), < 5s (E2E) | - |\"\n      }\n    },\n\n    {\n      \"id\": 1,\n      \"title\": \"📊 ERROR BUDGET STATUS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 3 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Producer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Consumer Error Budget\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 6, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"orange\", \"value\": 20 },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 75 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Producer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Consumer Burn Rate (1h)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 6, \"w\": 3, \"x\": 15, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"x\",\n          \"decimals\": 2,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"orange\", \"value\": 6 },\n              { \"color\": \"red\", \"value\": 14.4 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Budget Burn Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 6, \"x\": 18, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"error_budget:producer:consumed_percent\",\n          \"legendFormat\": \"Producer Consumed\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"error_budget:consumer:consumed_percent\",\n          \"legendFormat\": \"Consumer Consumed\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📈 SLI METRICS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 10 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Consumer Success Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.9 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 13,\n      \"title\": \"Producer Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 14,\n      \"title\": \"E2E Latency Compliance\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:consumer:latency_good * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 95 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 15,\n      \"title\": \"Database Availability\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 3,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 99 },\n              { \"color\": \"green\", \"value\": 99.95 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 16,\n      \"title\": \"Consumer Lag\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 11 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 500 },\n              { \"color\": \"red\", \"value\": 1000 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"SLI Trends Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"legendFormat\": \"Producer Success Rate\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"legendFormat\": \"Consumer Success Rate\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"99.9\",\n          \"legendFormat\": \"SLO Target (99.9%)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"min\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"percent\",\n          \"min\": 99,\n          \"max\": 100\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"SLO Target (99.9%)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Burn Rate Trend\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 15 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"burn_rate:producer:1h\",\n          \"legendFormat\": \"Producer (1h)\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"burn_rate:consumer:1h\",\n          \"legendFormat\": \"Consumer (1h)\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"1\",\n          \"legendFormat\": \"Normal Rate (1x)\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"14.4\",\n          \"legendFormat\": \"Critical (14.4x)\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"x\",\n          \"min\": 0\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Normal Rate (1x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Critical (14.4x)\" }, \n            \"properties\": [\n              { \"id\": \"custom.lineStyle\", \"value\": { \"fill\": \"dash\", \"dash\": [10, 10] } },\n              { \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }\n            ] \n          }\n        ]\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📋 SLO SUMMARY TABLE\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 23 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"SLO Status\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 24 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sli:producer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sli:consumer:success_rate * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"sli:database:availability * 100\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"error_budget:producer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        },\n        {\n          \"refId\": \"E\",\n          \"expr\": \"error_budget:consumer:remaining_percent\",\n          \"instant\": true,\n          \"format\": \"table\"\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true },\n            \"renameByName\": { \n              \"Value #A\": \"Producer Success %\",\n              \"Value #B\": \"Consumer Success %\",\n              \"Value #C\": \"Database Availability %\",\n              \"Value #D\": \"Producer Budget %\",\n              \"Value #E\": \"Consumer Budget %\"\n            }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"md\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"center\", \"displayMode\": \"color-background-solid\" },\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 50 },\n              { \"color\": \"green\", \"value\": 99 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/infrastructure/container-metrics.json\n````json\n{\n  \"uid\": \"container-metrics\",\n  \"title\": \"Container Metrics\",\n  \"description\": \"Docker container resource usage monitoring\",\n  \"tags\": [\"infrastructure\", \"containers\", \"cadvisor\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"container\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(container_cpu_usage_seconds_total{name=~\\\".+\\\"}, name)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Running Containers\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(container_last_seen{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total CPU Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m])) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Memory Usage\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(container_memory_usage_bytes{name=~\\\".+\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 4294967296 }, { \"color\": \"red\", \"value\": 8589934592 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Network In\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_receive_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Network Out\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(container_network_transmit_bytes_total{name=~\\\".+\\\"}[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"CPU Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\"$container\\\"}[5m]) * 100\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"percent\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 70 }, { \"color\": \"red\", \"value\": 90 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Memory Usage by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\"$container\\\"}\",\n          \"legendFormat\": \"{{name}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Network I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_network_receive_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Receive\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_network_transmit_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Transmit\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Disk I/O by Container\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_fs_reads_bytes_total{name=~\\\"$container\\\"}[5m])\",\n          \"legendFormat\": \"{{name}} - Read\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(container_fs_writes_bytes_total{name=~\\\"$container\\\"}[5m]) * -1\",\n          \"legendFormat\": \"{{name}} - Write\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"Bps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"orange\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Container Resource Table\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(container_cpu_usage_seconds_total{name=~\\\".+\\\"}[5m]) * 100\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"container_memory_usage_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"container_spec_memory_limit_bytes{name=~\\\".+\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"merge\", \"options\": {} },\n        { \n          \"id\": \"organize\", \n          \"options\": { \n            \"excludeByName\": { \"Time\": true, \"__name__\": true, \"id\": true, \"image\": true, \"instance\": true, \"job\": true },\n            \"renameByName\": { \"name\": \"Container\", \"Value #A\": \"CPU %\", \"Value #B\": \"Memory Used\", \"Value #C\": \"Memory Limit\" }\n          } \n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\",\n        \"footer\": { \"show\": true, \"reducer\": [\"sum\"], \"countRows\": false }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"CPU %\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"percent\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"max\", \"value\": 100 },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 80 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Used\" }, \n            \"properties\": [\n              { \"id\": \"unit\", \"value\": \"bytes\" },\n              { \"id\": \"custom.displayMode\", \"value\": \"gradient-gauge\" },\n              { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 536870912 }, { \"color\": \"red\", \"value\": 1073741824 }] } }\n            ] \n          },\n          { \n            \"matcher\": { \"id\": \"byName\", \"options\": \"Memory Limit\" }, \n            \"properties\": [{ \"id\": \"unit\", \"value\": \"bytes\" }] \n          }\n        ]\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/infrastructure/kafka.json\n````json\n{\n  \"uid\": \"kafka-dashboard\",\n  \"title\": \"Kafka Overview\",\n  \"description\": \"Monitoring Apache Kafka cluster health and performance\",\n  \"tags\": [\"infrastructure\", \"kafka\", \"messaging\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"topic\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n        \"query\": \"label_values(kafka_topic_partitions, topic)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Kafka Brokers Up\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_brokers)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Total Topics\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Total Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partitions)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"purple\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Under Replicated Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_under_replicated_partition)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Offline Partitions\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_topic_partition_leader{leader=\\\"-1\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Consumer Groups\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(count by (consumergroup) (kafka_consumergroup_current_offset))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Messages In per Second (by Topic)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (topic) (rate(kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}[1m]))\",\n          \"legendFormat\": \"{{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\" },\n          \"unit\": \"msg/s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Consumer Group Lag\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (consumergroup, topic) (kafka_consumergroup_lag{topic=~\\\"$topic\\\"})\",\n          \"legendFormat\": \"{{consumergroup}} - {{topic}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\", \"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1000 }, { \"color\": \"red\", \"value\": 10000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Topic Partitions\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partitions{topic=~\\\"$topic\\\"}\",\n          \"format\": \"table\",\n          \"instant\": true\n        }\n      ],\n      \"transformations\": [\n        { \"id\": \"organize\", \"options\": { \"excludeByName\": { \"Time\": true, \"__name__\": true, \"instance\": true, \"job\": true }, \"renameByName\": { \"topic\": \"Topic\", \"Value\": \"Partitions\" } } }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\", \"filterable\": true }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Partitions\" }, \"properties\": [{ \"id\": \"custom.displayMode\", \"value\": \"color-background\" }, { \"id\": \"thresholds\", \"value\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Current Offset by Partition\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-{{partition}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"last\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"stepAfter\", \"fillOpacity\": 0, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Consumer Group Lag Sum\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 8, \"x\": 0, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(kafka_consumergroup_lag)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"min\": 0,\n          \"max\": 100000,\n          \"noValue\": \"0\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5000 }, { \"color\": \"red\", \"value\": 50000 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Messages per Partition\",\n      \"type\": \"bargauge\",\n      \"gridPos\": { \"h\": 6, \"w\": 16, \"x\": 8, \"y\": 20 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"kafka_topic_partition_current_offset{topic=~\\\"$topic\\\"}\",\n          \"legendFormat\": \"{{topic}}-p{{partition}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"orientation\": \"horizontal\",\n        \"displayMode\": \"gradient\",\n        \"showUnfilled\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/infrastructure/postgresql.json\n````json\n{\n  \"uid\": \"postgresql-dashboard\",\n  \"title\": \"PostgreSQL Overview\",\n  \"description\": \"Monitoring PostgreSQL database performance and health\",\n  \"tags\": [\"infrastructure\", \"postgresql\", \"database\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": { \"type\": \"grafana\", \"uid\": \"-- Grafana --\" },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"PostgreSQL Status\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_up\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            { \"type\": \"value\", \"options\": { \"0\": { \"text\": \"DOWN\", \"color\": \"red\" }, \"1\": { \"text\": \"UP\", \"color\": \"green\" } } }\n          ],\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"green\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Database Size\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 4, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_database_size_bytes{datname=\\\"datawarehouse\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1073741824 }, { \"color\": \"red\", \"value\": 5368709120 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Active Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 9, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(pg_stat_activity_count{datname=\\\"datawarehouse\\\"})\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 50 }, { \"color\": \"red\", \"value\": 100 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Max Connections\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 14, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_settings_max_connections\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 5, \"x\": 19, \"y\": 0 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - pg_postmaster_start_time_seconds\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Connections by State\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_activity_count{datname=\\\"datawarehouse\\\"}\",\n          \"legendFormat\": \"{{state}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"gradientMode\": \"scheme\", \"lineWidth\": 2, \"pointSize\": 5, \"showPoints\": \"auto\", \"stacking\": { \"mode\": \"none\" } },\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Transactions per Second\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_xact_commit{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Commits\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_xact_rollback{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Rollbacks\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"ops\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"Rollbacks\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 8,\n      \"title\": \"Rows Operations\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"rate(pg_stat_database_tup_inserted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Inserted\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"rate(pg_stat_database_tup_updated{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Updated\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"rate(pg_stat_database_tup_deleted{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Deleted\"\n        },\n        {\n          \"refId\": \"D\",\n          \"expr\": \"rate(pg_stat_database_tup_fetched{datname=\\\"datawarehouse\\\"}[1m])\",\n          \"legendFormat\": \"Fetched\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"gradientMode\": \"scheme\", \"lineWidth\": 2 },\n          \"unit\": \"rowsps\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 9,\n      \"title\": \"Cache Hit Ratio\",\n      \"type\": \"gauge\",\n      \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 12, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} / (pg_stat_database_blks_hit{datname=\\\"datawarehouse\\\"} + pg_stat_database_blks_read{datname=\\\"datawarehouse\\\"}) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"red\", \"value\": null }, { \"color\": \"yellow\", \"value\": 80 }, { \"color\": \"green\", \"value\": 95 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"Deadlocks\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 12 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_deadlocks{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"background\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Temp Files Created\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 6, \"x\": 18, \"y\": 16 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"increase(pg_stat_database_temp_files{datname=\\\"datawarehouse\\\"}[1h])\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 10 }, { \"color\": \"red\", \"value\": 50 }] }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/logs/logs-explorer.json\n````json\n{\n  \"uid\": \"logs-dashboard\",\n  \"title\": \"Logs Explorer\",\n  \"description\": \"Centralized logs from all services\",\n  \"tags\": [\"logs\", \"loki\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"level\",\n        \"type\": \"query\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"includeAll\": true,\n        \"multi\": true,\n        \"current\": { \"selected\": true, \"text\": \"All\", \"value\": \"$__all\" }\n      },\n      {\n        \"name\": \"search\",\n        \"type\": \"textbox\",\n        \"current\": { \"value\": \"\" },\n        \"label\": \"Search\"\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 6, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"sum\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 50, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 5, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1 },\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"fixed\", \"fixedColor\": \"red\" }\n        },\n        \"overrides\": []\n      }\n    },\n     {\n      \"id\": 3,\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | json | level=~\\\".+\\\" | __error__=\\\"\\\" [$__range]))\",\n          \"legendFormat\": \"{{level}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"pieType\": \"donut\",\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"values\": [\"value\", \"percent\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"color\": { \"mode\": \"palette-classic\" },\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"INFO\": { \"color\": \"green\", \"index\": 0 },\n                \"info\": { \"color\": \"green\", \"index\": 1 },\n                \"ERROR\": { \"color\": \"red\", \"index\": 2 },\n                \"error\": { \"color\": \"red\", \"index\": 3 },\n                \"WARNING\": { \"color\": \"yellow\", \"index\": 4 },\n                \"warning\": { \"color\": \"yellow\", \"index\": 5 },\n                \"WARN\": { \"color\": \"yellow\", \"index\": 6 },\n                \"warn\": { \"color\": \"yellow\", \"index\": 7 },\n                \"DEBUG\": { \"color\": \"blue\", \"index\": 8 },\n                \"debug\": { \"color\": \"blue\", \"index\": 9 }\n              }\n            }\n          ]\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 5, \"w\": 6, \"x\": 18, \"y\": 6 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"noValue\": \"0\",\n          \"thresholds\": { \n            \"mode\": \"absolute\", \n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 1 },\n              { \"color\": \"red\", \"value\": 10 }\n            ] \n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 11 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"All Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 15, \"w\": 24, \"x\": 0, \"y\": 12 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"dedupStrategy\": \"none\",\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 22,\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 12, \"x\": 12, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 38 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Order Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 32,\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 39 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/logs/test.json\n````json\n{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"description\": \"Centralized logs from all services\",\n  \"editable\": true,\n  \"fiscalYearStartMonth\": 0,\n  \"graphTooltip\": 0,\n  \"id\": 12,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"normal\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 1,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [\n            \"sum\"\n          ],\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Log Volume\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"fixedColor\": \"red\",\n            \"mode\": \"fixed\"\n          },\n          \"custom\": {\n            \"axisBorderShow\": false,\n            \"axisCenteredZero\": false,\n            \"axisColorMode\": \"text\",\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"barWidthFactor\": 0.6,\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 70,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            },\n            \"insertNulls\": false,\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"auto\",\n            \"showValues\": false,\n            \"spanNulls\": false,\n            \"stacking\": {\n              \"group\": \"A\",\n              \"mode\": \"none\"\n            },\n            \"thresholdsStyle\": {\n              \"mode\": \"off\"\n            }\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 80\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 6\n      },\n      \"id\": 2,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [],\n          \"displayMode\": \"list\",\n          \"placement\": \"bottom\",\n          \"showLegend\": true\n        },\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum by (service) (count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [$__interval]))\",\n          \"legendFormat\": \"{{service}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Error Logs\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"hideFrom\": {\n              \"legend\": false,\n              \"tooltip\": false,\n              \"viz\": false\n            }\n          },\n          \"mappings\": [],\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"ERROR\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"red\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"WARNING\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"yellow\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"INFO\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"green\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          },\n          {\n            \"matcher\": {\n              \"id\": \"byName\",\n              \"options\": \"DEBUG\"\n            },\n            \"properties\": [\n              {\n                \"id\": \"color\",\n                \"value\": {\n                  \"fixedColor\": \"blue\",\n                  \"mode\": \"fixed\"\n                }\n              }\n            ]\n          }\n        ]\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 6\n      },\n      \"id\": 3,\n      \"options\": {\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"values\": [\n            \"value\",\n            \"percent\"\n          ]\n        },\n        \"pieType\": \"donut\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"sort\": \"desc\",\n        \"tooltip\": {\n          \"hideZeros\": false,\n          \"mode\": \"single\",\n          \"sort\": \"none\"\n        }\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"direction\": \"backward\",\n          \"editorMode\": \"code\",\n          \"expr\": \"sum by (level) (count_over_time({service=~\\\"$service\\\"} | __error__=\\\"\\\" [$__range]))\",\n          \"instant\": true,\n          \"legendFormat\": \"{{level}}\",\n          \"queryType\": \"range\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Logs by Level\",\n      \"type\": \"piechart\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [],\n          \"noValue\": \"0\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": 0\n              },\n              {\n                \"color\": \"yellow\",\n                \"value\": 1\n              },\n              {\n                \"color\": \"red\",\n                \"value\": 10\n              }\n            ]\n          },\n          \"unit\": \"short\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 5,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 6\n      },\n      \"id\": 4,\n      \"options\": {\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"orientation\": \"auto\",\n        \"percentChangeColorMode\": \"standard\",\n        \"reduceOptions\": {\n          \"calcs\": [\n            \"lastNotNull\"\n          ],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"showPercentChange\": false,\n        \"textMode\": \"auto\",\n        \"wideLayout\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"sum(count_over_time({service=~\\\"$service\\\"} |= \\\"ERROR\\\" [5m]))\",\n          \"instant\": true,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Recent Errors\",\n      \"type\": \"stat\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 11\n      },\n      \"id\": 10,\n      \"panels\": [],\n      \"title\": \"📋 LOG STREAM\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 15,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"id\": 11,\n      \"options\": {\n        \"dedupStrategy\": \"none\",\n        \"enableInfiniteScrolling\": false,\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showCommonLabels\": false,\n        \"showControls\": false,\n        \"showLabels\": true,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"pluginVersion\": \"12.3.1\",\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"$service\\\", level=~\\\"$level\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 500,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"All Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 27\n      },\n      \"id\": 20,\n      \"panels\": [],\n      \"title\": \"🔍 SERVICE LOGS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 28\n      },\n      \"id\": 21,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-producer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Producer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 10,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 28\n      },\n      \"id\": 22,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=\\\"data-consumer\\\"} |~ \\\"$search\\\"\",\n          \"maxLines\": 200,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Consumer Logs\",\n      \"type\": \"logs\"\n    },\n    {\n      \"collapsed\": false,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 38\n      },\n      \"id\": 30,\n      \"panels\": [],\n      \"title\": \"📊 BUSINESS EVENTS\",\n      \"type\": \"row\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 39\n      },\n      \"id\": 31,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"order\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Order Events\",\n      \"type\": \"logs\"\n    },\n    {\n      \"datasource\": {\n        \"type\": \"loki\",\n        \"uid\": \"loki\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {},\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 39\n      },\n      \"id\": 32,\n      \"options\": {\n        \"enableLogDetails\": true,\n        \"prettifyLogMessage\": true,\n        \"showLabels\": false,\n        \"showTime\": true,\n        \"sortOrder\": \"Descending\",\n        \"wrapLogMessage\": true\n      },\n      \"targets\": [\n        {\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"batch\\\" | json\",\n          \"maxLines\": 100,\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Batch Events\",\n      \"type\": \"logs\"\n    }\n  ],\n  \"preload\": false,\n  \"refresh\": \"10s\",\n  \"schemaVersion\": 42,\n  \"tags\": [\n    \"logs\",\n    \"loki\",\n    \"observability\"\n  ],\n  \"templating\": {\n    \"list\": [\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"service\",\n        \"options\": [],\n        \"query\": \"label_values(service)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"text\": \"All\",\n          \"value\": \"$__all\"\n        },\n        \"datasource\": {\n          \"type\": \"loki\",\n          \"uid\": \"loki\"\n        },\n        \"includeAll\": true,\n        \"multi\": true,\n        \"name\": \"level\",\n        \"options\": [],\n        \"query\": \"label_values(level)\",\n        \"refresh\": 2,\n        \"type\": \"query\"\n      },\n      {\n        \"current\": {\n          \"value\": \"\"\n        },\n        \"label\": \"Search\",\n        \"name\": \"search\",\n        \"type\": \"textbox\"\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {},\n  \"timezone\": \"browser\",\n  \"title\": \"Logs Explorer\",\n  \"uid\": \"logs-dashboard\",\n  \"version\": 3\n}\n````\n\n## File: monitoring/grafana/dashboards/overview/correlation-dashboard.json\n````json\n{\n  \"uid\": \"correlation-dashboard\",\n  \"title\": \"Metrics & Logs Correlation\",\n  \"description\": \"View metrics and related logs side by side\",\n  \"tags\": [\"correlation\", \"metrics\", \"logs\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"10s\",\n  \n  \"time\": {\n    \"from\": \"now-30m\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"name\": \"Errors\",\n        \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n        \"enable\": true,\n        \"iconColor\": \"red\",\n        \"expr\": \"{service=\\\"$service\\\"} |= \\\"ERROR\\\"\",\n        \"titleFormat\": \"Error\",\n        \"textFormat\": \"{{ __line__ }}\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"📊 Metrics Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Message Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"success\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"msg/min\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Error Rate\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Producer Errors\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"sum(rate(pipeline_consumer_messages_total{status=\\\"failed\\\"}[1m])) * 60\",\n          \"legendFormat\": \"Consumer Errors\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 20, \"lineWidth\": 2 },\n          \"unit\": \"short\",\n          \"noValue\": \"0\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byRegexp\", \"options\": \"/Error/\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Latency P95\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 7, \"w\": 8, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Producer P95\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"Consumer E2E P95\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 8 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Log Volume (matches time range above)\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 4, \"w\": 24, \"x\": 0, \"y\": 9 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum by (level) (count_over_time({service=\\\"$service\\\"} | json | __error__=\\\"\\\" [$__interval]))\",\n          \"legendFormat\": \"{{level}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"list\", \"placement\": \"right\", \"showLegend\": true }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"bars\", \"fillOpacity\": 70, \"lineWidth\": 1, \"stacking\": { \"mode\": \"normal\" } },\n          \"unit\": \"short\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"ERROR\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"INFO\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"WARNING\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"Service Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 13 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"}\",\n          \"maxLines\": 200\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔴 Error Logs Only\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 25 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Error Logs\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 8, \"w\": 24, \"x\": 0, \"y\": 26 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=\\\"$service\\\"} |~ \\\"(?i)error|ERROR\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/overview/system-overview.json\n````json\n{\n  \"uid\": \"system-overview\",\n  \"title\": \"System Overview\",\n  \"description\": \"Overview of all monitored services\",\n  \"tags\": [\"overview\", \"prometheus\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": []\n  },\n\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": {\n          \"type\": \"grafana\",\n          \"uid\": \"-- Grafana --\"\n        },\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"Services Health\",\n      \"description\": \"Status of all monitored services (1 = UP, 0 = DOWN)\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 4,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"up\",\n          \"legendFormat\": \"{{job}}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"orientation\": \"horizontal\",\n        \"textMode\": \"auto\",\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"mappings\": [\n            {\n              \"type\": \"value\",\n              \"options\": {\n                \"0\": {\n                  \"text\": \"DOWN\",\n                  \"color\": \"red\",\n                  \"index\": 0\n                },\n                \"1\": {\n                  \"text\": \"UP\",\n                  \"color\": \"green\",\n                  \"index\": 1\n                }\n              }\n            }\n          ],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"red\", \"value\": null },\n              { \"color\": \"green\", \"value\": 1 }\n            ]\n          },\n          \"unit\": \"none\"\n        },\n        \"overrides\": []\n      }\n    },\n    \n    {\n      \"id\": 2,\n      \"title\": \"Prometheus Scrape Duration\",\n      \"description\": \"Time taken to scrape each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_duration_seconds\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"line\",\n            \"lineInterpolation\": \"smooth\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"scheme\",\n            \"spanNulls\": false,\n            \"lineWidth\": 2,\n            \"pointSize\": 5,\n            \"showPoints\": \"auto\"\n          },\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 0.5 },\n              { \"color\": \"red\", \"value\": 1 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Samples Scraped\",\n      \"description\": \"Number of metrics scraped from each target\",\n      \"type\": \"timeseries\",\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 4\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"scrape_samples_scraped\",\n          \"legendFormat\": \"{{job}}\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": {\n          \"mode\": \"multi\",\n          \"sort\": \"desc\"\n        },\n        \"legend\": {\n          \"displayMode\": \"table\",\n          \"placement\": \"right\",\n          \"showLegend\": true,\n          \"calcs\": [\"mean\", \"max\"]\n        }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": {\n            \"drawStyle\": \"bars\",\n            \"fillOpacity\": 50,\n            \"gradientMode\": \"hue\",\n            \"lineWidth\": 1\n          },\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Prometheus Memory Usage\",\n      \"type\": \"gauge\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 0,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"process_resident_memory_bytes{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"showThresholdLabels\": false,\n        \"showThresholdMarkers\": true\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"min\": 0,\n          \"max\": 1073741824,\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 536870912 },\n              { \"color\": \"red\", \"value\": 858993459 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"Prometheus Storage Size\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 6,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_storage_blocks_bytes\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"bytes\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null },\n              { \"color\": \"yellow\", \"value\": 5368709120 },\n              { \"color\": \"red\", \"value\": 10737418240 }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Total Time Series\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"prometheus_tsdb_head_series\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"blue\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Uptime\",\n      \"type\": \"stat\",\n      \"gridPos\": {\n        \"h\": 6,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 12\n      },\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"time() - process_start_time_seconds{job=\\\"prometheus\\\"}\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": {\n          \"values\": false,\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\"\n        },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              { \"color\": \"green\", \"value\": null }\n            ]\n          }\n        },\n        \"overrides\": []\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/dashboards/tracing/tracing-overview.json\n````json\n{\n  \"uid\": \"tracing-overview\",\n  \"title\": \"Distributed Tracing Overview\",\n  \"description\": \"Jaeger traces visualization and analysis\",\n  \"tags\": [\"tracing\", \"jaeger\", \"observability\"],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"30s\",\n  \n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n\n  \"templating\": {\n    \"list\": [\n      {\n        \"name\": \"service\",\n        \"type\": \"custom\",\n        \"query\": \"data-producer,data-consumer\",\n        \"current\": { \"text\": \"data-producer\", \"value\": \"data-producer\" },\n        \"options\": [\n          { \"text\": \"data-producer\", \"value\": \"data-producer\", \"selected\": true },\n          { \"text\": \"data-consumer\", \"value\": \"data-consumer\", \"selected\": false }\n        ]\n      },\n      {\n        \"name\": \"operation\",\n        \"type\": \"custom\",\n        \"query\": \"all,produce_order,produce_batch,consume_message,process_batch,db_insert_orders\",\n        \"includeAll\": true,\n        \"current\": { \"text\": \"All\", \"value\": \"$__all\" },\n        \"options\": [\n          { \"text\": \"All\", \"value\": \"$__all\", \"selected\": true },\n          { \"text\": \"produce_order\", \"value\": \"produce_order\", \"selected\": false },\n          { \"text\": \"produce_batch\", \"value\": \"produce_batch\", \"selected\": false },\n          { \"text\": \"consume_message\", \"value\": \"consume_message\", \"selected\": false },\n          { \"text\": \"process_batch\", \"value\": \"process_batch\", \"selected\": false },\n          { \"text\": \"db_insert_orders\", \"value\": \"db_insert_orders\", \"selected\": false }\n        ]\n      }\n    ]\n  },\n\n  \"annotations\": {\n    \"list\": []\n  },\n\n  \"panels\": [\n    {\n      \"id\": 1,\n      \"title\": \"🔍 Tracing Overview\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 0 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 2,\n      \"title\": \"Trace Count\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 0, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"sum(rate(pipeline_producer_messages_total{status=\\\"success\\\"}[5m])) * 300\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"decimals\": 0,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"blue\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 3,\n      \"title\": \"Avg Duration (Producer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 4, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.1 }, { \"color\": \"red\", \"value\": 0.5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 4,\n      \"title\": \"Avg Duration (Consumer)\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 8, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 3,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 0.5 }, { \"color\": \"red\", \"value\": 1 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 5,\n      \"title\": \"E2E Latency P95\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 12, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"s\",\n          \"decimals\": 2,\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 5 }, { \"color\": \"red\", \"value\": 10 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 6,\n      \"title\": \"Error Rate\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 16, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"(sum(rate(pipeline_producer_messages_total{status=\\\"failed\\\"}[5m])) / (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)) * 100\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"area\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"decimals\": 2,\n          \"noValue\": \"0%\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }, { \"color\": \"yellow\", \"value\": 1 }, { \"color\": \"red\", \"value\": 5 }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 7,\n      \"title\": \"Services\",\n      \"type\": \"stat\",\n      \"gridPos\": { \"h\": 4, \"w\": 4, \"x\": 20, \"y\": 1 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"count(up{job=~\\\"data-producer|data-consumer\\\"} == 1)\",\n          \"instant\": true\n        }\n      ],\n      \"options\": {\n        \"reduceOptions\": { \"values\": false, \"calcs\": [\"lastNotNull\"], \"fields\": \"\" },\n        \"colorMode\": \"value\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"textMode\": \"auto\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"short\",\n          \"thresholds\": { \"mode\": \"absolute\", \"steps\": [{ \"color\": \"green\", \"value\": null }] }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 10,\n      \"title\": \"📊 Latency Distribution\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 5 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 11,\n      \"title\": \"Producer Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": [\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p50\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"green\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p95\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"yellow\", \"mode\": \"fixed\" } }] },\n          { \"matcher\": { \"id\": \"byName\", \"options\": \"p99\" }, \"properties\": [{ \"id\": \"color\", \"value\": { \"fixedColor\": \"red\", \"mode\": \"fixed\" } }] }\n        ]\n      }\n    },\n\n    {\n      \"id\": 12,\n      \"title\": \"End-to-End Latency Over Time\",\n      \"type\": \"timeseries\",\n      \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 12, \"y\": 6 },\n      \"datasource\": { \"type\": \"prometheus\", \"uid\": \"prometheus\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p50\"\n        },\n        {\n          \"refId\": \"B\",\n          \"expr\": \"histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p95\"\n        },\n        {\n          \"refId\": \"C\",\n          \"expr\": \"histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\",\n          \"legendFormat\": \"p99\"\n        }\n      ],\n      \"options\": {\n        \"tooltip\": { \"mode\": \"multi\", \"sort\": \"desc\" },\n        \"legend\": { \"displayMode\": \"table\", \"placement\": \"right\", \"showLegend\": true, \"calcs\": [\"mean\", \"max\"] }\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"drawStyle\": \"line\", \"lineInterpolation\": \"smooth\", \"fillOpacity\": 10, \"lineWidth\": 2 },\n          \"unit\": \"s\"\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 20,\n      \"title\": \"🔎 Trace Search\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 14 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 21,\n      \"title\": \"Recent Traces\",\n      \"type\": \"table\",\n      \"gridPos\": { \"h\": 12, \"w\": 24, \"x\": 0, \"y\": 15 },\n      \"datasource\": { \"type\": \"jaeger\", \"uid\": \"jaeger\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"queryType\": \"search\",\n          \"service\": \"$service\",\n          \"limit\": 20\n        }\n      ],\n      \"options\": {\n        \"showHeader\": true,\n        \"cellHeight\": \"sm\"\n      },\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"custom\": { \"align\": \"auto\", \"displayMode\": \"auto\" }\n        },\n        \"overrides\": []\n      }\n    },\n\n    {\n      \"id\": 30,\n      \"title\": \"📝 Related Logs\",\n      \"type\": \"row\",\n      \"gridPos\": { \"h\": 1, \"w\": 24, \"x\": 0, \"y\": 27 },\n      \"collapsed\": false\n    },\n\n    {\n      \"id\": 31,\n      \"title\": \"Application Logs (with trace_id)\",\n      \"type\": \"logs\",\n      \"gridPos\": { \"h\": 10, \"w\": 24, \"x\": 0, \"y\": 28 },\n      \"datasource\": { \"type\": \"loki\", \"uid\": \"loki\" },\n      \"targets\": [\n        {\n          \"refId\": \"A\",\n          \"expr\": \"{service=~\\\"data-producer|data-consumer\\\"} | json | __error__=\\\"\\\" | trace_id =~ \\\".+\\\"\",\n          \"maxLines\": 100\n        }\n      ],\n      \"options\": {\n        \"showTime\": true,\n        \"showLabels\": true,\n        \"showCommonLabels\": false,\n        \"wrapLogMessage\": true,\n        \"prettifyLogMessage\": true,\n        \"enableLogDetails\": true,\n        \"sortOrder\": \"Descending\"\n      }\n    }\n  ]\n}\n````\n\n## File: monitoring/grafana/provisioning/dashboards/dashboards.yml\n````yaml\n# monitoring/grafana/provisioning/dashboards/dashboards.yml\n\napiVersion: 1\n\nproviders:\n  # ===========================================\n  # Overview Dashboards\n  # ===========================================\n  - name: 'overview'\n    orgId: 1\n    folder: 'Overview'\n    folderUid: 'overview'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: true              # ✅  cho edit trên UI\n    options:\n      path: /var/lib/grafana/dashboards/overview\n\n  # ===========================================\n  # Infrastructure Dashboards\n  # ===========================================\n  - name: 'infrastructure'\n    orgId: 1\n    folder: 'Infrastructure'\n    folderUid: 'infrastructure'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/infrastructure\n\n  # ===========================================\n  # Application Dashboards\n  # ===========================================\n  - name: 'applications'\n    orgId: 1\n    folder: 'Applications'\n    folderUid: 'applications'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/applications\n\n  # ===========================================\n  # Logs Dashboards\n  # ===========================================\n  - name: 'logs'\n    orgId: 1\n    folder: 'Logs'\n    folderUid: 'logs'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/logs\n\n  - name: 'tracing'\n    orgId: 1\n    folder: 'Tracing'\n    folderUid: 'tracing'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 30\n    allowUiUpdates: false\n    options:\n      path: /var/lib/grafana/dashboards/tracing\n````\n\n## File: monitoring/grafana/provisioning/datasources/datasources.yml\n````yaml\n# monitoring/grafana/provisioning/datasources/datasources.yml\n\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    uid: prometheus                    # ✅ UID cố định - RẤT QUAN TRỌNG!\n    access: proxy\n    url: http://prometheus:9090\n    isDefault: true\n    editable: true                    # Không cho edit trên UI để tránh drift\n    jsonData:\n      timeInterval: \"15s\"\n      httpMethod: POST                 # POST tốt hơn cho large queries\n\n  - name: Alertmanager\n    type: alertmanager\n    uid: alertmanager\n    access: proxy\n    url: http://alertmanager:9093\n    editable: false\n    jsonData:\n      implementation: prometheus\n\n  - name: Loki\n    type: loki\n    uid: loki\n    access: proxy\n    url: http://loki:3100\n    editable: false\n    jsonData:\n      timeout: 60\n      maxLines: 1000\n      derivedFields:\n        # Link từ logs sang related logs by order_id\n        - name: order_id\n          matcherRegex: '\"order_id\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"loki\",\"queries\":[{\"expr\":\"{service=~\\\"data-producer|data-consumer\\\"} |= \\\"$${__value.raw}\\\"\"}]}'\n          datasourceUid: loki\n          urlDisplayLabel: \"View related logs\"\n        \n        # Link từ logs sang metrics by service\n        - name: service_metrics\n          matcherRegex: '\"service\":\\s*\"([^\"]+)\"'\n          url: '/explore?orgId=1&left={\"datasource\":\"prometheus\",\"queries\":[{\"expr\":\"up{job=\\\"$${__value.raw}\\\"}\"}]}'\n          datasourceUid: prometheus\n          urlDisplayLabel: \"View metrics\"\n\n        - name: TraceID\n          matcherRegex: '\"trace_id\":\\s*\"([a-f0-9]+)\"'\n          url: '$${__value.raw}'\n          datasourceUid: jaeger\n          urlDisplayLabel: \"View Trace\"\n\n  - name: Jaeger\n    type: jaeger\n    uid: jaeger\n    access: proxy\n    url: http://jaeger:16686\n    editable: false\n    jsonData:\n      tracesToLogs:\n        datasourceUid: loki\n        tags: ['service']\n        mappedTags: [{ key: 'service.name', value: 'service' }]\n        mapTagNamesEnabled: true\n        spanStartTimeShift: '-1h'\n        spanEndTimeShift: '1h'\n        filterByTraceID: true\n        filterBySpanID: false\n        lokiSearch: true\n````\n\n## File: monitoring/loki/rules/fake/alerts.yml\n````yaml\n# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"\n````\n\n## File: monitoring/loki/rules/alerts.yml\n````yaml\n# loki/rules/alerts.yml\n\ngroups:\n  - name: log_alerts\n    interval: 1m  # Evaluation interval\n    rules:\n      - alert: HighErrorLogRate\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)error\" [5m])) > 10\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"High error rate in application logs\"\n          description: \"{{ $value }} ERROR logs detected in the last 5 minutes\"\n\n      - alert: DatabaseConnectionError\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} |~ \"(?i)connection.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: database\n        annotations:\n          summary: \"Database connection errors detected\"\n          description: \"{{ $value }} database connection errors in consumer logs\"\n\n      - alert: KafkaConnectionError\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)kafka.*(error|failed)\" [5m])) > 3\n        for: 1m\n        labels:\n          severity: critical\n          category: logs\n          service: kafka\n        annotations:\n          summary: \"Kafka connection errors detected\"\n          description: \"{{ $value }} Kafka errors detected in application logs\"\n\n      - alert: ExceptionDetected\n        expr: |\n          sum(count_over_time({service=~\"data-producer|data-consumer\"} |~ \"(?i)(exception|traceback)\" [5m])) > 5\n        for: 2m\n        labels:\n          severity: warning\n          category: logs\n        annotations:\n          summary: \"Exception detected in logs\"\n          description: \"{{ $value }} exceptions or stack traces found in application logs\"\n\n      - alert: NoLogsFromProducer\n        expr: |\n          sum(count_over_time({service=\"data-producer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: producer\n        annotations:\n          summary: \"No logs from Producer\"\n          description: \"Producer has not generated any logs in the last 5 minutes\"\n\n      - alert: NoLogsFromConsumer\n        expr: |\n          sum(count_over_time({service=\"data-consumer\"} [5m])) == 0\n        for: 5m\n        labels:\n          severity: warning\n          category: logs\n          service: consumer\n        annotations:\n          summary: \"No logs from Consumer\"\n          description: \"Consumer has not generated any logs in the last 5 minutes\"\n````\n\n## File: monitoring/loki/loki-config.yml\n````yaml\n# monitoring/loki/loki-config.yml\n\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n  grpc_listen_port: 9096\n  log_level: info\n\ncommon:\n  instance_addr: 127.0.0.1\n  path_prefix: /loki\n  storage:\n    filesystem:\n      chunks_directory: /loki/chunks\n      rules_directory: /loki/rules\n  replication_factor: 1\n  ring:\n    kvstore:\n      store: inmemory\n\nquery_range:\n  results_cache:\n    cache:\n      embedded_cache:\n        enabled: true\n        max_size_mb: 100\n\nschema_config:\n  configs:\n    - from: 2024-01-01\n      store: tsdb\n      object_store: filesystem\n      schema: v13\n      index:\n        prefix: index_\n        period: 24h\n\nstorage_config:\n  filesystem:\n    directory: /loki/storage\n\nlimits_config:\n  retention_period: 168h  # 7 days\n  ingestion_rate_mb: 10\n  ingestion_burst_size_mb: 20\n  max_streams_per_user: 10000\n  max_line_size: 256kb\n\ncompactor:\n  working_directory: /loki/compactor\n  compaction_interval: 10m\n  retention_enabled: true\n  retention_delete_delay: 2h\n  delete_request_store: filesystem\n\nruler:\n  alertmanager_url: http://alertmanager:9093\n  storage:\n    type: local\n    local:\n      directory: /loki/rules\n  rule_path: /loki/rules-temp\n  enable_api: true\n  enable_alertmanager_v2: true\n  ring:\n    kvstore:\n      store: inmemory\n\nanalytics:\n  reporting_enabled: false\n````\n\n## File: monitoring/prometheus/rules/alert_rules.yml\n````yaml\n# monitoring/prometheus/rules/alert_rules.yml\n\ngroups:\n  # ===========================================\n  # Service Health Alerts\n  # ===========================================\n  - name: service_health_alerts\n    rules:\n      # Service Down - Critical\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"{{ $labels.job }} has been down for more than 1 minute.\"\n          runbook_url: \"https://wiki.example.com/runbook/service-down\"\n\n      # Service Flapping - Warning\n      - alert: ServiceFlapping\n        expr: changes(up[10m]) > 3\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Service {{ $labels.job }} is flapping\"\n          description: \"{{ $labels.job }} has changed state {{ $value }} times in the last 10 minutes.\"\n\n  # ===========================================\n  # Data Pipeline Alerts\n  # ===========================================\n  - name: pipeline_alerts\n    rules:\n      # Producer Down\n      - alert: ProducerDown\n        expr: up{job=\"data-producer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n        annotations:\n          summary: \"Data Producer is down\"\n          description: \"Data Producer has been down for more than 1 minute. No data is being produced to Kafka.\"\n\n      # Consumer Down\n      - alert: ConsumerDown\n        expr: up{job=\"data-consumer\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: consumer\n        annotations:\n          summary: \"Data Consumer is down\"\n          description: \"Data Consumer has been down for more than 1 minute. No data is being processed.\"\n\n      # Producer Not Producing\n      - alert: ProducerNotProducing\n        expr: rate(pipeline_messages_produced_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer is not producing messages\"\n          description: \"No messages have been produced in the last 5 minutes.\"\n\n      # Consumer Not Consuming\n      - alert: ConsumerNotConsuming\n        expr: rate(pipeline_messages_consumed_total{status=\"success\"}[5m]) == 0\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer is not consuming messages\"\n          description: \"No messages have been consumed in the last 5 minutes.\"\n\n      # High Error Rate - Producer\n      - alert: ProducerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_produced_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_produced_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n        annotations:\n          summary: \"Producer error rate is high\"\n          description: \"Producer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Error Rate - Consumer\n      - alert: ConsumerHighErrorRate\n        expr: |\n          (\n            rate(pipeline_messages_consumed_total{status=\"failed\"}[5m]) /\n            (rate(pipeline_messages_consumed_total[5m]) + 0.001)\n          ) * 100 > 5\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Consumer error rate is high\"\n          description: \"Consumer error rate is {{ $value | printf \\\"%.2f\\\" }}% (threshold: 5%).\"\n\n      # High Processing Latency\n      - alert: HighProcessingLatency\n        expr: pipeline:processing_latency_p95:seconds > 1\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n        annotations:\n          summary: \"Processing latency is high\"\n          description: \"P95 processing latency is {{ $value | printf \\\"%.2f\\\" }}s (threshold: 1s).\"\n\n  # ===========================================\n  # Kafka Alerts\n  # ===========================================\n  - name: kafka_alerts\n    rules:\n      # Kafka Down\n      - alert: KafkaDown\n        expr: up{job=\"kafka\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka exporter is down\"\n          description: \"Cannot scrape Kafka metrics. Kafka might be down.\"\n\n      # High Consumer Lag\n      - alert: KafkaHighConsumerLag\n        expr: kafka:consumer_lag:sum > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is high\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 1000).\"\n\n      # Critical Consumer Lag\n      - alert: KafkaCriticalConsumerLag\n        expr: kafka:consumer_lag:sum > 10000\n        for: 5m\n        labels:\n          severity: critical\n          service: kafka\n        annotations:\n          summary: \"Kafka consumer lag is critical\"\n          description: \"Total consumer lag is {{ $value }} messages (threshold: 10000). Consumer might be stuck.\"\n\n      # No Messages Flowing\n      - alert: KafkaNoMessagesFlowing\n        expr: kafka:messages_in:rate1m:total == 0\n        for: 10m\n        labels:\n          severity: warning\n          service: kafka\n        annotations:\n          summary: \"No messages flowing through Kafka\"\n          description: \"No messages have been produced to Kafka in the last 10 minutes.\"\n\n  # ===========================================\n  # PostgreSQL Alerts\n  # ===========================================\n  - name: postgresql_alerts\n    rules:\n      # PostgreSQL Down\n      - alert: PostgreSQLDown\n        expr: pg_up == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL is down\"\n          description: \"PostgreSQL database is not responding.\"\n\n      # High Connection Usage\n      - alert: PostgreSQLHighConnections\n        expr: postgresql:connections:usage_percent > 80\n        for: 5m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is high\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical Connection Usage\n      - alert: PostgreSQLCriticalConnections\n        expr: postgresql:connections:usage_percent > 95\n        for: 2m\n        labels:\n          severity: critical\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL connection usage is critical\"\n          description: \"Connection usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%). New connections may fail.\"\n\n      # Low Cache Hit Ratio\n      - alert: PostgreSQLLowCacheHitRatio\n        expr: postgresql:cache_hit_ratio:percent < 90\n        for: 10m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL cache hit ratio is low\"\n          description: \"Cache hit ratio is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 90%). Consider increasing shared_buffers.\"\n\n      # Deadlocks Detected\n      - alert: PostgreSQLDeadlocks\n        expr: increase(pg_stat_database_deadlocks{datname=\"datawarehouse\"}[5m]) > 0\n        for: 0m\n        labels:\n          severity: warning\n          service: postgresql\n        annotations:\n          summary: \"PostgreSQL deadlocks detected\"\n          description: \"{{ $value }} deadlocks detected in the last 5 minutes.\"\n\n  # ===========================================\n  # Container Resource Alerts (Fixed)\n  # ===========================================\n  - name: container_alerts\n    rules:\n      # High CPU Usage\n      - alert: ContainerHighCPU\n        expr: container:cpu_usage_percent:rate5m > 80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 80%).\"\n\n      # Critical CPU Usage\n      - alert: ContainerCriticalCPU\n        expr: container:cpu_usage_percent:rate5m > 95\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical CPU usage\"\n          description: \"CPU usage is {{ $value | printf \\\"%.1f\\\" }}% (threshold: 95%).\"\n\n      # High Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerHighMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 80\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} has high memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 80%).\"\n\n      # Critical Memory Usage (chỉ cho containers có limit)\n      - alert: ContainerCriticalMemory\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} /\n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100 > 95\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} has critical memory usage\"\n          description: \"Memory usage is {{ $value | printf \\\"%.1f\\\" }}% of limit (threshold: 95%). OOM kill imminent.\"\n\n      # High Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerHighMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 1073741824\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} using high memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 1GB).\"\n\n      # Critical Memory Usage (absolute - cho containers không có limit)\n      - alert: ContainerCriticalMemoryAbsolute\n        expr: container_memory_usage_bytes{name=~\".+\"} > 2147483648\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Container {{ $labels.name }} using critical memory\"\n          description: \"Memory usage is {{ $value | humanize1024 }}B (threshold: 2GB).\"\n\n      # Container Restarting\n      - alert: ContainerRestarting\n        expr: increase(container_restart_count{name=~\".+\"}[1h]) > 3\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Container {{ $labels.name }} is restarting frequently\"\n          description: \"Container has restarted {{ $value | printf \\\"%.0f\\\" }} times in the last hour.\"\n````\n\n## File: monitoring/prometheus/rules/recording_rules.yml\n````yaml\n# monitoring/prometheus/rules/recording_rules.yml\n\ngroups:\n  # ===========================================\n  # Container Metrics - Pre-computed\n  # ===========================================\n  - name: container_metrics\n    interval: 15s\n    rules:\n      - record: container:cpu_usage_percent:rate5m\n        expr: rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]) * 100\n\n      - record: container:memory_usage_bytes:current\n        expr: container_memory_usage_bytes{name=~\".+\"}\n\n      - record: container:memory_usage_percent:current\n        expr: |\n          (\n            container_memory_usage_bytes{name=~\".+\"} / \n            container_spec_memory_limit_bytes{name=~\".+\"}\n          ) * 100\n          and\n          container_spec_memory_limit_bytes{name=~\".+\"} > 0\n\n      - record: container:network_receive_bytes:rate5m\n        expr: rate(container_network_receive_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:network_transmit_bytes:rate5m\n        expr: rate(container_network_transmit_bytes_total{name=~\".+\"}[5m])\n\n      - record: container:cpu_usage_percent:total\n        expr: sum(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m])) * 100\n\n      - record: container:memory_usage_bytes:total\n        expr: sum(container_memory_usage_bytes{name=~\".+\"})\n\n      - record: container:count:total\n        expr: count(container_last_seen{name=~\".+\"})\n\n  # ===========================================\n  # RED Metrics - Producer\n  # ===========================================\n  - name: red_producer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:producer:rate_per_second\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:producer:rate_per_minute\n        expr: sum(rate(pipeline_producer_messages_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:producer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_producer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:producer:errors_per_minute\n        expr: sum(rate(pipeline_producer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:producer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n      - record: red:producer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_producer_message_duration_seconds_bucket[5m]))\n\n  # ===========================================\n  # RED Metrics - Consumer\n  # ===========================================\n  - name: red_consumer\n    interval: 15s\n    rules:\n      # Rate\n      - record: red:consumer:rate_per_second\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m]))\n\n      - record: red:consumer:rate_per_minute\n        expr: sum(rate(pipeline_consumer_messages_total{status=\"success\"}[1m])) * 60\n\n      - record: red:consumer:db_inserts_per_minute\n        expr: sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[1m])) * 60\n\n      # Errors\n      - record: red:consumer:error_rate_percent\n        expr: |\n          (\n            sum(rate(pipeline_consumer_messages_total{status=\"failed\"}[5m])) /\n            (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n          ) * 100\n\n      - record: red:consumer:errors_per_minute\n        expr: sum(rate(pipeline_consumer_errors_total[1m])) * 60\n\n      # Duration\n      - record: red:consumer:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      - record: red:consumer:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_batch_duration_seconds_bucket[5m]))\n\n      # End-to-End Latency\n      - record: red:e2e:latency_p50\n        expr: histogram_quantile(0.50, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p95\n        expr: histogram_quantile(0.95, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n      - record: red:e2e:latency_p99\n        expr: histogram_quantile(0.99, rate(pipeline_consumer_end_to_end_latency_seconds_bucket[5m]))\n\n  # ===========================================\n  # Business Metrics\n  # ===========================================\n  - name: business_metrics\n    interval: 15s\n    rules:\n      - record: business:orders:rate_per_minute\n        expr: sum(rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue:rate_per_minute\n        expr: sum(rate(pipeline_business_revenue_total[1m])) * 60\n\n      - record: business:avg_order_value\n        expr: |\n          sum(rate(pipeline_business_revenue_total[5m])) /\n          (sum(rate(pipeline_business_orders_total[5m])) + 0.001)\n\n      - record: business:orders_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_orders_total[1m])) * 60\n\n      - record: business:revenue_by_category:rate\n        expr: sum by (category) (rate(pipeline_business_revenue_total[1m])) * 60\n\n  # ===========================================\n  # Kafka Metrics\n  # ===========================================\n  - name: kafka_metrics\n    interval: 15s\n    rules:\n      - record: kafka:messages_in:rate1m\n        expr: sum by (topic) (rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:messages_in:rate1m:total\n        expr: sum(rate(kafka_topic_partition_current_offset[1m]))\n\n      - record: kafka:consumer_lag:total\n        expr: sum by (consumergroup, topic) (kafka_consumergroup_lag)\n\n      - record: kafka:consumer_lag:sum\n        expr: sum(kafka_consumergroup_lag)\n\n      - record: kafka:partitions:count\n        expr: sum(kafka_topic_partitions)\n\n  # ===========================================\n  # PostgreSQL Metrics\n  # ===========================================\n  - name: postgresql_metrics\n    interval: 15s\n    rules:\n      - record: postgresql:transactions:rate1m\n        expr: rate(pg_stat_database_xact_commit{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_inserted:rate1m\n        expr: rate(pg_stat_database_tup_inserted{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:rows_fetched:rate1m\n        expr: rate(pg_stat_database_tup_fetched{datname=\"datawarehouse\"}[1m])\n\n      - record: postgresql:connections:active\n        expr: sum(pg_stat_activity_count{datname=\"datawarehouse\"})\n\n      - record: postgresql:connections:usage_percent\n        expr: |\n          (sum(pg_stat_activity_count{datname=\"datawarehouse\"}) / \n           pg_settings_max_connections) * 100\n\n      - record: postgresql:cache_hit_ratio:percent\n        expr: |\n          (\n            pg_stat_database_blks_hit{datname=\"datawarehouse\"} /\n            (pg_stat_database_blks_hit{datname=\"datawarehouse\"} + \n             pg_stat_database_blks_read{datname=\"datawarehouse\"} + 0.001)\n          ) * 100\n\n      - record: postgresql:database_size:bytes\n        expr: pg_database_size_bytes{datname=\"datawarehouse\"}\n\n  # ===========================================\n  # Service Health\n  # ===========================================\n  - name: service_health\n    interval: 15s\n    rules:\n      - record: service:up:status\n        expr: up\n\n      - record: service:healthy:count\n        expr: count(up == 1)\n\n      - record: service:unhealthy:count\n        expr: count(up == 0)\n\n      - record: service:health:percent\n        expr: (count(up == 1) / count(up)) * 100\n````\n\n## File: monitoring/prometheus/rules/slo_alerts.yml\n````yaml\n# monitoring/prometheus/rules/slo_alerts.yml\n\ngroups:\n  # ===========================================\n  # Error Budget Alerts\n  # ===========================================\n  - name: error_budget_alerts\n    rules:\n      # Producer error budget low\n      - alert: ProducerErrorBudgetLow\n        expr: error_budget:producer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is running low\"\n          description: \"Producer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      # Producer error budget critical\n      - alert: ProducerErrorBudgetCritical\n        expr: error_budget:producer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget is critically low\"\n          description: \"Producer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining. Consider freezing deployments.\"\n\n      # Producer error budget exhausted\n      - alert: ProducerErrorBudgetExhausted\n        expr: error_budget:producer:remaining_percent <= 0\n        for: 1m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer error budget exhausted!\"\n          description: \"Producer has exhausted its error budget. SLO is being violated.\"\n\n      # Consumer error budget alerts\n      - alert: ConsumerErrorBudgetLow\n        expr: error_budget:consumer:remaining_percent < 50\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is running low\"\n          description: \"Consumer has consumed {{ $value | printf \\\"%.1f\\\" }}% of its error budget.\"\n\n      - alert: ConsumerErrorBudgetCritical\n        expr: error_budget:consumer:remaining_percent < 20\n        for: 5m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer error budget is critically low\"\n          description: \"Consumer has only {{ $value | printf \\\"%.1f\\\" }}% error budget remaining.\"\n\n  # ===========================================\n  # Burn Rate Alerts (Multi-window)\n  # ===========================================\n  - name: burn_rate_alerts\n    rules:\n      # Fast burn - will exhaust budget in ~2 hours\n      # 1h window with 14.4x burn rate\n      - alert: ProducerHighBurnRate\n        expr: |\n          burn_rate:producer:1h > 14.4\n          and\n          burn_rate:producer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer is burning error budget too fast\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. At this rate, error budget will be exhausted in ~2 hours.\"\n\n      # Slow burn - will exhaust budget in ~1 day\n      - alert: ProducerModerateBurnRate\n        expr: |\n          burn_rate:producer:6h > 6\n          and\n          burn_rate:producer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer burn rate is elevated\"\n          description: \"Producer burn rate is {{ $value | printf \\\"%.1f\\\" }}x. Error budget may be exhausted within a day.\"\n\n      # Consumer burn rate alerts\n      - alert: ConsumerHighBurnRate\n        expr: |\n          burn_rate:consumer:1h > 14.4\n          and\n          burn_rate:consumer:6h > 6\n        for: 2m\n        labels:\n          severity: critical\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer is burning error budget too fast\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n      - alert: ConsumerModerateBurnRate\n        expr: |\n          burn_rate:consumer:6h > 6\n          and\n          burn_rate:consumer:1h > 3\n        for: 15m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer burn rate is elevated\"\n          description: \"Consumer burn rate is {{ $value | printf \\\"%.1f\\\" }}x.\"\n\n  # ===========================================\n  # SLO Violation Alerts\n  # ===========================================\n  - name: slo_violation_alerts\n    rules:\n      # Producer SLO violations\n      - alert: ProducerSLOViolation\n        expr: sli:producer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer SLO is being violated\"\n          description: \"Producer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ProducerLatencySLOViolation\n        expr: sli:producer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: producer\n          category: slo\n        annotations:\n          summary: \"Producer latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of requests are under 100ms (target: 99%)\"\n\n      # Consumer SLO violations\n      - alert: ConsumerSLOViolation\n        expr: sli:consumer:success_rate < 0.999\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer SLO is being violated\"\n          description: \"Consumer success rate is {{ $value | printf \\\"%.3f\\\" }} (target: 0.999)\"\n\n      - alert: ConsumerLatencySLOViolation\n        expr: sli:consumer:latency_good < 0.99\n        for: 5m\n        labels:\n          severity: warning\n          service: consumer\n          category: slo\n        annotations:\n          summary: \"Consumer E2E latency SLO is being violated\"\n          description: \"Only {{ $value | printf \\\"%.1f\\\" }}% of messages are processed under 5s (target: 99%)\"\n\n      # Data freshness SLO\n      - alert: DataFreshnessSLOViolation\n        expr: sum(kafka_consumergroup_lag) > 1000\n        for: 5m\n        labels:\n          severity: warning\n          service: pipeline\n          category: slo\n        annotations:\n          summary: \"Data freshness SLO is being violated\"\n          description: \"Consumer lag is {{ $value }} messages (threshold: 1000)\"\n````\n\n## File: monitoring/prometheus/rules/slo_rules.yml\n````yaml\n# monitoring/prometheus/rules/slo_rules.yml\n\ngroups:\n  # ===========================================\n  # SLI Definitions\n  # ===========================================\n  - name: sli_metrics\n    interval: 30s\n    rules:\n      # --- Producer SLIs ---\n      \n      # Availability SLI: % of time producer is up\n      - record: sli:producer:availability\n        expr: avg_over_time(up{job=\"data-producer\"}[5m])\n\n      # Success Rate SLI: % of successful messages\n      - record: sli:producer:success_rate\n        expr: |\n          sum(rate(pipeline_producer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_producer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of requests under threshold (100ms)\n      - record: sli:producer:latency_good\n        expr: |\n          sum(rate(pipeline_producer_message_duration_seconds_bucket{le=\"0.1\"}[5m])) /\n          (sum(rate(pipeline_producer_message_duration_seconds_count[5m])) + 0.001)\n\n      # --- Consumer SLIs ---\n      \n      # Availability SLI\n      - record: sli:consumer:availability\n        expr: avg_over_time(up{job=\"data-consumer\"}[5m])\n\n      # Success Rate SLI\n      - record: sli:consumer:success_rate\n        expr: |\n          sum(rate(pipeline_consumer_messages_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_messages_total[5m])) + 0.001)\n\n      # Latency SLI: % of E2E latency under threshold (5s)\n      - record: sli:consumer:latency_good\n        expr: |\n          sum(rate(pipeline_consumer_end_to_end_latency_seconds_bucket{le=\"5.0\"}[5m])) /\n          (sum(rate(pipeline_consumer_end_to_end_latency_seconds_count[5m])) + 0.001)\n\n      # --- Pipeline SLIs ---\n      \n      # Data Freshness SLI: Consumer lag < 1000 messages\n      - record: sli:pipeline:data_freshness\n        expr: |\n          (sum(kafka_consumergroup_lag) < 1000) or vector(0)\n\n      # --- Database SLIs ---\n      \n      # Availability SLI\n      - record: sli:database:availability\n        expr: avg_over_time(pg_up[5m])\n\n      # Query Success Rate\n      - record: sli:database:query_success_rate\n        expr: |\n          sum(rate(pipeline_consumer_db_operations_total{status=\"success\"}[5m])) /\n          (sum(rate(pipeline_consumer_db_operations_total[5m])) + 0.001)\n\n  # ===========================================\n  # SLO Calculations (Rolling Windows)\n  # ===========================================\n  - name: slo_calculations\n    interval: 1m\n    rules:\n      # --- Producer SLOs ---\n      \n      # 30-day rolling availability (target: 99.9%)\n      - record: slo:producer:availability_30d\n        expr: avg_over_time(sli:producer:availability[30d])\n\n      # 30-day rolling success rate (target: 99.9%)\n      - record: slo:producer:success_rate_30d\n        expr: avg_over_time(sli:producer:success_rate[30d])\n\n      # 30-day rolling latency compliance (target: 99%)\n      - record: slo:producer:latency_compliance_30d\n        expr: avg_over_time(sli:producer:latency_good[30d])\n\n      # --- Consumer SLOs ---\n      \n      - record: slo:consumer:availability_30d\n        expr: avg_over_time(sli:consumer:availability[30d])\n\n      - record: slo:consumer:success_rate_30d\n        expr: avg_over_time(sli:consumer:success_rate[30d])\n\n      - record: slo:consumer:latency_compliance_30d\n        expr: avg_over_time(sli:consumer:latency_good[30d])\n\n      # --- Database SLOs ---\n      \n      - record: slo:database:availability_30d\n        expr: avg_over_time(sli:database:availability[30d])\n\n      - record: slo:database:query_success_rate_30d\n        expr: avg_over_time(sli:database:query_success_rate[30d])\n\n  # ===========================================\n  # Error Budget Calculations\n  # ===========================================\n  - name: error_budget\n    interval: 1m\n    rules:\n      # --- Producer Error Budget ---\n      \n      # Error budget remaining (target 99.9% = 0.1% budget)\n      # Formula: (SLO - (1 - current_success_rate)) / (1 - SLO)\n      - record: error_budget:producer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:producer:success_rate)) / 0.001\n          ) * 100\n\n      # Error budget consumed\n      - record: error_budget:producer:consumed_percent\n        expr: 100 - error_budget:producer:remaining_percent\n\n      # --- Consumer Error Budget ---\n      \n      - record: error_budget:consumer:remaining_percent\n        expr: |\n          (\n            (0.999 - (1 - sli:consumer:success_rate)) / 0.001\n          ) * 100\n\n      - record: error_budget:consumer:consumed_percent\n        expr: 100 - error_budget:consumer:remaining_percent\n\n      # --- Combined Pipeline Error Budget ---\n      \n      - record: error_budget:pipeline:remaining_percent\n        expr: |\n          (\n            error_budget:producer:remaining_percent + \n            error_budget:consumer:remaining_percent\n          ) / 2\n\n  # ===========================================\n  # Burn Rate Calculations\n  # ===========================================\n  - name: burn_rate\n    interval: 1m\n    rules:\n      # Burn rate = actual error rate / allowed error rate\n      # If burn rate > 1, we're consuming budget faster than allowed\n      \n      # Producer burn rate (1h window)\n      - record: burn_rate:producer:1h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[1h])) / 0.001\n\n      # Producer burn rate (6h window)\n      - record: burn_rate:producer:6h\n        expr: |\n          (1 - avg_over_time(sli:producer:success_rate[6h])) / 0.001\n\n      # Consumer burn rate (1h window)\n      - record: burn_rate:consumer:1h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[1h])) / 0.001\n\n      # Consumer burn rate (6h window)\n      - record: burn_rate:consumer:6h\n        expr: |\n          (1 - avg_over_time(sli:consumer:success_rate[6h])) / 0.001\n````\n\n## File: monitoring/prometheus/prometheus.yml\n````yaml\n# monitoring/prometheus/prometheus.yml\n\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    monitor: 'data-pipeline-monitor'\n\notlp:\n  promote_resource_attributes:\n    - service.name\n    - service.namespace\n    - service.instance.id\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n            - alertmanager:9093\n\nrule_files:\n  - /etc/prometheus/rules/*.yml\n\nscrape_configs:\n  # ===========================================\n  # Monitoring Stack\n  # ===========================================\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n        labels:\n          service: 'prometheus'\n          layer: 'monitoring'\n\n  - job_name: 'grafana'\n    static_configs:\n      - targets: ['grafana:3000']\n        labels:\n          service: 'grafana'\n          layer: 'monitoring'\n\n  - job_name: 'alertmanager'\n    static_configs:\n      - targets: ['alertmanager:9093']\n        labels:\n          service: 'alertmanager'\n          layer: 'monitoring'\n\n  - job_name: 'cadvisor'\n    static_configs:\n      - targets: ['cadvisor:8080']\n        labels:\n          service: 'cadvisor'\n          layer: 'monitoring'\n    metric_relabel_configs:\n      # Giữ lại metrics của containers trong docker-compose project\n      - source_labels: [container_label_com_docker_compose_project]\n        regex: '.+'\n        action: keep\n      # Chỉ drop các metrics không cần thiết (bỏ container_last_seen khỏi list)\n      - source_labels: [__name__]\n        regex: 'container_(tasks_state|memory_failures_total)'\n        action: drop\n\n  # ===========================================\n  # Infrastructure\n  # ===========================================\n  - job_name: 'postgresql'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n        labels:\n          service: 'postgresql'\n          layer: 'infrastructure'\n          database: 'datawarehouse'\n\n  - job_name: 'kafka'\n    static_configs:\n      - targets: ['kafka-exporter:9308']\n        labels:\n          service: 'kafka'\n          layer: 'infrastructure'\n\n  # ===========================================\n  # Applications\n  # ===========================================\n  - job_name: 'data-producer'\n    static_configs:\n      - targets: ['data-producer:8000']\n        labels:\n          service: 'producer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  - job_name: 'data-consumer'\n    static_configs:\n      - targets: ['data-consumer:8001']\n        labels:\n          service: 'consumer'\n          layer: 'application'\n          app: 'data-pipeline'\n\n  # ===========================================\n  # stress test\n  # ===========================================\n  - job_name: 'load-test'\n    static_configs:\n      - targets: ['load-test:8002']\n        labels:\n          service: 'load-test'\n          layer: 'testing'\n    scrape_interval: 5s\n````\n\n## File: monitoring/promtail/promtail-config.yml\n````yaml\n# monitoring/promtail/promtail-config.yml\n\nserver:\n  http_listen_port: 9080\n  grpc_listen_port: 0\n  log_level: info\n\npositions:\n  filename: /tmp/positions.yaml\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n    tenant_id: fake\n\nscrape_configs:\n  # ===========================================\n  # Docker Container Logs\n  # ===========================================\n  - job_name: docker\n    docker_sd_configs:\n      - host: unix:///var/run/docker.sock\n        refresh_interval: 5s\n    relabel_configs:\n      # Lấy container name làm label\n      - source_labels: ['__meta_docker_container_name']\n        regex: '/(.*)'\n        target_label: 'container'\n      \n      # Lấy compose service name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'service'\n      \n      # Lấy compose project name\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        target_label: 'project'\n      \n      # Lấy container ID\n      - source_labels: ['__meta_docker_container_id']\n        target_label: 'container_id'\n      \n      # Thêm job label\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']\n        target_label: 'job'\n      \n      # Filter: chỉ lấy logs từ containers có compose project\n      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']\n        regex: '.+'\n        action: keep\n\n    pipeline_stages:\n      # ===========================================\n      # Parse JSON logs (từ Producer/Consumer)\n      # ===========================================\n      - match:\n          selector: '{service=~\"data-producer|data-consumer\"}'\n          stages:\n            - json:\n                expressions:\n                  level: level\n                  message: message\n                  event: event\n                  service: service\n                  timestamp: timestamp\n                  order_id: order_id\n                  category: category\n                  error: error\n                  error_type: error_type\n                  batch_size: batch_size\n                  duration_ms: duration_ms\n                  throughput_per_sec: throughput_per_sec\n            \n            # Set log level as label\n            - labels:\n                level:\n                event:\n            \n            # Set timestamp from log\n            - timestamp:\n                source: timestamp\n                format: RFC3339Nano\n                fallback_formats:\n                  - RFC3339\n            \n            # Extract metrics from logs (optional)\n            - metrics:\n                log_lines_total:\n                  type: Counter\n                  description: \"Total log lines\"\n                  source: message\n                  config:\n                    action: inc\n                    match_all: true\n\n      # ===========================================\n      # Parse Kafka logs\n      # ===========================================\n      - match:\n          selector: '{service=\"kafka\"}'\n          stages:\n            - regex:\n                expression: '^\\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})\\] (?P<level>\\w+) (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Parse PostgreSQL logs\n      # ===========================================\n      - match:\n          selector: '{service=\"postgres\"}'\n          stages:\n            - regex:\n                expression: '^(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3} \\w+) \\[(?P<pid>\\d+)\\] (?P<level>\\w+):  (?P<message>.*)'\n            - labels:\n                level:\n\n      # ===========================================\n      # Default: keep raw log\n      # ===========================================\n      - match:\n          selector: '{level=\"\"}'\n          stages:\n            - static_labels:\n                level: info\n````\n\n## File: monitoring/stress-testing/load-test/Dockerfile\n````\n# stress-testing/load-test/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY load_test.py .\n\nENTRYPOINT [\"python\", \"load_test.py\"]\nCMD [\"--help\"]\n````\n\n## File: monitoring/stress-testing/load-test/load_test.py\n````python\n# stress-testing/load-test/load_test.py\n\nimport json\nimport random\nimport time\nimport uuid\nimport threading\nimport signal\nimport sys\nfrom datetime import datetime, timezone\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any\n\nimport click\nfrom faker import Faker\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\n\n# ===========================================\n# Metrics\n# ===========================================\nMESSAGES_SENT = Counter(\n    'loadtest_messages_sent_total',\n    'Total messages sent',\n    ['status']\n)\n\nSEND_DURATION = Histogram(\n    'loadtest_send_duration_seconds',\n    'Time to send message',\n    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)\n)\n\nCURRENT_RPS = Gauge(\n    'loadtest_current_rps',\n    'Current requests per second'\n)\n\nACTIVE_THREADS = Gauge(\n    'loadtest_active_threads',\n    'Number of active threads'\n)\n\nTARGET_RPS = Gauge(\n    'loadtest_target_rps',\n    'Target requests per second'\n)\n\n# ===========================================\n# Fake Data Generator\n# ===========================================\nfake = Faker()\n\nPRODUCTS = {\n    \"Electronics\": [\n        {\"name\": \"Smartphone\", \"price_range\": (299, 1299)},\n        {\"name\": \"Laptop\", \"price_range\": (499, 2499)},\n        {\"name\": \"Tablet\", \"price_range\": (199, 999)},\n        {\"name\": \"Headphones\", \"price_range\": (29, 399)},\n        {\"name\": \"Smart Watch\", \"price_range\": (99, 599)},\n    ],\n    \"Clothing\": [\n        {\"name\": \"T-Shirt\", \"price_range\": (15, 45)},\n        {\"name\": \"Jeans\", \"price_range\": (35, 120)},\n        {\"name\": \"Jacket\", \"price_range\": (50, 250)},\n        {\"name\": \"Sneakers\", \"price_range\": (45, 180)},\n        {\"name\": \"Dress\", \"price_range\": (30, 150)},\n    ],\n    \"Home & Garden\": [\n        {\"name\": \"Coffee Maker\", \"price_range\": (25, 200)},\n        {\"name\": \"Vacuum Cleaner\", \"price_range\": (80, 400)},\n        {\"name\": \"Bed Sheets\", \"price_range\": (30, 120)},\n        {\"name\": \"Plant Pot\", \"price_range\": (10, 50)},\n        {\"name\": \"Lamp\", \"price_range\": (20, 150)},\n    ],\n    \"Books\": [\n        {\"name\": \"Fiction Novel\", \"price_range\": (8, 25)},\n        {\"name\": \"Technical Book\", \"price_range\": (30, 80)},\n        {\"name\": \"Cookbook\", \"price_range\": (15, 45)},\n        {\"name\": \"Biography\", \"price_range\": (12, 35)},\n        {\"name\": \"Children Book\", \"price_range\": (5, 20)},\n    ],\n    \"Sports\": [\n        {\"name\": \"Yoga Mat\", \"price_range\": (15, 60)},\n        {\"name\": \"Dumbbell Set\", \"price_range\": (30, 150)},\n        {\"name\": \"Running Shoes\", \"price_range\": (60, 200)},\n        {\"name\": \"Bicycle\", \"price_range\": (200, 1500)},\n        {\"name\": \"Tennis Racket\", \"price_range\": (25, 200)},\n    ],\n}\n\n\ndef generate_order() -> dict[str, Any]:\n    \"\"\"Generate a fake order\"\"\"\n    category = random.choice(list(PRODUCTS.keys()))\n    product = random.choice(PRODUCTS[category])\n    \n    quantity = random.randint(1, 5)\n    unit_price = round(random.uniform(*product[\"price_range\"]), 2)\n    total_amount = round(quantity * unit_price, 2)\n    \n    return {\n        \"order_id\": str(uuid.uuid4()),\n        \"customer_id\": f\"CUST-{fake.random_number(digits=6, fix_len=True)}\",\n        \"product_id\": f\"PROD-{fake.random_number(digits=8, fix_len=True)}\",\n        \"product_name\": product[\"name\"],\n        \"category\": category,\n        \"quantity\": quantity,\n        \"unit_price\": unit_price,\n        \"total_amount\": total_amount,\n        \"order_status\": \"pending\",\n        \"customer_email\": fake.email(),\n        \"shipping_address\": fake.address().replace(\"\\n\", \", \"),\n        \"created_at\": datetime.now(timezone.utc).isoformat(),\n        \"load_test\": True,\n    }\n\n\n# ===========================================\n# Load Tester\n# ===========================================\nclass LoadTester:\n    def __init__(self, bootstrap_servers: str, topic: str):\n        self.bootstrap_servers = bootstrap_servers\n        self.topic = topic\n        self.producer = None\n        self.running = False\n        self.stats = {\n            \"sent\": 0,\n            \"failed\": 0,\n            \"start_time\": None,\n        }\n        self._connect()\n    \n    def _connect(self):\n        \"\"\"Connect to Kafka\"\"\"\n        max_retries = 10\n        for attempt in range(max_retries):\n            try:\n                self.producer = KafkaProducer(\n                    bootstrap_servers=self.bootstrap_servers,\n                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                    key_serializer=lambda k: k.encode('utf-8') if k else None,\n                    acks=1,  # Faster for load testing\n                    linger_ms=5,\n                    batch_size=16384,\n                    buffer_memory=33554432,\n                )\n                click.echo(f\"✅ Connected to Kafka: {self.bootstrap_servers}\")\n                return\n            except KafkaError as e:\n                click.echo(f\"⏳ Kafka connection attempt {attempt + 1}/{max_retries}: {e}\")\n                time.sleep(2)\n        \n        raise Exception(\"Failed to connect to Kafka\")\n    \n    def send_message(self) -> bool:\n        \"\"\"Send a single message\"\"\"\n        order = generate_order()\n        \n        try:\n            start = time.time()\n            future = self.producer.send(\n                self.topic,\n                key=order[\"order_id\"],\n                value=order\n            )\n            future.get(timeout=10)\n            duration = time.time() - start\n            \n            MESSAGES_SENT.labels(status=\"success\").inc()\n            SEND_DURATION.observe(duration)\n            self.stats[\"sent\"] += 1\n            return True\n            \n        except Exception as e:\n            MESSAGES_SENT.labels(status=\"failed\").inc()\n            self.stats[\"failed\"] += 1\n            return False\n    \n    def run_constant_load(self, rps: int, duration_seconds: int, threads: int = 10):\n        \"\"\"Run constant load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        TARGET_RPS.set(rps)\n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n🚀 Starting Constant Load Test\")\n        click.echo(f\"   Target RPS: {rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        interval = 1.0 / rps if rps > 0 else 1\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                batch_start = time.time()\n                futures = []\n                \n                # Submit batch of requests\n                for _ in range(min(rps, 100)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                # Wait for completion\n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                # Calculate actual RPS\n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                # Print progress\n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Sent: {self.stats['sent']:,} | \"\n                    f\"Failed: {self.stats['failed']:,} | \"\n                    f\"RPS: {actual_rps:.1f}\",\n                    nl=False\n                )\n                \n                # Rate limiting\n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_ramp_up(self, start_rps: int, end_rps: int, duration_seconds: int, threads: int = 20):\n        \"\"\"Run ramp-up load test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n📈 Starting Ramp-Up Load Test\")\n        click.echo(f\"   Start RPS: {start_rps}\")\n        click.echo(f\"   End RPS: {end_rps}\")\n        click.echo(f\"   Duration: {duration_seconds}s\")\n        click.echo(f\"   Threads: {threads}\")\n        click.echo(\"-\" * 50)\n        \n        rps_increment = (end_rps - start_rps) / duration_seconds\n        end_time = time.time() + duration_seconds\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                elapsed = time.time() - self.stats[\"start_time\"]\n                current_target_rps = int(start_rps + (rps_increment * elapsed))\n                TARGET_RPS.set(current_target_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_target_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r⏱️  Remaining: {remaining:3d}s | \"\n                    f\"Target RPS: {current_target_rps:3d} | \"\n                    f\"Actual RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def run_spike(self, base_rps: int, spike_rps: int, spike_duration: int, total_duration: int, threads: int = 20):\n        \"\"\"Run spike test\"\"\"\n        self.running = True\n        self.stats[\"start_time\"] = time.time()\n        \n        ACTIVE_THREADS.set(threads)\n        \n        click.echo(f\"\\n⚡ Starting Spike Test\")\n        click.echo(f\"   Base RPS: {base_rps}\")\n        click.echo(f\"   Spike RPS: {spike_rps}\")\n        click.echo(f\"   Spike Duration: {spike_duration}s\")\n        click.echo(f\"   Total Duration: {total_duration}s\")\n        click.echo(\"-\" * 50)\n        \n        end_time = time.time() + total_duration\n        spike_start = time.time() + (total_duration - spike_duration) / 2\n        spike_end = spike_start + spike_duration\n        \n        with ThreadPoolExecutor(max_workers=threads) as executor:\n            while self.running and time.time() < end_time:\n                current_time = time.time()\n                \n                # Determine current RPS\n                if spike_start <= current_time <= spike_end:\n                    current_rps = spike_rps\n                    phase = \"🔥 SPIKE\"\n                else:\n                    current_rps = base_rps\n                    phase = \"📊 BASE \"\n                \n                TARGET_RPS.set(current_rps)\n                \n                batch_start = time.time()\n                futures = []\n                \n                for _ in range(min(current_rps, 200)):\n                    if not self.running:\n                        break\n                    futures.append(executor.submit(self.send_message))\n                \n                for future in as_completed(futures):\n                    try:\n                        future.result()\n                    except Exception:\n                        pass\n                \n                elapsed = time.time() - self.stats[\"start_time\"]\n                actual_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n                CURRENT_RPS.set(actual_rps)\n                \n                remaining = int(end_time - time.time())\n                click.echo(\n                    f\"\\r{phase} | Remaining: {remaining:3d}s | \"\n                    f\"RPS: {actual_rps:.1f} | \"\n                    f\"Sent: {self.stats['sent']:,}\",\n                    nl=False\n                )\n                \n                batch_duration = time.time() - batch_start\n                sleep_time = max(0, 1.0 - batch_duration)\n                if sleep_time > 0:\n                    time.sleep(sleep_time)\n        \n        self.producer.flush()\n        self._print_summary()\n    \n    def _print_summary(self):\n        \"\"\"Print test summary\"\"\"\n        elapsed = time.time() - self.stats[\"start_time\"]\n        total = self.stats[\"sent\"] + self.stats[\"failed\"]\n        success_rate = (self.stats[\"sent\"] / total * 100) if total > 0 else 0\n        avg_rps = self.stats[\"sent\"] / elapsed if elapsed > 0 else 0\n        \n        click.echo(\"\\n\")\n        click.echo(\"=\" * 50)\n        click.echo(\"📊 LOAD TEST SUMMARY\")\n        click.echo(\"=\" * 50)\n        click.echo(f\"   Duration:     {elapsed:.1f}s\")\n        click.echo(f\"   Total Sent:   {self.stats['sent']:,}\")\n        click.echo(f\"   Failed:       {self.stats['failed']:,}\")\n        click.echo(f\"   Success Rate: {success_rate:.2f}%\")\n        click.echo(f\"   Avg RPS:      {avg_rps:.1f}\")\n        click.echo(\"=\" * 50)\n    \n    def stop(self):\n        \"\"\"Stop the test\"\"\"\n        self.running = False\n        if self.producer:\n            self.producer.close()\n\n\n# ===========================================\n# CLI\n# ===========================================\n@click.group()\ndef cli():\n    \"\"\"Load Testing Tool for Data Pipeline\"\"\"\n    pass\n\n\n@cli.command()\n@click.option('--rps', default=50, help='Requests per second')\n@click.option('--duration', default=60, help='Duration in seconds')\n@click.option('--threads', default=10, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef constant(rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run constant load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_constant_load(rps, duration, threads)\n\n\n@cli.command()\n@click.option('--start-rps', default=10, help='Starting RPS')\n@click.option('--end-rps', default=100, help='Ending RPS')\n@click.option('--duration', default=120, help='Duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef rampup(start_rps, end_rps, duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run ramp-up load test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_ramp_up(start_rps, end_rps, duration, threads)\n\n\n@cli.command()\n@click.option('--base-rps', default=20, help='Base RPS')\n@click.option('--spike-rps', default=200, help='Spike RPS')\n@click.option('--spike-duration', default=30, help='Spike duration in seconds')\n@click.option('--total-duration', default=120, help='Total duration in seconds')\n@click.option('--threads', default=20, help='Number of threads')\n@click.option('--kafka', default='kafka:29092', help='Kafka bootstrap servers')\n@click.option('--topic', default='ecommerce.orders', help='Kafka topic')\n@click.option('--metrics-port', default=8002, help='Prometheus metrics port')\ndef spike(base_rps, spike_rps, spike_duration, total_duration, threads, kafka, topic, metrics_port):\n    \"\"\"Run spike test\"\"\"\n    start_http_server(metrics_port)\n    click.echo(f\"📊 Metrics available at http://localhost:{metrics_port}/metrics\")\n    \n    tester = LoadTester(kafka, topic)\n    \n    def signal_handler(sig, frame):\n        click.echo(\"\\n\\n⚠️  Stopping test...\")\n        tester.stop()\n        sys.exit(0)\n    \n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    \n    tester.run_spike(base_rps, spike_rps, spike_duration, total_duration, threads)\n\n\nif __name__ == \"__main__\":\n    cli()\n````\n\n## File: monitoring/stress-testing/load-test/requirements.txt\n````\n# stress-testing/load-test/requirements.txt\n\n# Kafka\nkafka-python-ng==2.2.2\n\n# Fake data\nfaker==24.4.0\n\n# CLI\nclick==8.1.7\n\n# Metrics\nprometheus-client==0.20.0\n\n# Utilities\npydantic==2.6.4\npydantic-settings==2.2.1\n````\n\n## File: monitoring/stress-testing/scripts/chaos_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/chaos_test.sh\n\necho \"💥 Starting Chaos Test\"\necho \"======================\"\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\ncd \"$(dirname \"$0\")/../../\"\n\n# Function to check service health\ncheck_health() {\n    local service=$1\n    local url=$2\n    if curl -s \"$url\" > /dev/null 2>&1; then\n        echo -e \"${GREEN}✅ $service is UP${NC}\"\n        return 0\n    else\n        echo -e \"${RED}❌ $service is DOWN${NC}\"\n        return 1\n    fi\n}\n\n# Function to wait for recovery\nwait_for_recovery() {\n    local service=$1\n    local url=$2\n    local max_wait=60\n    local waited=0\n    \n    echo -e \"${YELLOW}⏳ Waiting for $service to recover...${NC}\"\n    \n    while [ $waited -lt $max_wait ]; do\n        if curl -s \"$url\" > /dev/null 2>&1; then\n            echo -e \"${GREEN}✅ $service recovered after ${waited}s${NC}\"\n            return 0\n        fi\n        sleep 2\n        waited=$((waited + 2))\n    done\n    \n    echo -e \"${RED}❌ $service did not recover within ${max_wait}s${NC}\"\n    return 1\n}\n\necho \"\"\necho \"📊 Initial Health Check\"\necho \"-----------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\ncheck_health \"Prometheus\" \"http://localhost:9090/-/healthy\"\ncheck_health \"Grafana\" \"http://localhost:3000/api/health\"\n\necho \"\"\necho \"💥 Test 1: Kill Consumer\"\necho \"------------------------\"\ndocker stop data-consumer\necho -e \"${YELLOW}Consumer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Consumer...\"\ndocker start data-consumer\nwait_for_recovery \"Consumer\" \"http://localhost:8001/metrics\"\n\necho \"\"\necho \"💥 Test 2: Kill Producer\"\necho \"------------------------\"\ndocker stop data-producer\necho -e \"${YELLOW}Producer stopped. Check Grafana for alerts...${NC}\"\necho \"Waiting 30 seconds...\"\nsleep 30\n\necho \"\"\necho \"🔄 Restarting Producer...\"\ndocker start data-producer\nwait_for_recovery \"Producer\" \"http://localhost:8000/metrics\"\n\necho \"\"\necho \"💥 Test 3: Kill Kafka (WARNING: This will affect data flow)\"\necho \"------------------------------------------------------------\"\nread -p \"Do you want to proceed? (y/N) \" -n 1 -r\necho\nif [[ $REPLY =~ ^[Yy]$ ]]; then\n    docker stop kafka\n    echo -e \"${YELLOW}Kafka stopped. Check Grafana for cascading failures...${NC}\"\n    echo \"Waiting 60 seconds...\"\n    sleep 60\n    \n    echo \"\"\n    echo \"🔄 Restarting Kafka...\"\n    docker start kafka\n    sleep 10\n    wait_for_recovery \"Kafka\" \"http://localhost:9308/metrics\"\nfi\n\necho \"\"\necho \"📊 Final Health Check\"\necho \"---------------------\"\ncheck_health \"Producer\" \"http://localhost:8000/metrics\"\ncheck_health \"Consumer\" \"http://localhost:8001/metrics\"\ncheck_health \"Kafka\" \"http://localhost:9308/metrics\"\ncheck_health \"PostgreSQL\" \"http://localhost:9187/metrics\"\n\necho \"\"\necho \"✅ Chaos Test Complete!\"\necho \"Check Grafana dashboards and Alertmanager for results.\"\n````\n\n## File: monitoring/stress-testing/scripts/ramp_up_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/ramp_up_test.sh\n\necho \"📈 Starting Ramp-Up Load Test\"\necho \"==============================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test rampup \\\n    --start-rps ${START_RPS:-10} \\\n    --end-rps ${END_RPS:-100} \\\n    --duration ${DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n````\n\n## File: monitoring/stress-testing/scripts/run_load_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/run_load_test.sh\n\necho \"🚀 Starting Constant Load Test\"\necho \"================================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test constant \\\n    --rps ${RPS:-50} \\\n    --duration ${DURATION:-60} \\\n    --threads ${THREADS:-10} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n````\n\n## File: monitoring/stress-testing/scripts/spike_test.sh\n````bash\n#!/bin/bash\n# stress-testing/scripts/spike_test.sh\n\necho \"⚡ Starting Spike Test\"\necho \"======================\"\n\ncd \"$(dirname \"$0\")/..\"\n\ndocker-compose run --rm load-test spike \\\n    --base-rps ${BASE_RPS:-20} \\\n    --spike-rps ${SPIKE_RPS:-200} \\\n    --spike-duration ${SPIKE_DURATION:-30} \\\n    --total-duration ${TOTAL_DURATION:-120} \\\n    --threads ${THREADS:-20} \\\n    --kafka kafka:29092 \\\n    --topic ecommerce.orders \\\n    --metrics-port 8002\n````\n\n## File: monitoring/stress-testing/docker-compose-stress-test.yml\n````yaml\n# stress-testing/docker-compose.yml\n\nservices:\n  load-test:\n    build:\n      context: ./load-test\n      dockerfile: Dockerfile\n    container_name: load-test\n    networks:\n      - monitoring-net\n    ports:\n      - \"8002:8002\"\n    environment:\n      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092\n      - KAFKA_TOPIC=ecommerce.orders\n    # Command sẽ được override khi chạy\n    command: [\"--help\"]\n\nnetworks:\n  monitoring-net:\n    external: true\n````\n\n## File: monitoring/docker-compose-monitoring.yml\n````yaml\n# monitoring/docker-compose.yml\n\nservices:\n  # ===========================================\n  # Prometheus - Metrics Collection & Storage\n  # ===========================================\n  prometheus:\n    image: prom/prometheus:v3.5.0\n    container_name: prometheus\n    restart: unless-stopped\n    \n    # Command line arguments\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention.time=15d'        # Giữ data 15 ngày\n      - '--web.enable-lifecycle'                   # Cho phép reload config via API\n      - '--web.enable-admin-api'                   # Enable admin API\n      - '--web.enable-remote-write-receiver'\n    volumes:\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - ./prometheus/rules:/etc/prometheus/rules:ro\n      - prometheus_data:/prometheus\n    \n    ports:\n      - \"9090:9090\"\n    \n    networks:\n      - monitoring-net\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9090/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Alertmanager\n  # ===========================================\n  alertmanager:\n    image: prom/alertmanager:v0.27.0\n    container_name: alertmanager\n    restart: unless-stopped\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n      - '--web.external-url=http://localhost:9093'\n      - '--cluster.listen-address='\n    volumes:\n      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro\n      - alertmanager_data:/alertmanager\n    ports:\n      - \"9093:9093\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:9093/-/healthy\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n\n  # ===========================================\n  # Loki - Log Aggregation\n  # ===========================================\n  loki:\n    image: grafana/loki:3.3.2\n    container_name: loki\n    restart: unless-stopped\n    command: -config.file=/etc/loki/loki-config.yml\n    volumes:\n      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro\n      - ./loki/rules:/loki/rules:ro\n      - loki_data:/loki\n    ports:\n      - \"3100:3100\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3100/ready\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n  # ===========================================\n  # Promtail - Log Collector\n  # ===========================================\n  promtail:\n    image: grafana/promtail:3.3.2\n    container_name: promtail\n    restart: unless-stopped\n    command: -config.file=/etc/promtail/promtail-config.yml\n    volumes:\n      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n    ports:\n      - \"9080:9080\"\n    networks:\n      - monitoring-net\n    depends_on:\n      loki:\n        condition: service_healthy\n\n\n  # ===========================================\n  # Grafana - Visualization & Dashboards\n  # ===========================================\n  grafana:\n    image: grafana/grafana:12.3.1\n    container_name: grafana\n    restart: unless-stopped\n    \n    environment:\n      # Admin credentials\n      - GF_SECURITY_ADMIN_USER=admin\n      - GF_SECURITY_ADMIN_PASSWORD=admin123\n      \n      # Server settings\n      - GF_SERVER_ROOT_URL=http://localhost:3000\n      \n      # Disable analytics\n      - GF_ANALYTICS_REPORTING_ENABLED=false\n      - GF_ANALYTICS_CHECK_FOR_UPDATES=false\n      \n      # Feature toggles (Grafana 12 features)\n      - GF_FEATURE_TOGGLES_ENABLE=nestedFolders\n    volumes:\n      # Provisioning - auto setup datasources & dashboards\n      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro\n      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro\n      \n      # Dashboard JSON files - mount trực tiếp từ local 👇\n      - ./grafana/dashboards/overview:/var/lib/grafana/dashboards/overview:ro\n      - ./grafana/dashboards/infrastructure:/var/lib/grafana/dashboards/infrastructure:ro\n      - ./grafana/dashboards/applications:/var/lib/grafana/dashboards/applications:ro\n      - ./grafana/dashboards/logs:/var/lib/grafana/dashboards/logs:ro\n      - ./grafana/dashboards/tracing:/var/lib/grafana/dashboards/tracing:ro\n      # Persistent storage\n      - grafana_data:/var/lib/grafana\n    \n    ports:\n      - \"3000:3000\"\n    \n    networks:\n      - monitoring-net\n    \n    depends_on:\n      prometheus:\n        condition: service_healthy\n    \n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3000/api/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  # ===========================================\n  # cAdvisor - Container Metrics\n  # ===========================================\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor:v0.51.0\n    container_name: cadvisor\n    restart: unless-stopped\n    privileged: true\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    ports:\n      - \"8080:8080\"\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:8080/healthz\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n  # Jaeger - Distributed Tracing\n  # ===========================================\n  jaeger:\n    image: jaegertracing/all-in-one:1.54\n    container_name: jaeger\n    restart: unless-stopped\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n      - LOG_LEVEL=info\n    ports:\n      - \"16686:16686\"   # Jaeger UI\n      - \"4317:4317\"     # OTLP gRPC\n      - \"4318:4318\"     # OTLP HTTP\n    networks:\n      - monitoring-net\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:16686/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n\n# ===========================================\n# Volumes - Persistent Storage\n# ===========================================\nvolumes:\n  prometheus_data:\n    name: prometheus_data\n  grafana_data:\n    name: grafana_data\n  grafana_dashboards:\n    name: grafana_dashboards\n  alertmanager_data:\n    name: alertmanager_data\n  loki_data:\n    name: loki_data\n# ===========================================\n# Networks - External Network\n# ===========================================\nnetworks:\n  monitoring-net:\n    external: true\n````\n\n## File: monitoring/loki.md\n````markdown\n```mermaid\ngraph LR\n    subgraph \"Applications\"\n        P[Producer]\n        C[Consumer]\n    end\n    \n    subgraph \"Log Collection\"\n        PR[Promtail<br/>Log Collector]\n    end\n    \n    subgraph \"Storage & Query\"\n        L[Loki<br/>Log Aggregation]\n        G[Grafana<br/>Visualization]\n    end\n    \n    P -->|stdout/stderr| PR\n    C -->|stdout/stderr| PR\n    PR -->|push logs| L\n    L -->|query| G\n```\n````\n\n## File: monitoring/SLO.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"Definitions\"\n        SLI[SLI - Service Level Indicator<br/>📊 Metric đo lường]\n        SLO[SLO - Service Level Objective<br/>🎯 Mục tiêu cần đạt]\n        SLA[SLA - Service Level Agreement<br/>📝 Cam kết với khách hàng]\n    end\n    \n    SLI --> SLO --> SLA\n    \n    subgraph \"Example\"\n        E1[SLI: 99.5% requests < 500ms]\n        E2[SLO: 99.9% availability]\n        E3[SLA: Hoàn tiền nếu < 99.5%]\n    end\n```\n````\n\n## File: monitoring/stack.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"Observability Stack\"\n        M[Metrics<br/>Prometheus]\n        L[Logs<br/>Loki]\n        A[Alerts<br/>Alertmanager]\n        V[Visualization<br/>Grafana]\n    end\n    \n    subgraph \"Data Pipeline\"\n        P[Producer]\n        K[Kafka]\n        C[Consumer]\n        DB[(PostgreSQL)]\n    end\n    \n    subgraph \"Infrastructure\"\n        CA[cAdvisor]\n        KE[Kafka Exporter]\n        PE[Postgres Exporter]\n    end\n    \n    P --> K --> C --> DB\n    P & C --> M\n    P & C --> L\n    CA & KE & PE --> M\n    M & L --> A\n    M & L & A --> V\n```\n````\n\n## File: networks/docker-compose-network.yml\n````yaml\nnetworks:\n  monitoring-net:\n    name: monitoring-net\n    driver: bridge\n````\n\n## File: monitoring-overview.md\n````markdown\n```mermaid\ngraph LR\n    subgraph \"Data Source\"\n        A[🐍 Python Producer<br/>Fake e-commerce data]\n    end\n    \n    subgraph \"Message Queue\"\n        B[📨 Apache Kafka<br/>+ Zookeeper]\n    end\n    \n    subgraph \"Processing & Storage\"\n        C[🐍 Python Consumer<br/>Transform data]\n        D[(🐘 PostgreSQL<br/>Data Warehouse)]\n    end\n    \n    subgraph \"Monitoring Stack\"\n        E[📊 Prometheus]\n        F[📈 Grafana]\n    end\n    \n    A -->|produce orders| B\n    B -->|consume| C\n    C -->|insert| D\n    \n    A -.->|metrics| E\n    B -.->|metrics| E\n    C -.->|metrics| E\n    D -.->|metrics| E\n    E -->|visualize| F\n```\n````\n\n## File: structure.md\n````markdown\n```mermaid\ngraph TB\n    subgraph \"External Network: monitoring-net\"\n        subgraph \"monitoring/\"\n            P[Prometheus]\n            G[Grafana]\n        end\n        \n        subgraph \"infrastructure/\"\n            K[Kafka]\n            Z[Zookeeper]\n            PG[PostgreSQL]\n            \n            KE[Kafka Exporter]\n            PE[Postgres Exporter]\n        end\n        \n        subgraph \"applications/\"\n            PR[Producer]\n            CO[Consumer]\n        end\n    end\n    \n    P -.->|scrape| KE\n    P -.->|scrape| PE\n    P -.->|scrape| PR\n    P -.->|scrape| CO\n    G -->|query| P\n```\n````\n</file>\n\n<file path=\"structure.md\">\n```mermaid\ngraph TB\n    subgraph \"External Network: monitoring-net\"\n        subgraph \"monitoring/\"\n            P[Prometheus]\n            G[Grafana]\n        end\n        \n        subgraph \"infrastructure/\"\n            K[Kafka]\n            Z[Zookeeper]\n            PG[PostgreSQL]\n            \n            KE[Kafka Exporter]\n            PE[Postgres Exporter]\n        end\n        \n        subgraph \"applications/\"\n            PR[Producer]\n            CO[Consumer]\n        end\n    end\n    \n    P -.->|scrape| KE\n    P -.->|scrape| PE\n    P -.->|scrape| PR\n    P -.->|scrape| CO\n    G -->|query| P\n```\n</file>\n\n</files>",
    "structure.md": "```mermaid\ngraph TB\n    subgraph \"External Network: monitoring-net\"\n        subgraph \"monitoring/\"\n            P[Prometheus]\n            G[Grafana]\n        end\n        \n        subgraph \"infrastructure/\"\n            K[Kafka]\n            Z[Zookeeper]\n            PG[PostgreSQL]\n            \n            KE[Kafka Exporter]\n            PE[Postgres Exporter]\n        end\n        \n        subgraph \"applications/\"\n            PR[Producer]\n            CO[Consumer]\n        end\n    end\n    \n    P -.->|scrape| KE\n    P -.->|scrape| PE\n    P -.->|scrape| PR\n    P -.->|scrape| CO\n    G -->|query| P\n```"
  }
}